% copyright 2001 Colorado School of Mines, all rights reserved
\documentstyle[11pt,epsf]{book}

% make text fill more of the page
%\nofiles
%\scrollmode
\textwidth 6.25in
\textheight 8.75in
\oddsidemargin .125in
\evensidemargin .125in
\topmargin -.5in

\def\releasenumber{44RXX}
\def\currentyear{2019}
\def\website{wiki.Seismic-Unix.org}
\newtheorem{question}{Question}
\newtheorem{answer}{Answer}
\newenvironment{rmans}{\begin{answer} \em}{\end{answer}}

\begin{document}

\input title.tex

\tableofcontents
\newpage
\chapter*{Licensing and other Legal Matters}
\addtocontents{toc}{\protect \contentsline {chapter}{\protect \numberline {}Licensing and other Legal Matters}{vii}}
\markboth{LICENCE and LEGAL}{}
\input legal.tex

\chapter*{Acknowledgments}
%\addtocontents{toc}{{\bf Acknowledgments}}
\addtocontents{toc}{\protect \contentsline {chapter}{\protect \numberline {}Acknowledgements}{ix}}
\markboth{ACKNOWLEDGMENTS}{}

The Seismic Unix project was developed by the Center for Wave Phenomena (CWP), 
Department of Geophysical Engineering, Colorado School of Mines.  
The project has received funding in the past  from the Society of 
Exploration Geophysicists (SEG) and the Gas Research Institute (GRI).

The Seismic Unix was supported through its development phase, in part by
the Consortium Project on Imaging Complex Structures, through the Center
for Wave Phenomena.

So many faculty and students, both at our Center and elsewhere, have
contributed to SU, that it is impossible to list them all here.
However, certain people have made such important contributions that they
deserve explicit mention.

Einar Kjartansson began writing what is now called SU
(the SY package) in the late 1970's while still a graduate student
at Jon Claerbout's Stanford Exploration Project (SEP). He continued 
to expand the package while he was a professor at the University of 
Utah in the early eighties.  In 1984, during an extended visit to SEP
Einar introduced SY to Shuki Ronen, then a graduate student at Stanford.
Ronen further developed SY from 1984 to 1986.   Other students at SEP
started to use it and contributed code and ideas.  SY was inspired by much
other software developed at SEP and benefited from the foundations laid by
Clairbout and many of his students; Rob Clayton, Stew Levin, Dave Hale, Jeff
Thorson, Chuck Sword, and others who pioneered seismic processing on Unix
in the seventies and early eighties.  

In 1986, Shuki Ronen brought this work to our Center during a
one year postdoctoral appointment at the Colorado School of Mines. 
During this time, Ronen aided Cohen in turning SU into a 
supportable and exportable product.

Chris Liner, while a student at the Center, contributed to many of the graphics
codes used in the pre-workstation (i.e, graphics terminal) age of  SU.
Liner's broad knowledge of seismology and seismic processing enabled
him to make a positive and continuing influence on the SU coding
philosophy.

Craig Artley, now at Fairfield, made major contributions to
the graphics codes while a student at CWP and continues to make
significant contributions to the general package.

Dave Hale wrote several of the ``heavy duty'' processing codes as well
as most of the core scientific and graphics libraries.  His knowledge
of good C-language coding practice helped make our package an excellent
example for applied computer scientists.

Ken Larner contributed many user interface ideas based on his
extensive knowledge of seismic processing in the ``real world.''

John Scales showed how to use  SU effectively in the classroom in his
electronic text, Theory of Seismic Imaging, Samizdat Press, 1994.
This text is available from the Samizdat press site at:  samizdat.mines.edu.

John Stockwell's involvment with SU began in 1989. He is largely 
responsible for designing the Makefile structure that makes the 
package easy to install on the majority of Unix platforms.  He has 
been the main contact for the project since the first public release 
of SU in September of 1992 (Release 18).  After
Jack Cohen's death in 1996, Stockwell assumed the role of principal
investigator of the SU project and has remained in this capacity to
the present.   The number of codes has more than tripled in the 11
years, since John Stockwell has been PI of the project. 

When Stockwell retired, he has continued to remain active in the project
and he is currently collaborating with Dominique Rousset of the University
of Pau to distribute a version of this open source package.

The project has had extensive technical help from the
worldwide SU user community.
Among those who should be singled out for mention are Tony Kocurko at
Memorial University in Newfoundland, Toralf Foerster of the 
Baltic Sea Research Institute in Warnemuende Germany, Stewart A. Levin, John
Anderson, and Joe Oravetz at Mobil Oil, Joe Dellinger at Amoco, Matthew
Rutty, Jens Hartmann, Alexander Koek, Michelle Miller
of the University of Southern California at Chico,
Berend Scheffers, and Guy Drijkoningen, Delft,
Dominique Rousset, Pau and Marc Schaming, Strasbourg,
Matt Lamont, Curtin University, Australia, 
Wenying Cai at the University of Utah, Brian Macy of Phillips Petroleum,
Robert Krueger of Terrasys, Torsten Shoenfelder of the University
of Koeln, Germany, Ian Kay of the Geological Survey of Canada, Nils Maerklin of Keil Univerity,  Richard Hobbs of the University of Durham, Balasz Nemeth,
Dr. Toshiki Watanabe of the University of Kyoto, and independent
consultant Reg Beardsley.
Our apologies in advance for undoubtedly omitting mention 
of other deserving contributors to
 SU---we promise to include you in future updates of this manual!

We especially thank Barbara McLenon for her detailed suggestions on the text
and also for her excellent design of this document, as well as the
SU pamphlets, and other materials, which we have distribute at public meetings
in the past.

\section*{In Memoriam}
Dr. Jack K. Cohen passed away in October 1996. The Seismic Unix
package exists owing to his knowledge of seismic processing,
his creativity, and his desire to make a lasting contribution
to the scientific community.  He will surely be missed by all
who had contact with him in the mathematical, geophysical, and 
SU-user communities.
\chapter*{Preface}
More than 20 years has elapsed since the first {\em SU Users' Manual\/}
was issued. From 2006 to 2016 John Stockwell has been teaching
the Seismic Data Processing Lab that is part of the course
GPGN461/561 in the Geophysics Department of the Colorado School
of Mines.

The goal of the lab is for students to process a line of seismic
data, going the processes of reading the data QC (quality control),
repairing missing shots, muting, gaining, multiple suppression
with the radon transform, wavelet estimation and signature decon,
seismic time and depth migration (poststack), model building, and finally
prestack time and prestack depth migration. 

This course has been a proving ground for SU and has led to many
modifications and extensions to the package. Many of these
extentions are in the form of shell scripts that appear in
the directory {\bf demos/Course\/}.


\chapter*{Preface from the 2002 version}
In the 7 years that have elapsed since the first version of
this manual was distributed, there have been numerous changes
in the SU package.
These changes have been a result of
both in-house activities here at CWP and, equally important,
to the many contributions of code, bug fixes, extensions,
and suggestions from SU users outside of CWP.
After reviewing these many changes and extensions, and after
much discussion with members of the worldwide SU user community,
it has become apparent that a new manual was necessary.

This new version began in preparation for a short course on
SU at the CREWES project at the University of Calgary.
Many of the items that were in the original manual,
such as information about obtaining and installing the package,
have been have been moved to appendices.
Additional sections describing the core collection of
programs have been added.
The entire manual has been expanded to include more detailed
descriptions of the codes.

The intent is that you will be able to find your way around
the package, via the help mechanisms, that you will learn to
run the programs by running the demos, and finally will be
able to see how to begin writing SU code, drawing on the
source code of the package.

\chapter{About SU}
\pagenumbering{arabic}

In 1987, Jack K. Cohen and Shuki Ronen of the Center for Wave Phenomena
(CWP) at the Colorado School of Mines (CSM) conceived a bold plan. This
plan was to create a seismic processing environment for Unix-based systems,
written in the C language, that would extend the Unix operating system to
seismic processing and research tasks. Furthermore, they intended that the
package be freely available as full source code to anyone who would want
it. 

They began with a package called SY, started by Einar Kjartansson while
he was a graduate student at Jon Claerbout's Stanford Exploration Project
(SEP) in the late seventies, and improved while he was a professor at 
the University of Utah in the eighties.  In 1984 Einar returned to 
SEP to sit in for John (and Jon's house) while Claerbout went 
for a summer vacation.
Einar introduced SY to Shuki Ronen, then a graduate student at SEP (a day
before Claerbout returned from vacation and Einar left back to Utah).
Ronen further developed SY from 1984 to 1986.   Other students at SEP
started to use it and contributed code and ideas.  SY was inspired by much
other software developed at SEP and benefited from the foundations laid by
Clarbout and many of his students; Rob Clayton, Stew Levin, Dave Hale, Jeff
Thorson, Chuck Sword, and others who pioneered seismic processing on Unix
in the seventies and early eighties.  

In 1986, when Ronen accepted a one
year visiting professorship at CSM he was encouraged by Claerbout to
disseminate whatever SEP ideas and software he cared for.  In the days
before Internet and ftp, SY was on a 9 track tape in the trunk of the car
he drove from California to Colorado.  Ronen went to CSM to replace Jack
Cohen who went on sabbatical.  Fortunately for SU, Jack's sabbatical lasted
only two months, instead of the whole year initially planned, because of the
poor health of Jack's son Dan. Upon Jack's return to CWP, he became
interested in SY.  This package was a sharp departure from the commercial
seismic processing software that was available at the time. The industry
standard at the time was to use Fortran programs on VAX-VMS based systems.

By the time Cohen and Ronen created the first version of SU in 1987 the
sponsors of CWP had already begun showing interest in the package. The
availability of Unix-based workstations combined with the influx of
Unix-literate geophysicists from the major academic institutions, shifted
the industry to using primarily Unix-based systems for research and for
processing, increasing the interest in Unix-based software, including SU.

Until September of 1992, SU was used primarily in-house at CWP. Earlier
versions of SU had been ported to CWP sponsor companies, even providing the
basis for in-house seismic processing packages developed by some of those
companies! Once SU became generally available on the Internet, it began to
be used by a much broader community. The package is used by exploration
geophysicists, earthquake seismologists, environmental engineers, software
developers and others. It is used by scientific staff in both small
geotechnical companies and major oil and gas companies, and by academics
and government researchers, both as a seismic data processing and software
development environment.

On 1 January 2018 John Stockwell retired from the Colorado School of 
Mines, but continues to maintain the package.

The 1st SU Summer School was held 8-19 July 2019 at the Unversity of Pau, under the direction of Dominique Rousset, beginning a new era in SU.

\section{What SU is}

The SU package is {\em open source software}, meaning that you may have
unrestricted use of the codes for both processing and software development,
provided that you honor the (Berkeley style Open Source) license 
that appears at the beginning of 
this manual and as the file LEGAL\_STATEMENT in the current release
of the package. (This latter file takes precedence).
The package is maintained and expanded periodically, with
each new release appearing at 3 to 6 month intervals, depending
on changes that accumulate in the official version here at CWP.
The package is distributed with the full source code, so that users
can alter and extend its capabilities.   The philosophy behind the package
is to provide both a free processing and development environment
in a proven structure that can be maintained and expanded to suit
the needs of a variety of users.

The package is not necessarily restricted to seismic processing tasks,
however.  A broad suite of wave-related processing can be done with SU,
making it a somewhat more general package than the word ``seismic'' implies.
SU is intended as an extension of the Unix operating system,
and therefore shares many characteristics of the Unix, including  Unix
flexibility and expandibility. 

The fundamental Unix philosophy is that all operating system commands
are programs run under that operating system.
The idea is that individual tasks be identified, and that small programs
be written to do those tasks, and those tasks alone.
The commands may have options that permit variations on those tasks,
but the fundamental idea is one-program, one-task.
Because Unix is a multi-tasking operating system, multiple processes
may be strung together in a cascade via ``pipes'' ($|$).

This decentralization has the advantage of minimizing overhead
by not launching single ``monster'' applications that try to do
everything, as is seen in Microsoft applications, or 
in some commercial seismic utilities, for example.

Unix has the added feature of supporting a variety of shell languages,
making Unix, itself, a meta-language. Seismic Unix benefits from all
of these attributes as well. In combination with standard Unix
programs, Seismic Unix programs may be used in shell scripts to
extend the functionality of the package.

Of course, it may be that no Unix or Seismic Unix program will fulfill
a specific task. 
This means that new code has to be written.
The availability of a standard set of source code, designed
to be readable by human beings, can expedite the process of extending
the package through the addition of new source code.

\section{What SU is not}

The SU package is not a graphical user interface driven utility
at this time, though there are several possibilities including Java
and TCL/TK scripts, which may be exploited for this purpose in the future.
Because most commercial seismic processing packages are GUI-based,
it is unavoidable that users will expect SU to be similar to these
packages.
However, this is not a fair comparison, for several reasons. 

As mentioned above, SU is an extension of the Unix operating system.
Just as there are no GUI driven Unix operating systems that give full
access to all of the capabilities of Unix from menus, it is not 
reasonable to expect full access to Seismic Unix through such an 
interface.   At most, any SU interface will give limited access
to the capabilities of the package.

SU is not a replacement for commercial seismic packages. Commercial
seismic software packages are industrial-strength utilities designed
for the express purpose of production-level seismic processing.
If you do commercial-level processing, and have dedicated, or plan
to dedicate funds to purchase one or more license of such commercial
software, then it is unlikely that you will be able to substitute
SU for the commercial utility.

However, SU can be an important adjunct to any commercial package
you use. Where commercial packages are used for 
production work, SU often has found a place as a prototyping package. 
Also, if new code needs to be written, SU can provide a starting base
for new software applications.
Indeed, the availability of seismic processing capability may encourage
non-processors to experiment with processing, non-software-developers
to experiment with software development, and non-researchers to 
engage in research.

SU is not confined to seismic applications. It may find use, both
in geophysical and more general signal processing applications. It certainly
can be useful for teaching students about ``wave related'' signal
processing and, particularly, Fourier transform properties.
This can include radar, non-seismic acoustics, and image processing,
just to name a few.

Another thing that SU is not, at least in its current version,
is a 3D package. However, it is not, expressly a 2D package, either,
as there are numerous filtering and trace manipulation tasks that
are the same in 2D as in 3D. It is likely, however, that there will
be 3D applications in future releases of SU. A single 3D migration
code appeared first in Release~32, which will hopefully set the
stage for new developments.

\section{Obtaining and Installing SU}

Because the coding standards of SU stress portability, the
package will install on any system that is running some form
of the Unix operating system, and has a decent version of ``make''
and an ANSI C compiler.
The programs GNU Make and GCC may be substituted for these respective
programs on most systems.\footnote{Remarkably, a software package called
{\bf CYGWIN32}, recently released by the GNU project, provides enough
Unix-like functionality, so that SU may be ported to Windows NT, 
with no changes to the SU source code or Makefiles!}

New releases of SU are issued at 3 to 6 month intervals, depending
upon the accumulation of changes in the home version at CWP.
Old releases are not supported. However, if materials appear to break
between releases, please contact me (John Stockwell) by email
so that we can fix the problem.

Instructions for obtaining and installing SU may be found in
Appendix~\ref{app:A} of this manual.

\chapter{Help facilities}

Like the Unix operating system, Seismic Unix may be thought of
as a language (or meta-language). As with any language, a certain 
amount of vocabulary must be mastered before useful operations may be
performed.
Because the SU contains many programs,
there must be a ``dictionary'' to permit the inevitable
questions about vocabulary can be answered.
It is intended that this manual be the beginning of such a dictionary.

SU does not have ``man pages,'' in the same way that
Unix does, but it does have equivalent internal documentation
mechanisms.
There is a general level help utilities which give an
overview of what is available.
For information about specific aspects of a particular code,
the majority of the programs contain a {\bf selfdoc}---a
self-documentation paragraph, which will appear when the name
of the program is typed on the commandline, with no options.

(In all of the examples that follow, the percent sign ``\%''
indicates a Unix commandline prompt, and is not typed
as part of the command.)

The following tools provide internal documentation at various
levels of detail for the main programs, shell scripts, and
library functions in the package:

\begin{itemize}
\item SUHELP - list the CWP/SU programs and shells
\item SUNAME - get name line from self-docs and location of the source code
\item The selfdoc - is an internal documentation utility which exists
in the majority of executable mains and shell scripts. The selfdoc
is seen by typing the name of the program or shell script on the commandline
with no arguments, and without redirection of input or output via
pipes $|$ or Unix redirects $>$ $<$,
For non-executables (library routines) and for programs without the
selfdoc feature, there is a dummy selfdoc included which provides a
database of information about those items, as well,
\item SUDOC - get DOC listing for code 
\item SUFIND - get info from self-docs 
\item GENDOCS - generate complete list of selfdocs in latex form 
\item suhelp.html - is an HTML global overview of SU programs by subject
matter,
\item SUKEYWORD -- guide to SU keywords in segy.h 
\end{itemize}

This chapter discusses each of these utilities, with the intent
of showing the reader how to get from the most general to the
most specific information about SU programs.

\section{SUHELP - List the Executable Programs and Shell Scripts} 

For a general listing of the contents of SU, which includes each
executable (that is, each main program and shell script) in the package
type:
{\small\begin{verbatim}
% suhelp

CWP PROGRAMS: (no self-documentation)
ctrlstrip       fcat            maxints         t  
downfort        isatty          pause           upfort  

PAR PROGRAMS: (programs with self-documentation)
a2b             kaperture       resamp          transp          vtlvz  
b2a             makevel         smooth2         unif2           wkbj  
farith          mkparfile       smoothint2      unisam  
ftnstrip        prplot          subset          unisam2  
h2b             recast          swapbytes       velconv  


press return key to continue
....
\end{verbatim}}\noindent
Please type {\bf suhelp\/} or see Appendix~{\ref{app:B} for the full text.

Another useful command sequence is:
\begin{verbatim}
% suhelp | lpr
\end{verbatim} \noindent
which will the output from {\bf suhelp\/} to the local print.


\section{SUNAME - Lists the Name and Short Description of Every Item in SU}

A more complete listing of the contents of the CWP/SU package
may be obtained by typing
{\small\begin{verbatim}
% suname
 -----  CWP Free Programs -----   
CWPROOT=/usr/local/cwp

Mains: 

In CWPROOT/src/cwp/main:
* CTRLSTRIP - Strip non-graphic characters
* DOWNFORT - change Fortran programs to lower case, preserving strings
* FCAT - fast cat with 1 read per file 
* ISATTY - pass on return from isatty(2)
* MAXINTS - Compute maximum and minimum sizes for integer types 
* PAUSE - prompt and wait for user signal to continue
* T - time and date for non-military types
* UPFORT - change Fortran programs to upper case, preserving strings

In CWPROOT/src/par/main:
A2B - convert ascii floats to binary 				
A2I - convert Ascii to binary Integers			
ADDRVL3D - Add a random velocity layer (RVL) to a gridded             
B2A - convert binary floats to ascii				
CELLAUTO - Two-dimensional CELLular AUTOmata			  	
char* sdoc[] = {
CSHOTPLOT - convert CSHOT data to files for CWP graphers		
DZDV - determine depth derivative with respect to the velocity	",  
FARITH - File ARITHmetic -- perform simple arithmetic with binary files
FLOAT2IBM - convert native binary FLOATS to IBM tape FLOATS	
FTNSTRIP - convert a file of binary data plus record delimiters created
FTNUNSTRIP - convert C binary floats to Fortran style floats	
GRM - Generalized Reciprocal refraction analysis for a single layer	
H2B - convert 8 bit hexidecimal floats to binary		
HTI2STIFF - convert HTI parameters alpha, beta, d(V), e(V), gamma	
 HUDSON - compute  effective parameters of anisotropic solids	        
I2A - convert binary integers to ascii				
IBM2FLOAT - convert IBM tape FLOATS to native binary FLOATS	
KAPERTURE - generate the k domain of a line scatterer for a seismic array
LINRORT - linearized P-P, P-S1 and P-S2 reflection coefficients 	

MAKEVEL - MAKE a VELocity function v(x,y,z)				
MKPARFILE - convert ascii to par file format 				
MRAFXZWT - Multi-Resolution Analysis of a function F(X,Z) by Wavelet	
PDFHISTOGRAM - generate a HISTOGRAM of the Probability Density function
PRPLOT - PRinter PLOT of 1-D arrays f(x1) from a 2-D function f(x1,x2)
RANDVEL3D - Add a random velocity layer (RVL) to a gridded             
RAYT2D - traveltime Tables calculated by 2D paraxial RAY tracing	
RAYT2DAN -- P- and SV-wave raytracing in 2D anisotropic media		
RECAST - RECAST data type (convert from one data type to another)	
REFREALAZIHTI -  REAL AZImuthal REFL/transm coeff for HTI media 	
REFREALVTI -  REAL REFL/transm coeff for VTI media and symmetry-axis	
REGRID3 - REwrite a [ni3][ni2][ni1] GRID to a [no3][no2][no1] 3-D grid
RESAMP - RESAMPle the 1st dimension of a 2-dimensional function f(x1,x2)

SMOOTH2 --- SMOOTH a uniformly sampled 2d array of data, within a user-
SMOOTH3D - 3D grid velocity SMOOTHing by the damped least squares	
SMOOTHINT2 --- SMOOTH non-uniformly sampled INTerfaces, via the damped
STIFF2VEL - Transforms 2D elastic stiffnesses to (vp,vs,epsilon,delta) 
SUBSET - select a SUBSET of the samples from a 3-dimensional file	
SWAPBYTES - SWAP the BYTES of various  data types			
THOM2HTI - Convert Thompson parameters V_p0, V_s0, eps, gamma,	
THOM2STIFF - convert Thomsen's parameters into (density normalized)	
TRANSP - TRANSPose an n1 by n2 element matrix				
TRANSP3D - TRANSPose an n1 by n2 by n3 element matrix			
TVNMOQC - Check tnmo-vnmo pairs; create t-v column files           
UNIF2 - generate a 2-D UNIFormly sampled velocity profile from a layered
UNIF2ANISO - generate a 2-D UNIFormly sampled profile of elastic	
UNIF2TI2 - generate a 2-D UNIFormly sampled profile of stiffness 	
UNISAM - UNIformly SAMple a function y(x) specified as x,y pairs	
UNISAM2 - UNIformly SAMple a 2-D function f(x1,x2)			
UTMCONV - CONVert longitude and latitude to UTM, and vice versa       
VEL2STIFF - Transforms VELocities, densities, and Thomsen or Sayers   
VELCONV - VELocity CONVersion					
VELPERT - estimate velocity parameter perturbation from covariance 	
VELPERTAN - Velocity PERTerbation analysis in ANisotropic media to    

VTLVZ -- Velocity as function of Time for Linear V(Z);		
WKBJ - Compute WKBJ ray theoretic parameters, via finite differencing	
XY2Z - converts (X,Y)-pairs to spike Z values on a uniform grid	
Z2XYZ - convert binary floats representing Z-values to ascii	

In CWPROOT/src/psplot/main:
PSBBOX - change BoundingBOX of existing PostScript file	
PSCONTOUR - PostScript CONTOURing of a two-dimensional function f(x1,x2)
PSCUBE - PostScript image plot with Legend of a data CUBE       
PSCCONTOUR - PostScript Contour plot of a data CUBE		        
PSEPSI - add an EPSI formatted preview bitmap to an EPS file		
PSGRAPH - PostScript GRAPHer						
PSIMAGE - PostScript IMAGE plot of a uniformly-sampled function f(x1,x2)
PSLABEL - output PostScript file consisting of a single TEXT string	
PSMANAGER - printer MANAGER for HP 4MV and HP 5Si Mx Laserjet 
PSMERGE - MERGE PostScript files					
PSMOVIE - PostScript MOVIE plot of a uniformly-sampled function f(x1,x2,x3)
PSWIGB - PostScript WIGgle-trace plot of f(x1,x2) via Bitmap		
PSWIGP - PostScript WIGgle-trace plot of f(x1,x2) via Polygons	

In CWPROOT/src/xplot/main:
* LCMAP - List Color Map of root window of default screen 
* LPROP - List PROPerties of root window of default screen of display 
* SCMAP - set default standard color map (RGB_DEFAULT_MAP)
XCONTOUR - X CONTOUR plot of f(x1,x2) via vector plot call		
* XESPB - X windows display of Encapsulated PostScript as a single Bitmap
* XEPSP - X windows display of Encapsulated PostScript
XIMAGE - X IMAGE plot of a uniformly-sampled function f(x1,x2)     	
XPICKER - X wiggle-trace plot of f(x1,x2) via Bitmap with PICKing	
* XPSP - Display conforming PostScript in an X-window
XWIGB - X WIGgle-trace plot of f(x1,x2) via Bitmap			

In CWPROOT/src/Xtcwp/main:
XGRAPH - X GRAPHer							
XMOVIE - image one or more frames of a uniformly sampled function f(x1,x2)
XRECTS - plot rectangles on a two-dimensional grid			

In CWPROOT/src/Xmcwp/main:
* FFTLAB - Motif-X based graphical 1D Fourier Transform

In CWPROOT/src/su/graphics/psplot:
SUPSCONTOUR - PostScript CONTOUR plot of a segy data set		
SUPSCUBE - PostScript CUBE plot of a segy data set			
SUPSCUBECONTOUR - PostScript CUBE plot of a segy data set		
SUPSGRAPH - PostScript GRAPH plot of a segy data set			
SUPSIMAGE - PostScript IMAGE plot of a segy data set			
SUPSMAX - PostScript of the MAX, min, or absolute max value on each trace
SUPSMOVIE - PostScript MOVIE plot of a segy data set			
SUPSWIGB - PostScript Bit-mapped WIGgle plot of a segy data set	
SUPSWIGP - PostScript Polygon-filled WIGgle plot of a segy data set	

In CWPROOT/src/su/main/amplitudes:
suai2r <stdin >sdout 						
SUCENTSAMP - CENTRoid SAMPle seismic traces			
SUDIPDIVCOR - Dip-dependent Divergence (spreading) correction	
SUDIVCOR - Divergence (spreading) correction				
SUGAIN - apply various types of gain				  	
SUNAN - remove NaNs & Infs from the input stream		
SUNORMALIZE - Trace NORMALIZation by rms, max, or median       ", 
SUPGC   -   Programmed Gain Control--apply agc like function	
sur2ai <stdin >sdout 						
SUWEIGHT - weight traces by header parameter, such as offset		
SUZERO -- zero-out (or set constant) data within a time window	

In CWPROOT/src/su/main/attributes_parameter_estimation:
SUATTRIBUTES - instantaneous trace ATTRIBUTES 			

SUHISTOGRAM - create histogram of input amplitudes		
SUMAX - get trace by trace local/global maxima, minima, or absolute maximum
SUMEAN - get the mean values of data traces				",	
SUQUANTILE - display some quantiles or ranks of a data set            

In CWPROOT/src/su/main/convolution_correlation:
SUACOR - auto-correlation						
SUACORFRAC -- general FRACtional Auto-CORrelation/convolution		
SUCONV - convolution with user-supplied filter			
SUREFCON -  Convolution of user-supplied Forward and Reverse		
SUXCOR - correlation with user-supplied filter			

In CWPROOT/src/su/main/data_compression:
DCTCOMP - Compression by Discrete Cosine Transform			
SUPACK1 - pack segy trace data into chars			
SUPACK2 - pack segy trace data into 2 byte shorts		
SUUNPACK1 - unpack segy trace data from chars to floats	
SUUNPACK2 - unpack segy trace data from shorts to floats	

In CWPROOT/src/su/main/data_conversion:
DT1TOSU - Convert ground-penetrating radar data in the	
SEGYCLEAN - zero out unassigned portion of header		
SEGYHDRMOD - replace the text header on a SEGY file		
SEGYHDRS - make SEG-Y ascii and binary headers for segywrite		
SEGYREAD - read an SEG-Y tape						
SEGYSCAN -- SCANs SEGY file trace headers for min-max in  several	
SEGYWRITE - write an SEG-Y tape					
SETBHED - SET the fields in a SEGY Binary tape HEaDer file, as would be
SUASCII - print non zero header values and data in various formats    
SUINTVEL - convert stacking velocity model to interval velocity model	
SUOLDTONEW - convert existing su data to xdr format		
SUSTKVEL - convert constant dip layer interval velocity model to the	
SUSWAPBYTES - SWAP the BYTES in SU data to convert data from big endian
SWAPBHED - SWAP the BYTES in a SEGY Binary tape HEaDer file		

In CWPROOT/src/su/main/datuming:
SUDATUMFD - 2D zero-offset Finite Difference acoustic wave-equation	
SUDATUMK2DR - Kirchhoff datuming of receivers for 2D prestack data	
SUDATUMK2DS - Kirchhoff datuming of sources for 2D prestack data	
 SUKDMDCR - 2.5D datuming of receivers for prestack, common source    
 SUKDMDCS - 2.5D datuming of sources for prestack common receiver 	

In CWPROOT/src/su/main/decon_shaping:
SUCDDECON - DECONvolution with user-supplied filter by straightforward
SUFXDECON - random noise attenuation by FX-DECONvolution              
SUPEF - Wiener (least squares) predictive error filtering		
SUPHIDECON - PHase Inversion Deconvolution				
SUSHAPE - Wiener shaping filter					

In CWPROOT/src/su/main/dip_moveout:
SUDMOFK - DMO via F-K domain (log-stretch) method for common-offset gathers
SUDMOFKCW - converted-wave DMO via F-K domain (log-stretch) method for
SUDMOTIVZ - DMO for Transeversely Isotropic V(Z) media for common-offset
SUDMOTX - DMO via T-X domain (Kirchhoff) method for common-offset gathers
SUDMOVZ - DMO for V(Z) media for common-offset gathers		
SUTIHALEDMO - TI Hale Dip MoveOut (based on Hale's PhD thesis)	

In CWPROOT/src/su/main/filters:
SUBFILT - apply Butterworth bandpass filter 			
SUCCFILT -  FX domain Correlation Coefficient FILTER			
SUDIPFILT - DIP--or better--SLOPE Filter in f-k domain	
SUFILTER - applies a zero-phase, sine-squared tapered filter		
SUFRAC -- take general (fractional) time derivative or integral of	
SUFWATRIM - FX domain Alpha TRIM					
SUK1K2FILTER - symmetric box-like K-domain filter defined by the	
SUKFILTER - radially symmetric K-domain, sin^2-tapered, polygonal	
SUKFRAC - apply FRACtional powers of i|k| to data, with phase shift 
SULFAF -  Low frequency array forming					", 
SUMEDIAN - MEDIAN filter about a user-defined polygonal curve with	
SUPHASE - PHASE manipulation by linear transformation			
SUSMGAUSS2 --- SMOOTH a uniformly sampled 2d array of velocities	
SUSVDFILT - SVD (Eigen) filter					
SUTVBAND - time-variant bandpass filter (sine-squared taper)  

In CWPROOT/src/su/main/headers:
BHEDTOPAR - convert a Binary tape HEaDer file to PAR file format	
SU3DCHART - plot x-midpoints vs. y-midpoints for 3-D data	
SUABSHW - replace header key word by its absolute value	
SUADDHEAD - put headers on bare traces and set the tracl and ns fields
SUAHW - Assign Header Word using another header word			
SUAZIMUTH - compute trace AZIMUTH, offset, and midpoint coordinates    
SUCDPBIN - Compute CDP bin number					
SUCHART - prepare data for x vs. y plot			
SUCHW - Change Header Word using one or two header word fields	
SUCLIPHEAD - Clip header values					
SUCOUNTKEY - COUNT the number of unique values for a given KEYword.	
SUDUMPTRACE - print selected header values and data.              
SUEDIT - examine segy diskfiles and edit headers			
SUGEOM - Fill up geometry in trace headers.                              
SUGETHW - sugethw writes the values of the selected key words		
SUHTMATH - do unary arithmetic operation on segy traces with 	
SUKEYCOUNT - sukeycount writes a count of a selected key    
SULCTHW - Linear Coordinate Transformation of Header Words		
SULHEAD - Load information from an ascii column file into HEADERS based
SUPASTE - paste existing SU headers on existing binary data	
surandhw - set header word to random variable 		
SURANGE - get max and min values for non-zero header entries	
SUSEHW - Set the value the Header Word denoting trace number within	
SUSHW - Set one or more Header Words using trace number, mod and	
SUSTRIP - remove the SEGY headers from the traces		
SUTRCOUNT - SU program to count the TRaces in infile		
SUUTM - UTM projection of longitude and latitude in SU trace headers  
SUXEDIT - examine segy diskfiles and edit headers			

In CWPROOT/src/su/main/interp_extrap:
SUINTERP - interpolate traces using automatic event picking		
SUINTERPFOWLER - interpolate output image from constant velocity panels
SUOCEXT - smaller Offset EXTrapolation via Offset Continuation        

In CWPROOT/src/su/main/migration_inversion:
SUGAZMIGQ - SU version of Jeno GAZDAG's phase-shift migration 	
SUINVXZCO - Seismic INVersion of Common Offset data for a smooth 	
SUINVZCO3D - Seismic INVersion of Common Offset data with V(Z) velocity
SUKDMIG2D - Kirchhoff Depth Migration of 2D poststack/prestack data	
	"SUKDMIG2D - Kirchhoff Depth Migration of 2D poststack/prestack data	
SUKDMIG3D - Kirchhoff Depth Migration of 3D poststack/prestack data	
SUKTMIG2D - prestack time migration of a common-offset	
SUMIGFD - 45-90 degree Finite difference depth migration for		
SUMIGFFD - Fourier finite difference depth migration for		
SUMIGGBZO - MIGration via Gaussian Beams of Zero-Offset SU data	
SUMIGGBZOAN - MIGration via Gaussian beams ANisotropic media (P-wave)	
SUMIGPREFD --- The 2-D prestack common-shot 45-90 degree		
SUMIGPREFFD - The 2-D prestack common-shot Fourier finite-difference	
char *sdoc[] = {
SUMIGPRESP - The 2-D prestack common-shot split-step Fourier		", 
SUMIGPS - MIGration by Phase Shift with turning rays			
SUMIGPSPI - Gazdag's phase-shift plus interpolation depth migration   
SUMIGPSTI - MIGration by Phase Shift for TI media with turning rays	
SUMIGSPLIT - Split-step depth migration for zero-offset data.         
SUMIGTK - MIGration via T-K domain method for common-midpoint stacked data
SUMIGTOPO2D - Kirchhoff Depth Migration of 2D postack/prestack data	
SUSTOLT - Stolt migration for stacked data or common-offset gathers	
SUTIFOWLER   VTI constant velocity prestack time migration		

In CWPROOT/src/su/main/multicomponent:
SUALFORD - trace by trace Alford Rotation of shear wave data volumes  
SUEIPOFI - EIgenimage (SVD) based POlarization FIlter for             
SUHROT - Horizontal ROTation of three-component data			
SULTT - trace by trace, sample by sample, rotation of shear wave data 
SUPOFILT - POlarization FILTer for three-component data               
SUPOLAR - POLarization analysis of three-component data               

In CWPROOT/src/su/main/noise:
SUADDNOISE - add noise to traces					
SUGROLL - Ground roll supression using Karhunen-Loeve transform	
SUHARLAN - signal-noise separation by the invertible linear		
SUJITTER - Add random time shifts to seismic traces			

In CWPROOT/src/su/main/operations:
SUFLIP - flip a data set in various ways			
SUFWMIX -  FX domain multidimensional Weighted Mix			
SUMATH - do math operation on su data 		
SUMIX - compute weighted moving average (trace MIX) on a panel	
SUOP - do unary arithmetic operation on segys 		
SUOP2 - do a binary operation on two data sets			
SUPERMUTE - permute or transpose a 3d datacube	 		
SURECMO - compensate for the continuously moving streamer in marine   
SUSIMAT - Correlation similarity matrix for two traces.		
SUVCAT -  append one data set to another, with or without an  ", 
SUVLENGTH - Adjust variable length traces to common length   	

In CWPROOT/src/su/main/picking:
SUFBPICKW - First break auto picker				
SUFNZERO - get Time of First Non-ZERO sample by trace              
SUPICKAMP - pick amplitudes within user defined and resampled window	

In CWPROOT/src/su/main/stacking:
SUCVS4FOWLER --compute constant velocity stacks for input to Fowler codes
SUDIVSTACK -  Diversity Stacking using either average power or peak   
SUPWS - Phase stack or phase-weighted stack (PWS) of adjacent traces	
SURECIP - sum opposing offsets in prepared data (see below)	
SUSTACK - stack adjacent traces having the same key header word

In CWPROOT/src/su/main/statics:
SUADDSTATICS - ADD random STATICS on seismic data			
SURANDSTAT - Add RANDom time shifts STATIC errors to seismic traces	
SURESSTAT - Surface consistent source and receiver statics calculation
SUSTATIC - Elevation static corrections, apply corrections from	
SUSTATICB - Elevation static corrections, apply corrections from	
SUSTATICRRS - Elevation STATIC corrections, apply corrections from	

In CWPROOT/src/su/main/stretching_moveout_resamp:
SUDLMO -- Dynamic Linear Move Out Correction for Surface Waves  	
SUILOG -- time axis inverse log-stretch of seismic traces	
SULOG -- time axis log-stretch of seismic traces		
SUNMO - NMO for an arbitrary velocity function of time and CDP	     
SUNMO_a - NMO for an arbitrary velocity function of time and CDP with	     
SUREDUCE - convert traces to display in reduced time		", 
SURESAMP - Resample in time                                       
SUSHIFT - shifted/windowed traces in time				
SUTAUPNMO - NMO for an arbitrary velocity function of tau and CDP	
SUTSQ -- time axis time-squared stretch of seismic traces	
SUTTOZ - resample from time to depth					
SUZTOT - resample from depth to time					

In CWPROOT/src/su/main/supromax:
SUGET  - Connect SU program to file descriptor for input stream.	
SUPUT - Connect SU program to file descriptor for output stream.	

In CWPROOT/src/su/main/synthetics_waveforms_testpatterns:

SUDGWAVEFORM - make Gaussian derivative waveform in SU format		
SUEA2DF - SU version of (an)elastic anisotropic 2D finite difference 	
SUFCTANISMOD - Flux-Corrected Transport correction applied to the 2D
SUFDMOD1 - Finite difference modelling (1-D 1rst order) for the	
SUFDMOD2 - Finite-Difference MODeling (2nd order) for acoustic wave equation
SUFDMOD2_PML - Finite-Difference MODeling (2nd order) for acoustic wave
SUGOUPILLAUD - calculate 1D impulse response of	 		
SUGOUPILLAUDPO - calculate Primaries-Only impulse response of a lossless
SUIMP2D - generate shot records for a line scatterer	
SUIMP3D - generate inplane shot records for a point 	
SUIMPEDANCE - Convert reflection coefficients to impedances.  
SUKDSYN2D - Kirchhoff Depth SYNthesis of 2D seismic data from a	
SUNHMOSPIKE - generates SPIKE test data set with a choice of several   
SUNULL - create null (all zeroes) traces	 		
SUPLANE - create common offset data file with up to 3 planes	
SURANDSPIKE - make a small data set of RANDom SPIKEs 		
SUREMAC2D - Acoustic 2D Fourier method modeling with high accuracy     
SUREMEL2DAN - Elastic anisotropic 2D Fourier method modeling with high 
SUSPIKE - make a small spike data set 			
SUSYNCZ - SYNthetic seismograms for piecewise constant V(Z) function	
SUSYNLV - SYNthetic seismograms for Linear Velocity function		
SUSYNLVCW - SYNthetic seismograms for Linear Velocity function	
SUSYNLVFTI - SYNthetic seismograms for Linear Velocity function in a  
SUSYNVXZ - SYNthetic seismograms of common offset V(X,Z) media via	
SUSYNVXZCS - SYNthetic seismograms of common shot in V(X,Z) media via	
SUVIBRO - Generates a Vibroseis sweep (linear, linear-segment,
SUWAVEFORM - generate a seismic wavelet				

In CWPROOT/src/su/main/tapering:
SUGAUSSTAPER - Multiply traces with gaussian taper		
SURAMP - Linearly taper the start and/or end of traces to zero.	
SUTAPER - Taper the edge traces of a data panel to zero.	
SUTXTAPER - TAPER in (X,T) the edges of a data panel to zero.	

In CWPROOT/src/su/main/transforms:
SUAMP - output amp, phase, real or imag trace from			
SUANALYTIC - use the Hilbert transform to generate an ANALYTIC	
SUCCEPSTRUM - Compute the complex CEPSTRUM of a seismic trace 	"
SUCCWT - Complex continuous wavelet transform of seismic traces	
SUCEPSTRUM - transform to the CEPSTRal domain				
SUCLOGFFT - fft real time traces to complex log frequency domain traces
SUCWT - generates Continous Wavelet Transform amplitude, regularity	
SUFFT - fft real time traces to complex frequency traces		
SUGABOR -  Outputs a time-frequency representation of seismic data via
SUHILB - Hilbert transform					
SUICEPSTRUM - fft of complex log frequency traces to real time traces
SUICLOGFFT - fft of complex log frequency traces to real time traces
SUIFFT - fft complex frequency traces to real time traces	
SUMINPHASE - convert input to minimum phase				
SUPHASEVEL - Multi-mode PHASE VELocity dispersion map computed
SURADON - compute forward or reverse Radon transform or remove multiples
SUSLOWFT - Fourier Transforms by a (SLOW) DFT algorithm (Not an FFT)
SUSLOWIFT - Fourier Transforms by (SLOW) DFT algorithm (Not an FFT)
SUSPECFK - F-K Fourier SPECtrum of data set			
SUSPECFX - Fourier SPECtrum (T -> F) of traces 		
SUSPECK1K2 - 2D (K1,K2) Fourier SPECtrum of (x1,x2) data set		
SUST -  Outputs a time-frequency representation of seismic data via
SUTAUP - forward and inverse T-X and F-K global slant stacks		
SUWFFT - Weighted amplitude FFT with spectrum flattening 0->Nyquist	
SUZEROPHASE - convert input to zero phase equivalent			

In CWPROOT/src/su/main/velocity_analysis:
SURELAN - compute residual-moveout semblance for cdp gathers based	
SURELANAN - REsiduaL-moveout semblance ANalysis for ANisotropic media	
 SUTIVEL -  SU Transversely Isotropic velocity table builder		
SUVEL2DF - compute stacking VELocity semblance for a single time in   
SUVELAN - compute stacking velocity semblance for cdp gathers		     
SUVELAN_NCCS - compute stacking VELocity panel for cdp gathers	     
SUVELAN_NSEL - compute stacking VELocity panel for cdp gathers	     
SUVELAN_UCCS - compute stacking VELocity panel for cdp gathers	     
SUVELAN_USEL - compute stacking velocity panel for cdp gathers	     

In CWPROOT/src/su/main/well_logs:
LAS2SU - convert las2 format well log curves to su traces	
SUBACKUS - calculate Thomsen anisotropy parameters from 	
SUBACKUSH - calculate Thomsen anisotropy parameters from 	
SUGASSMAN - Model reflectivity change with rock/fluid properties	
SULPRIME - find appropriate Backus average length for  	
SUWELLRF - convert WELL log depth, velocity, density data into a	

In CWPROOT/src/su/main/windowing_sorting_muting:
SUCOMMAND - pipe traces having the same key header word to command	
SUGETGTHR - Gets su files from a directory and put them               
SUGPRFB - SU program to remove First Breaks from GPR data		
SUKILL - zero out traces					
SUMIXGATHERS - mix two gathers					
SUMUTE - MUTE above (or below) a user-defined polygonal curve with	", 
SUPAD - Pad zero traces						
SUPUTGTHR - split the stdout flow to gathers on the bases of given	
SUSORT - sort on any segy header keywords			
SUSORTY - make a small 2-D common shot off-end  		
SUSPLIT - Split traces into different output files by keyword value	
SUWIND - window traces by key word					
SUWINDPOLY - WINDow data to extract traces on or within a respective	

In CWPROOT/src/su/graphics/psplot:
SUPSCONTOUR - PostScript CONTOUR plot of a segy data set		
SUPSCUBE - PostScript CUBE plot of a segy data set			
SUPSCUBECONTOUR - PostScript CUBE plot of a segy data set		
SUPSGRAPH - PostScript GRAPH plot of a segy data set			
SUPSIMAGE - PostScript IMAGE plot of a segy data set			
SUPSMAX - PostScript of the MAX, min, or absolute max value on each trace
SUPSMOVIE - PostScript MOVIE plot of a segy data set			
SUPSWIGB - PostScript Bit-mapped WIGgle plot of a segy data set	
SUPSWIGP - PostScript Polygon-filled WIGgle plot of a segy data set	

In CWPROOT/src/su/graphics/xplot:
SUXCONTOUR - X CONTOUR plot of Seismic UNIX tracefile via vector plot call
SUXGRAPH - X-windows GRAPH plot of a segy data set			
SUXIMAGE - X-windows IMAGE plot of a segy data set	                
SUXMAX - X-windows graph of the MAX, min, or absolute max value on	
SUXMOVIE - X MOVIE plot of a 2D or 3D segy data set 			
SUXPICKER - X-windows  WIGgle plot PICKER of a segy data set		
SUXWIGB - X-windows Bit-mapped WIGgle plot of a segy data set		

In CWPROOT/src/tri/main:
GBBEAM - Gaussian beam synthetic seismograms for a sloth model 	
NORMRAY - dynamic ray tracing for normal incidence rays in a sloth model
TRI2UNI - convert a TRIangulated model to UNIformly sampled model	
TRIMODEL - make a triangulated sloth (1/velocity^2) model                  		
TRIRAY - dynamic RAY tracing for a TRIangulated sloth model		
TRISEIS - Gaussian beam synthetic seismograms for a sloth model	
UNI2TRI - convert UNIformly sampled model to a TRIangulated model	

In CWPROOT/src/xtri:
SXPLOT - X Window plot a triangulated sloth function s(x1,x2)		

In CWPROOT/src/tri/graphics/psplot:
SPSPLOT - plot a triangulated sloth function s(x,z) via PostScript	

In CWPROOT/src/comp/dct/main:
DCTCOMP - Compression by Discrete Cosine Transform			
DCTUNCOMP - Discrete Cosine Transform Uncompression 			
ENTROPY - compute the ENTROPY of a signal			
WPTCOMP - Compression by Wavelet Packet Compression 			
WPTUNCOMP - Uncompress  WPT compressed data				
WTCOMP - Compression by Wavelet Transform				
WTUNCOMP - UNCOMPression of WT compressed data			

In CWPROOT/src/comp/dwpt/1d/main:
WPC1COMP2 --- COMPress a 2D seismic section trace-by-trace using 	
WPC1UNCOMP2 --- UNCOMPRESS a 2D seismic section, which has been	

In CWPROOT/src/comp/dwpt/2d/main:
WPCCOMPRESS --- COMPRESS a 2D section using Wavelet Packets		
WPCUNCOMPRESS --- UNCOMPRESS a 2D section 				

Shells: 

In CWPROOT/src/cwp/shell:
# Grep  - recursively call egrep in pwd
# ARGV - give examples of dereferencing char **argv
# COPYRIGHT - insert CSM COPYRIGHT lines at top of files in working directory
# CPALL , RCPALL - for local and remote directory tree/file transfer
# CWPFIND - look for files with patterns in CWPROOT/src/cwp/lib
# DIRTREE - show DIRectory TREE
# FILETYPE - list all files of given type
# NEWCASE - Changes the case of all the filenames in a directory, dir
# OVERWRITE - copy stdin to stdout after EOF
# PRECEDENCE - give table of C precedences from Kernighan and Ritchie
# REPLACE - REPLACE string1 with string2  in files
# THIS_YEAR - print the current year
# TIME_NOW - prints time in ZULU format with no spaces 
# TODAYS_DATE - prints today's date in ZULU format with no spaces 
# USERNAMES - get list of all login names
# VARLIST - list variables used in a Fortran program
# WEEKDAY - prints today's WEEKDAY designation
# ZAP - kill processes by name

In CWPROOT/src/par/shell:
# GENDOCS - generate complete list of selfdocs in latex form
# STRIPTOTXT -  put files from $CWPROOT/src/doc/Stripped into a new
# UPDATEDOC - put self-docs in ../doc/Stripped and ../doc/Headers
# UPDATEDOCALL - put self-docs in ../doc/Stripped
# UPDATEHEAD - update ../doc/Headers/Headers.all

In CWPROOT/src/psplot/shell:
# MERGE2 - put 2 standard size PostScript figures on one page
# MERGE4 - put 4 standard size PostScript plots on one page

In CWPROOT/src/su/shell:
# LOOKPAR - show getpar lines in SU code with defines evaluated
# MAXDIFF - find absolute maximum difference in two segy data sets
# RAMAC2SU - converts RAMAC GPR files to su format with a nominal geometry
# RECIP - sum opposing (reciprocal) offsets in cdp sorted data
# RMAXDIFF - find percentage maximum difference in two segy data sets
# SUAGC - perform agc on SU data 
# SUBAND - Trapezoid-like Sin squared tapered Bandpass filter via  SUFILTER
# SUDIFF, SUSUM, SUPROD, SUQUO, SUPTDIFF, SUPTSUM,
# SUDOC - get DOC listing for code
# SUENV - Instantaneous amplitude, frequency, and phase via: SUATTRIBUTES
# SUFIND - get info from self-docs
# SUFIND - get info from self-docs
# SUGENDOCS - generate complete list of selfdocs in latex form
# SUHELP - list the CWP/SU programs and shells
# SUKEYWORD -- guide to SU keywords in segy.h
# SUNAME - get name line from self-docs
# UNGLITCH - zonk outliers in data

Libs: 

In CWPROOT/src/cwp/lib:
ABEL - Functions to compute the discrete ABEL transform:
AIRY - Approximate the Airy functions  Ai(x), Bi(x) and their respective
ALLOC - Allocate and free multi-dimensional arrays
ANTIALIAS - Butterworth anti-aliasing filter
AXB - Functions to solve a linear system of equations Ax=b by LU
BIGMATRIX - Functions to manipulate 2-dimensional matrices that are too big 
BUTTERWORTH - Functions to design and apply Butterworth filters:
COMPLEX - Functions to manipulate complex numbers
COMPLEXD - Functions to manipulate double-precision complex numbers
COMPLEXF  - Subroutines to perform operations on complex numbers.
COMPLEXFD  - Subroutines to perform operations on double complex numbers.
Conjugate Gradient routines -
CONVOLUTION - Compute z = x convolved with y
CUBICSPLINE - Functions to compute CUBIC SPLINE interpolation coefficients
DBLAS - Double precision Basic Linear Algebra subroutines
DGE - Double precision Gaussian Elimination matrix subroutines  adapted
DIFFERENTIATE - simple DIFFERENTIATOR codes
PFAFFT - Functions to perform Prime Factor (PFA) FFT's, in place
*CWP_Exit - exit subroutine for CWP/SU codes
FRANNOR - functions to generate a pseudo-random float normally distributed
FRANUNI - Functions to generate a pseudo-random float uniformly distributed
HANKEL - Functions to compute discrete Hankel transforms
Hartley - routines for fast Hartley transform
HILBERT - Compute Hilbert transform y of x
HOLBERG1D - Compute coefficients of Holberg's 1st derivative filter
INTCUB - evaluate y(x), y'(x), y''(x), ... via piecewise cubic interpolation
INTL2B - bilinear interpolation of a 2-D array of bytes
INTLIN - evaluate y(x) via linear interpolation of y(x[0]), y(x[1]), ...
INTLINC - evaluate complex y(x) via linear interpolation of y(x[0]), y(x[1]), ...
INTLIRR2B - bilinear interpolation of a 2-D array of bytes
INTSINC8 - Functions to interpolate uniformly-sampled data via 8-coeff. sinc
INTTABLE8 -  Interpolation of a uniformly-sampled complex function y(x)
LINEAR_REGRESSION - Compute linear regression of (y1,y2,...,yn) against 
maxmin - subroutines that pertain to maximum and minimum values
MKDIFF - Make an n-th order DIFFerentiator via Taylor's series method.
MKHDIFF - Compute filter approximating the bandlimited HalF-DIFFerentiator.
MKSINC - Compute least-squares optimal sinc interpolation coefficients.
MNEWT - Solve non-linear system of equations f(x) = 0 via Newton's method
ORTHPOLYNOMIALS - compute ORTHogonal POLYNOMIALS
PFAFFT - Functions to perform Prime Factor (PFA) FFT's, in place
POLAR - Functions to map data in rectangular coordinates to polar and vise versa
PRINTERPLOT - Functions to make a printer plot of a 1-dimensional array
QUEST - Functions to ESTimate Quantiles:
RESSINC8 - Functions to resample uniformly-sampled data  via 8-coefficient sinc
RFWTVA - Rasterize a Float array as Wiggle-Trace-Variable-Area.
RFWTVAINT - Rasterize a Float array as Wiggle-Trace-Variable-Area, with
SBLAS - Single precision Basic Linear Algebra Subroutines
SCAXIS - compute a readable scale for use in plotting axes
SGA - Single precision general matrix functions adapted from LINPACK FORTRAN:
SHFS8R - Shift a uniformly-sampled real-valued function y(x) via
SINC - Return SINC(x) for as floats or as doubles
SORT - Functions to sort arrays of data or arrays of indices
SQR - Single precision QR decomposition functions adapted from LINPACK FORTRAN:
STOEP - Functions to solve a symmetric Toeplitz linear system of equations
STRSTUFF -- STRing manuplation subs
SWAPBYTE - Functions to SWAP the BYTE order of binary data 
SYMMEIGEN - Functions solving the eigenvalue problem for symmetric matrices

TRIDIAGONAL - Functions to solve tridiagonal systems of equations Tu=r for u.
UNWRAP_PHASE - routines to UNWRAP phase of fourier transformed data
VANDERMONDE - Functions to solve Vandermonde system of equations Vx=b 
WAVEFORMS   Subroutines to define some wavelets for modeling of seismic
WINDOW - windowing routines
wrapArray - wrap an array
XCOR - Compute z = x cross-correlated with y
XINDEX - determine index of x with respect to an array of x values
YCLIP - Clip a function y(x) defined by linear interpolation of the
YXTOXY - Compute a regularly-sampled, monotonically increasing function x(y)
ZASC - routine to translate ncharacters from ebcdic to ascii
ZEBC - routine to translate ncharacters from ascii to ebcdic

In CWPROOT/src/par/lib:
VND - large out-of-core multidimensional block matrix transpose
ATOPKGE - convert ascii to arithmetic and with error checking
DOCPKGE - Function to implement the CWP self-documentation facility
EALLOC - Allocate and free multi-dimensional arrays with error reports.
ERRPKGE - routines for reporting errors
FILESTAT - Functions to determine and output the type of a file from file
GETPARS - Functions to GET PARameterS from the command line. Numeric
LINCOEFF - subroutines to create linearized reflection coefficients
MINFUNC - routines to MINimize FUNCtions
MODELING - Seismic Modeling Subroutines for SUSYNLV and clones
REFANISO - Reflection coefficients for Anisotropic media
RKE - integrate a system of n-first order ordinary differential equations
SMOOTH - Functions to compute smoothing of 1-D or 2-D input data
SUBCALLS - routines for system functions with error checking
SYSCALLS - routines for SYSTEM CALLs with error checking
TAUP - Functions to perform forward and inverse taup transforms (radon or
UPWEIK - Upwind Finite Difference Eikonal Solver
WTLIB - Functions for wavelet transforms

In CWPROOT/src/su/lib:
FGETGTHR - get gathers from SU datafiles
fgethdr - get segy tape identification headers from the file by file pointer
FGETTR - Routines to get an SU trace from a file 
FPUTGTHR - put gathers to a file
FPUTTR - Routines to put an SU trace to a file 
HDRPKGE - routines to access the SEGY header via the hdr structure.
TABPLOT - TABPLOT selected sample points on selected trace
VALPKGE - routines to handle variables of type Value

In CWPROOT/src/psplot/lib:
BASIC - Basic C function interface to PostScript
PSAXESBOX - Functions to draw PostScript axes and estimate bounding box
PSAXESBOX3 -  Functions draw an axes box via PostScript, estimate bounding box
PSCAXESBOX - Draw an axes box for cube via PostScript
PSCONTOUR - draw contour of a two-dimensional array via PostScript
PSDRAWCURVE - Functions to draw a curve from a set of points
PSLEGENDBOX - Functions to draw PostScript axes and estimate bounding box
PSWIGGLE - draw wiggle-trace with (optional) area-fill via PostScript

In CWPROOT/src/xplot/lib:
AXESBOX - Functions to draw axes in X-windows graphics
COLORMAP - Functions to manipulate X colormaps:
DRAWCURVE - Functions to draw a curve from a set of points
IMAGE - Function for making the image in an X-windows image plot
LEGENDBOX - draw a labeled axes box for a legend (i.e. colorscale)
RUBBERBOX -  Function to draw a rubberband box in X-windows plots
WINDOW - Function to create a window in X-windows graphics
XCONTOUR - draw contour of a two-dimensional array via X vectorplot calls

In CWPROOT/src/Xtcwp/lib:
AXES - the Axes Widget
COLORMAP - Functions to manipulate X colormaps:
FX - Functions to support floating point coordinates in X
MISC - Miscellaneous X-Toolkit functions
RESCONV - general purpose resource type converters
RUBBERBOX -  Function to draw a rubberband box in X-windows plots

In CWPROOT/src/Xmcwp/lib:
RADIOBUTTONS -  convenience functions creating and using radio buttons
SAMPLES - Motif-based Graphics Functions

In CWPROOT/src/tri/lib:
CHECK - CHECK triangulated models
CIRCUM - define CIRCUMcircles for Delaunay triangulation
COLINEAR - determine if edges or vertecies are COLINEAR in triangulated
CREATE - create model, boundary edge triangles, edge face, edge vertex, add
DELETE - DELETE vertex, model, edge, or boundary edge from triangulated model
DTE - Distance to Edge
FIXEDGES - FIX or unFIX EDGES between verticies
INSIDE -  Is a vertex or point inside a circum circle, etc. of a triangulated
NEAREST - NEAREST edge or vertex in triangulated model
PROJECT - project to edge in triangulated model
READWRITE - READ or WRITE a triangulated model

In CWPROOT/src/cwputils:
CPUSEC - return cpu time (UNIX user time) in seconds
CPUTIME - return cpu time (UNIX user time) in seconds using ANSI C built-ins
WALLSEC - Functions to time processes
WALLTIME - Function to show time a process takes to run

In CWPROOT/src/comp/dct/lib:
BUFFALLOC - routines to ALLOCate/initialize and free BUFFers
DCT1 - 1D Discreet Cosine Transform Routines
DCT2 - 2D Discrete Cosine Transform Routines
DCTALLOC - ALLOCate space for transform tables for 1D DCT
GETFILTER - GET wavelet FILTER type
HUFFMAN - Routines for in memory Huffman coding/decoding
LCT1 - functions used to perform the 1D Local Cosine Transform (LCT)
LPRED - Lateral Prediction of Several Plane Waves
PENCODING - Routines to en/decode the quantized integers for lossless 
QUANT - QUANTization routines
RLE - routines for in memory silence en/decoding
WAVEPACK1 - 1D wavelet packet transform
WAVEPACK2 - 2D Wavelet PACKet transform 
WAVEPACK1 - 1D wavelet packet transform
WAVETRANS2 - 2D wavelet transform by tensor-product of two 1D transforms

In CWPROOT/src/comp/dct/lib:
BUFFALLOC - routines to ALLOCate/initialize and free BUFFers
DCT1 - 1D Discreet Cosine Transform Routines
DCT2 - 2D Discrete Cosine Transform Routines
DCTALLOC - ALLOCate space for transform tables for 1D DCT
GETFILTER - GET wavelet FILTER type
HUFFMAN - Routines for in memory Huffman coding/decoding
LCT1 - functions used to perform the 1D Local Cosine Transform (LCT)
LPRED - Lateral Prediction of Several Plane Waves
PENCODING - Routines to en/decode the quantized integers for lossless 
QUANT - QUANTization routines
RLE - routines for in memory silence en/decoding
WAVEPACK1 - 1D wavelet packet transform
WAVEPACK2 - 2D Wavelet PACKet transform 
WAVEPACK1 - 1D wavelet packet transform
WAVETRANS2 - 2D wavelet transform by tensor-product of two 1D transforms

In CWPROOT/src/comp/dwpt/1d/lib:
WBUFFALLOC -  routines to allocate/initialize and free buffers in wavelet
WPC1 - routines for compress a single seismic trace using wavelet packets 
WPC1CODING - routines for encoding the integer symbols in 1D WPC 
wpc1Quant - quantize
WPC1TRANS - routines to perform a 1D wavelet packet transform 

In CWPROOT/src/comp/dwpt/2d/lib:
WPC - routines for compress a 2D seismic section using wavelet packets 
WPCBUFFAL - routines to allocate/initialize and free buffers
WPCCODING - Routines for in memory coding of the quantized coeffiecients
WPCENDEC -  Wavelet Packet Coding, Encoding and Decoding routines
WPCHUFF -Routines for in memory Huffman coding/decoding
WPCPACK2 - routine to perform a 2D wavelet packet transform 
WPCQUANT - quantization routines for WPC
WPCSILENCE - routines for in memory silence en/decoding

To search on a program name fragment, type:
     "suname name_fragment <CR>"

For more information type: "program_name <CR>"

  Items labeled with an asterisk (*) are C programs that may
  or may not have a self documentation feature.

  Items labeled with a pound sign (#) are shell scripts that may,
  or may not have a self documentation feature.

 To find information about these codes type:   sudoc program_name

\end{verbatim}}\noindent

Please type: {\bf suhelp\/} or see Appendix~{\ref{app:B} for the full text.


\section{The Selfdoc - Program Self-Documentation}

There are no Unix man pages for SU. To some people that seems
to be a surprise (even a disappointment) as this would seem to be
a standard Unix feature, which Seismic Unix should emulate.
The package does contain an equivalent mechanism called a 
{\bf selfdoc\/} or self-documentation feature.

This is a paragraph that is written into every program, and arranged
so that if the name of the program is typed on the commandline
of a Unix terminal window, with no options or redirects to or 
from files, the paragraph is printed to standard error (the screen).

For example:
{\small\begin{verbatim}
% sustack

SUSTACK - stack adjacent traces having the same key header word
                                                                
     sustack <stdin >stdout [Optional parameters]               
                                                                
 Required parameters:                                           
        none                                                    
                                                                
 Optional parameters:                                           
        key=cdp         header key word to stack on             
        normpow=1.0     each sample is divided by the           
                        normpow'th number of non-zero values    
                        stacked (normpow=0 selects no division) 
        repeat=0        =1 repeats the stack trace nrepeat times
        nrepeat=10      repeats stack trace nrepeat times in    
                        output file                             
        verbose=0       verbose = 1 echos information           
                                                                
 Notes:                                                 
 ------                                                 
 The offset field is set to zero on the output traces, unless   
 the user is stacking with key=offset. In that case, the value 
 of the offset field is left unchanged.                         
                                                                
 Sushw can be used afterwards if this is not acceptable.        
                                                                
 For VSP users:                                         
 The stack trace appears ten times in the output file when      
 setting repeat=1 and nrepeat=10. Corridor stacking can be      
 achieved by properly muting the upgoing data with SUMUTE       
 before stacking.     
                                                                
\end{verbatim}}\noindent

The first line indicates the name of the program {\bf sustack\/} and
a short description of what the program does. 
This is the same line that appears for the listing of  {\bf sustack\/}
in the {\bf suname\/} listing.
The second line
\begin{verbatim}
     sustack <stdin >stdin [Optional parameters]
\end{verbatim}\noindent
indicates how the program is to be typed on the commandline, with
the words ``stdin'' and ``stdout'' indicating that the input
is from the standard input and standard output, respectively.
What this means in Unix terms is that the user could be inputting
and outputting data via diskfiles or the Unix ``redirect in'' $<$ 
and ``redirect out'' $>$ symbols, or via pipes $|$.

The paragraphs labeled by ``Required parameters:'' and ``Optional parameters''
indicate the commandline parameters which are required for the operation
of the program, and those which are optional. The default values of
the Optional parameters are given via the equality  are the values that the program assumes
for these parameters when data are supplied,  with no additional commandline
arguments given. For example: ``key=cdp'' indicates that {\bf sustack\/} will
stack over the common depth point gather number field, ``cdp.''
(The shell script {\bf sukeyword\/} tells the choices of keyword that
are available.) 

\section{SUDOC - List the Full Online Documentation of any Item in SU}
As has been alluded to in previous sections of this manual, there
is a database of selfdocumentation items that is available for each
main program, shell script, and library function.
This database exists in the directory \$CWPROOT/src/doc  and is
composed of all of the selfdocumentation paragraphs of all of the
items in SU.

Because not all all items with selfdocs are executable, an additional
mechanism is necessary to see the selfdoc for these items.
For example, information about the Abel transform routines, located in
\$CWPROOT/src/cwp/lib/abel.c (on the system at CWP,
 CWPROOT=/usr/local/cwp) is obtained via

{\small\begin{verbatim}
% sudoc abel

In /usr/local/cwp/src/cwp/lib: 
ABEL - Functions to compute the discrete ABEL transform:

abelalloc	allocate and return a pointer to an Abel transformer
abelfree 	free an Abel transformer
abel		compute the Abel transform

Function prototypes:
void *abelalloc (int n);
void abelfree (void *at);
void abel (void *at, float f[], float g[]);

Input:
ns		number of samples in the data to be transformed
f[]		array of floats, the function being transformed

Output:
at		pointer to Abel transformer returned by abelalloc(int n)
g[]		array of floats, the transformed data returned by 
		abel(*at,f[],g[])

Notes:
The Abel transform is defined by:

	         Infinity
	g(y) = 2 Integral dx f(x)/sqrt(1-(y/x)^2)
		   |y|

Linear interpolation is used to define the continuous function f(x)
corresponding to the samples in f[].  The first sample f[0] corresponds
to f(x=0) and the sampling interval is assumed to be 1.  Therefore, the
input samples correspond to 0 <= x <= n-1.  Samples of f(x) for x > n-1
are assumed to be zero.  These conventions imply that 

	g[0] = f[0] + 2*f[1] + 2*f[2] + ... + 2*f[n-1]

References:
Hansen, E. W., 1985, Fast Hankel transform algorithm:  IEEE Trans. on
Acoustics, Speech and Signal Processing, v. ASSP-33, n. 3, p. 666-671.
(Beware of several errors in the equations in this paper!)

Authors:  Dave Hale and Lydia Deng, Colorado School of Mines, 06/01/90
\end{verbatim}}\noindent

Here we see that sudoc 
shows information about the routines, including their names, usage
information (via the function prototype), some theory of how the
items are used, published references, and finally the author's names.

As an another example, type:
{\small \begin{verbatim}
% sugabor
                                                                        
 SUGABOR -  Outputs a time-frequency representation of seismic data via
                the Gabor transform-like multifilter analysis technique 
                presented by Dziewonski, Bloch and  Landisman, 1969.    
                                                                        
    sugabor <stdin >stdout [optional parameters]                        
                                                                        
 Required parameters:                                                   
        if dt is not set in header, then dt is mandatory                
                                                                        
 Optional parameters:                                                   
        dt=(from header)        time sampling interval (sec)            
        fmin=0                  minimum frequency of filter array (hz)  
        fmax=NYQUIST            maximum frequency of filter array (hz)  
        beta=3.0                ln[filter peak amp/filter endpoint amp] 
        band=.05*NYQUIST        filter bandwidth (hz)                   
        alpha=beta/band^2       filter width parameter                  
        verbose=0               =1 supply additional info               
                                                                        
 Notes: This program produces a muiltifilter (as opposed to moving window)
 representation of the instantaneous amplitude of seismic data in the   
 time-frequency domain. (With Gaussian filters, moving window and multi-
 filter analysis can be shown to be equivalent.)                        
                                                                        
 An input trace is passed through a collection of Gaussian filters      
 to produce a collection of traces, each representing a discrete frequency
 range in the input data. For each of these narrow bandwidth traces, a 
 quadrature trace is computed via the Hilbert transform. Treating the narrow
 bandwidth trace and its quadrature trace as the real and imaginary parts
 of a "complex" trace permits the "instantaneous" amplitude of each
 narrow bandwidth trace to be compute. The output is thus a representation
 of instantaneous amplitude as a function of time and frequency.        
                                                                        
 Some experimentation with the "band" parameter may necessary to produce
 the desired time-frequency resolution. A good rule of thumb is to run 
 sugabor with the default value for band and view the image. If band is
 too big, then the t-f plot will consist of stripes parallel to the frequency
 axis. Conversely, if band is too small, then the stripes will be parallel
 to the time axis.                                                      
                                                                        
 Examples:                                                              
    suvibro | sugabor | suximage                                        
    suvibro | sugabor | suxmovie n1= n2= n3=                            
     (because suxmovie scales it's amplitudes off of the first panel,  
     may have to experiment with the wclip and bclip parameters        
    suvibro | sugabor | supsimage | ... ( your local PostScript utility)

\end{verbatim}} \noindent

If you compare this output to the output from typing:

{\small \begin{verbatim}
% sudoc sugabor 
\end{verbatim}}\noindent

You will see the same output as above, preceeded by
a line showing the location of the source code,
and followed by the additional paragraphs: 
{\small \begin{verbatim}

 Credits:

	CWP: John Stockwell, Oct 1994

 Algorithm:

 This programs takes an input seismic trace and passes it
 through a collection of truncated Gaussian filters in the frequency
 domain.

 The bandwidth of each filter is given by the parameter "band". The
 decay of these filters is given by "alpha", and the number of filters
 is given by nfilt = (fmax - fmin)/band. The result, upon inverse
 Fourier transforming, is that nfilt traces are created, with each
 trace representing a different frequency band in the original data.

 For each of the resulting bandlimited traces, a quadrature (i.e. pi/2
 phase shifted) trace is computed via the Hilbert transform. The 
 bandlimited trace constitutes a "complex trace", with the bandlimited
 trace being the "real part" and the quadrature trace being the 
 "imaginary part".  The instantaneous amplitude of each bandlimited
 trace is then computed by computing the modulus of each complex trace.
 (See Taner, Koehler, and Sheriff, 1979, for a discussion of complex
 trace analysis.

 The final output for a given input trace is a map of instantaneous
 amplitude as a function of time and frequency.

 This is not a wavelet transform, but rather a redundant frame
 representation.

 References: 	Dziewonski, Bloch, and Landisman, 1969, A technique
		for the analysis of transient seismic signals,
		Bull. Seism. Soc. Am., 1969, vol. 59, no.1, pp.427-444.

		Taner, M., T., Koehler, F., and Sheriff, R., E., 1979,
		Complex seismic trace analysis, Geophysics, vol. 44,
		pp.1041-1063.

 		Chui, C., K.,1992, Introduction to Wavelets, Academic
		Press, New York.

 Trace header fields accessed: ns, dt, trid, ntr
 Trace header fields modified: tracl, tracr, d1, f2, d2, trid, ntr

\end{verbatim}}\noindent

There is more information in the {\bf sudoc\/} listing,
than in the selfdoc listing. The selfdoc is intended as a quick reference,
whereas the sudoc listing can provide additional information
that we do not necessarily want to see everytime we are, say, simply wanting
to know what a particular parameter means, for example.
 
\section{SUFIND - Find SU Items with a Given String}

The ``doc'' database (located in \$CWPROOT/src/doc) may also be
searched for specific 
strings, or topics. The shell script {\bf sufind\/}
serves this purpose.
If you type {\bf sufind\/} with no options, you will see
{\small\begin{verbatim}
% sufind

sufind - get info from self-docs about SU programs
Usage: sufind [-v -n -P<command_pattern>] string
        ("string" can be an "egrep" pattern)
"sufind string" gives brief synopses
"sufind -v string" verbose hunt for relevant items
"sufind -n name_fragment" searches for command name
"sufind -P<pattern> string"
        gives brief synopses by searching for "string" among
        commands/libraries whose names match the "pattern"

\end{verbatim}}\noindent
showing that there is some sophisticated functionality
which may help you find items of a particular type in
the SU package.

As an example of this,  let's say that you are looking
for programs that use the fast Fourier transform
algorithms in SU, type
{\small\begin{verbatim}
% sufind fft

 FFTLAB - Motif-X based graphical 1D Fourier Transform

 Usage:  fftlab


HANKEL - Functions to compute discrete Hankel transforms

hankelalloc     allocate and return a pointer to a Hankel transformer
hankelfree      free a Hankel transformer

PFAFFT - Functions to perform Prime Factor (PFA) FFT's, in place

npfa            return valid n for complex-to-complex PFA
npfar           return valid n for real-to-complex/complex-to-real PFA

 SUAMP - output amp, phase, real or imag trace from             
        (frequency, x) domain data                              

 suamp <stdin >stdout mode=amp                                  

 SUFFT - fft real time traces to complex frequency traces       

 suftt <stdin >sdout sign=1                                     


 SUFRAC -- take general (fractional) time derivative or integral of     
            data, plus a phase shift.  Input is TIME DOMAIN data.       

 sufrac power= [optional parameters] <indata >outdata                   

 SUIFFT - fft complex frequency traces to real time traces      

 suiftt <stdin >sdout sign=-1                                   


 SUMIGPS - MIGration by Phase Shift with turning rays                   

 sumigps <stdin >stdout [optional parms]                                


 SUMIGTK - MIGration via T-K domain method for common-midpoint stacked data

 sumigtk <stdin >stdout dxcdp= [optional parms]                 


 SURADON - forward generalized Radon transform from (x,t) -> (p,tau) space.

 suradon <stdin >stdout [Optional Parameters]                           


For more information type: "program_name <CR>"
\end{verbatim}}\noindent
The final line of this output ends with a symbol meant to indicate that the 
user is to type a carriage return.\footnote{The 
phrase ``carriage return'' refers to an older technology, the typewriter.
Ask your parents (or grandparents) for further details.}
As you can see, there is a mixed collection of programs which 
are either demos of the CWP/SU prime factor fft routines (pfafft),
libraries containing the routines, or programs that use fft routines.

\subsection{Getting information about SU programs}

A more specific example would be to use
{\bf sufind\/} to look for dip moveout (DMO) programs:
{\small\begin{verbatim}
% sufind dmo

 SUDMOFK - DMO via F-K domain (log-stretch) method for common-offset gathers

 sudmofk <stdin >stdout cdpmin= cdpmax= dxcdp= noffmix= [...]           


 SUDMOTX - DMO via T-X domain (Kirchhoff) method for common-offset gathers

 sudmotx <stdin >stdout cdpmin= cdpmax= dxcdp= noffmix= [optional parms]

 SUDMOVZ - DMO for V(Z) media for common-offset gathers

 sudmovz <stdin >stdout cdpmin= cdpmax= dxcdp= noffmix= [...]     

 SUFDMOD2 - Finite-Difference MODeling (2nd order) for acoustic wave equation

 sufdmod2 <vfile >wfile nx= nz= tmax= xs= zs= [optional parameters]     


 SUSTOLT - Stolt migration for stacked data or common-offset gathers    

 sustolt <stdin >stdout cdpmin= cdpmax= dxcdp= noffmix= [...]           
\end{verbatim}}\noindent
The last two ``hits'' are spurious,
but we see that three DMO programs have been found.

Now use the self-doc facility to get more information about {\bf sudmofk\/}:
{\small\begin{verbatim}
% sudmofk
                                                                        
 SUDMOFK - DMO via F-K domain (log-stretch) method for common-offset gathers
                                                                        
 sudmofk <stdin >stdout cdpmin= cdpmax= dxcdp= noffmix= [...]           
                                                                        
 Required Parameters:                                                   
 cdpmin                  minimum cdp (integer number) for which to apply DMO
 cdpmax                  maximum cdp (integer number) for which to apply DMO
 dxcdp                   distance between adjacent cdp bins (m) 
 noffmix                 number of offsets to mix (see notes)           
                                                                        
 Optional Parameters:                                                   
 tdmo=0.0                times corresponding to rms velocities in vdmo (s)
 vdmo=1500.0             rms velocities corresponding to times in tdmo (m/s)
 sdmo=1.0                DMO stretch factor; try 0.6 for typical v(z)   
 fmax=0.5/dt             maximum frequency in input traces (Hz) 
 verbose=0               =1 for diagnostic print                        
                                                                        
 Notes:                                                         
 Input traces should be sorted into common-offset gathers.  One common- 
 offset gather ends and another begins when the offset field of the trace
 headers changes.                                                       
                                                                        
 The cdp field of the input trace headers must be the cdp bin NUMBER, NOT
 the cdp location expressed in units of meters or feet.         
                                                                        
 The number of offsets to mix (noffmix) should typically equal the ratio of
 the shotpoint spacing to the cdp spacing.  This choice ensures that every
 cdp will be represented in each offset mix.  Traces in each mix will   
 contribute through DMO to other traces in adjacent cdps within that mix.
                                                                        
 The tdmo and vdmo arrays specify a velocity function of time that is   
 used to implement a first-order correction for depth-variable velocity.
 The times in tdmo must be monotonically increasing.                    
                                                                        
 For each offset, the minimum time at which a non-zero sample exists is 
 used to determine a mute time.  Output samples for times earlier than this
 mute time will be zeroed.  Computation time may be significantly reduced
 if the input traces are zeroed (muted) for early times at large offsets.
                                                                        
 Trace header fields accessed:  ns, dt, delrt, offset, cdp.             
\end{verbatim}}\noindent
By using {\bf sufind\/} in conjunction with the selfdoc feature (or {\bf sudoc}),
and choosing smart strings to search on, it is possible to find
a great deal of detailed information about SU's facilities.

\section{GENDOCS - An Instant LaTeX Document Containing All Selfdocs}
The ultimate shell script for exploiting the {\bf sudoc\/} database is
{\bf gendocs}. Typing:
{\small \begin{verbatim}
% gendocs -o
\end{verbatim}}\noindent
will generate the 528+ page document ``selfdocs.tex'', which is
in LaTeX format. You may process this using LaTeX on your system.
Obviously, you must {\em really\/} be sure that you want to print this
document, considering its size.
However, it does contain all of the self-documentations for all
CWP/SU programs, library routines, and shell scripts, and may be
a useful one-to-a-lab type reference.

\section{Suhelp.html}
Long-time SU contributor, Dr. Christopher L. Liner of the University
of Tulsa, created the following  document which may be accessed
from the CWP/SU web site, or from his location of:

\begin{verbatim}
http://sepwww.stanford.edu/oldsep/cliner/files/suhelp/suhelp.html
\end{verbatim}

{\small\begin{verbatim}

SeismicUn*x Help (v. 38)
     

This is a help browser for the core part of the SeismicUn*x (SU) free software package developed and maintained by the Center for Wave Phenomena (CWP) at the Colorado School of Mines. The SU project is directed by John Stockwell (john@dix.mines.edu).

The SU programs are arranged below in broad catagories. Clicking on a program name pulls up the selfdoc for that item. Your web browser's Find capability is useful if you have a name fragment in mind (e.g. sort or nmo). While programs may logically apply to more than one catagory below, each program appears only once.

For the many programmers who develop new SU applications (and fix the old ones), I have added a listing of non-graphics library routines and the full SEGY header abbreviations (i.e., segy.h) used in SU. This should be considered a minimal subset of the numerical routines in SU. Many others are embedded as subroutines in main programs.

This help page is current through version 38 (2005) and thus should be considered a historical archive. However, since SU grows by accretion, almost all of the programs given here will be in the current SU release, although there may occasionally be additional parameters and functionality in later versions. Finally, there will certainly be new codes in the current release that are not found here. Enjoy.

Prof. Christopher Liner
Maurice F. Storm Endowed Chair
Department of Geosciences
University of Arkansas
chris.liner@gmail.com

My blog: http://seismosblog.blogspot.com/
Catagory Listing of SU Programs

    Data Compression
    Editing, Sorting and Manipulation
    Filtering, Transforms and Attributes
    Gain, NMO, Stack and Standard Processes
    Graphics
    Import/Export
    Migration and Dip Moveout
    Simulation and Model Building
    Utilities

Core Numerical and Programming Library Routines
SEGY header abbreviations used in SU

...
\end{verbatim}}\noindent
To see the full listing see Appendix~{\ref{app:B} or point your
web browser to the http address above.


\section{Demos}
The SU package contains a suite of demos, which are shell scripts
located in the directory \$CWPROOT/src/demos.

The instructions for accessing the demos are located in the
\$CWPROOT/src/demos/README 

\begin{itemize}
\item The Making\_Data demos shows the basics of making synthetic data
shot gathers and common offset sections using susynlv.  Particular
attention is paid to illustrating good display labeling.

\item The Filtering/Sufilter demo illustrates some real data processing to
eliminate ground roll and first arrivals.  The demos in the
Filtering subdirectories give instructions for accessing the data
from the CWP ftp site.

\item The Deconvolution demo uses simple synthetic spike traces to
illustrate both dereverberation and spiking decon using supef and
other tools.  The demos include commands for systematically
examining the effect of the filter parameters using loops.

\item The Sorting\_Traces Tutorial is an interactive script that
reinforces some of the basic UNIX and {\small\sf SU} lore discussed in this
document.  The interactivity is limited to allowing you to set the
pace.  Such tutorials quickly get annoying, but we felt that one
such was needed to cover some issues that didn't fit into our
standard demo format.  There is also a standard, but less complete,
demo on this topic.

\item The next step is to activate the Selecting\_Traces Demo.  Then
proceed to the NMO Demo. 
\end{itemize}
Beyond that, visit the Demo directories
that interest you.  The {\bf demos\/} directory tree is still under
active development---please let us know if the demos are helpful and
how they can be improved.

\section{Other Help Mechanisms}

\begin{itemize}
\item SUKEYWORD - List SU Header Field Key Words
Many of the SU programs that draw on header field information
have the parameter ``key='' listed in their selfdocs, with
the reference to ``keywords.''
The SU keywords are based on the SEGY trace header fields.
(This will be explained later, in the sections on tape reading
and data format conversion.)
To find out what these header fields are, and what they stand
for in the SU data type, type:
{ \small\begin{verbatim} 
% sukeyword -o
\end{verbatim} } \noindent
Please see Appendix~\ref{app:B} for the full text of the output. 
See Section~\ref{sukeyword} for details on the usage of this
command.  A number of programs sort data, window data, and display data
by making use of the values in the SU header fields, making
{\bf sukeyword\/} an oft-used utility.

\item The essence of {\small\sf SU} usage is the construction of shell programs
  to carry out coordinated data processing.  The {\bf su/examples\/}
  directory contains a number of such programs.  By the way, the terms
  ``shell scripts,'' ``shell programs,'' ``shell files,'' and
  ``shells,'' are used interchangeably in the UNIX literature.

\item The {\bf faq\/} directory contains a growing collection of answers
  to frequently asked questions about {\small\sf SU}, including detailed information
  about tape reading, data format questions, and seismic processing tips.

\item The text book, {\em Theory of Seismic Imaging}, by John A.
  Scales, Samizdat Press, 1994, is available in our anonymous ftp site
  in both 300 and 400 dots per inch PostScript format:
  \verb:pub/samizdat/texts/imaging/imaging_300dpi.ps.Z: or
  \verb:imaging_400dpi.ps.Z:.  The exercises in this text make
  extensive use of {\small\sf SU}.
\end{itemize}

You should not hesitate to look at the source code itself.
Section~\ref{SU:sec:template} explains the key {\small\sf SU} coding idioms.
Please let us know if you discover any inconsistencies between the
source and our documentation of it.  We also welcome suggestions for
improving the comments and style of our codes.  The main
strength of the SU package is that it is a source-code product.
No commercial package can give you the source code, for obvious
reasons.

You may direct email to:  john@dix.mines.edu 
if you have comments, questions, suggestions regarding {\small\sf SU},
or if you have your own {\small\sf SU}-style programs to contribute
to the package.


\chapter{Core Seismic Unix Programs} 

The core of the Seismic Unix program set performs a broad
collection of tasks, which may be viewed as being common
to a large collection of research and processing disciplines.
Many, however, are purely seismic in nature, and are indicated
as such in the text.

Some of these tasks include
\begin{itemize}
\item input/output issues
\item data format conversion,
\item setting, viewing, and editing trace header fields
\item viewing SU data,
\item windowing, sorting, and editing the data.
\item general operations,
\item transform and filtering operations,
\item seismic operations on SU data.
\end{itemize}

It is the intent of the chapters that follow is to deal with
many of these issues.
Please note that more detailed information about any
of the programs discussed can be obtained by typing the name
of the program on the commandline with no arguments,
{\small\begin{verbatim}
% programname
\end{verbatim}} \noindent
or by typing:
{\small\begin{verbatim}
% sudoc programname
\end{verbatim} } \noindent
to see the selfdoc information.

This chapter does not cover all SU programs, but just a sufficient
number such that a person with some Unix experience can get started
with the package. Once you get the idea of how SU is used, then 
you may draw on the help facilities to discover additional programs
in the package.

\section{Reading and Writing Data to and from Tapes}

{\em Reading tapes is more of an art than a science.\/} This is
true in general, and is especially true for SU.
The variability of hardware formats, as well as the variability
of data format types, makes the creation of a ``general tape
reading utility'' a challenging, if not impossible, proposition.

The following programs are useful for the specialized data input and output
tasks related to geophysical applications, as well as to the internal
SU data format
\begin{itemize}
\item BHEDTOPAR - convert a Binary tape HEaDer file to PAR file format
\item DT1TOSU - Convert ground-penetrating radar data in the 
Sensors \& Software X.dt1 GPR format to SU format
\item SEGDREAD - read an SEG-D tape 
\item SEGYCLEAN - zero out unassigned portion of header
\item SEGYREAD - read an SEG-Y tape 
\item SEGYHDRS - make SEG-Y ascii and binary headers for segywrite
\item SEGYWRITE - write an SEG-Y tape 
\item SETBHED - SET the fields in a SEGY Binary tape HEaDer file
\item SUADDHEAD - put headers on bare traces and set the tracl and ns fields 
\item SUSTRIP - remove the SEGY headers from the traces 
\item SUPASTE - paste existing SEGY headers on existing data
\end{itemize}

The following programs are useful for general data input, output,
and data type conversion, which may also find use in tape reading,

\begin{itemize}
\item A2B - convert ascii floats to binary
\item B2A - convert binary floats to ascii
\item FTNSTRIP - convert Fortran floats to C-style floats 
\item H2B - convert 8 bit hexidecimal floats to binary
\item RECAST - RECAST data type (convert from one data type to another)
\item TRANSP - TRANSPose an n1 by n2 element matrix 
\end{itemize}

\subsection{The SEGY format and the SU data format}

The data format that is expected by all programs in the CWP/SU package
whose names begin with the letters `su'  (with the exception of the
program {\bf subset}), consists of ``SEGY traces written in the native
binary format of the machine you are running the programs on.''
To understand what this phrase means, we must understand what the
SEGY standard is.

In the early 1980's, the most common data storage format was SEG-Y.
This is the Society of Exploration Geophysicists Y format which is
described in the SEG's publication {\em Digital Tape Standards\/}.
The format is still widely used, today, though there is no
guarantee that the format is used ``by the book.''

The SEGY data format consists of 3 parts. The first part is a
3200 byte EBCDIC card image header which contains 40 cards
(i.e. 40 lines of text with 80 characters per line) worth of
text data describing the tape.
The second part is a 400 byte binary header containing information
about the contents of the tape reel. The third portion of 
the SEG-Y format consists of the actual seismic traces. Each trace
has a 240 byte {\em trace header\/}. The data follow, written in
one of 4 possible 32 formats in IBM floating point notation
as defined in IBM Form GA 22-6821. (Note, this ``IBM format'' is
not the common IEEE format found on modern IBM PC's.)

The SU data format is based on the trace portion of the SEGY format.
The primary difference between the SEGY traces and SU traces is that
the data portion of the SU format are floats, written in the native
binary float format of the machine you are running SU on. SU data
consists of the SEGY traces {\em only\/}! The ebcdic and binary
reel headers are not preserved in the SU format,
so simply redirecting in a SEGY file will not work with any SU 
program.

To convert SEGY data into a form that can be used by SU programs,
you will need to use {\bf segyread}.


\subsection{SEGYREAD - Getting SEG-Y data into SU}
The program {\bf segyread\/} is used to convert data from the SEGY format
to the SU format.
If you type:
{\small\begin{verbatim}
% segyread
\end{verbatim} }  \noindent
You will see the selfdoc for this program.

When reading a SEGY tape, or datafile, you will need to be aware of
the byte-order (endian) of the machine you are running on.
The so-called ``big-endian'' or high-byte IEEE format is found on SGI,
SUN, IBM RS6000, and all Motorola chip-based systems. 
The ``little-endian'' or low-byte systems are systems that are based
on Intel and Dec chips.
You will also need to know what Unix device your tape drive is.

A typical execution of {\bf segyread\/} on a big-endian machine, 
looks like this:
{\small\begin{verbatim}
% segyread tape=/dev/rmt0 verbose=1 endian=1 > data.su
\end{verbatim}}\noindent

More often you will have to use the following
{\small\begin{verbatim}
% segyread tape=/dev/rmt0 verbose=1 endian=1 | segyclean > data.su
\end{verbatim}}\noindent
for reading a tape on a big-endian platform.

There are optional header fields (bytes 181-240) in the SEGY trace
headers. There is no standard for what may go in these fields, so
many people have items that they place in these fields for their
own purposes. SU is no exception. There are several parameters 
used by SU graphics programs that may be stored in these fields.
The program {\bf segyclean\/} zeros out the values of the optional header 
fields so that SU graphics programs don't become confused by this information.

There are additional issues, such as whether or not your device
is buffered or unbufferd (i.e. 9 track 1/2 reel tape, or 8mm Exabyte)
tape which may have to be experimented with when you actually
try to read a tape.
Also, if you are trying to read a tape on a different system than
the one it was made on, you may simply not be able to read the tape.

The most common problem with reading tapes is matching the density
that the tape was written in, with the tapedrive that the tape is
being read on.
Some systems, for example Silicon Graphics (SGI) systems, have
many tape devices, which support different hardware configurations
and tape densities.
Other systems, most
notably recent versions of Linux have an improved version of the Unix
command ``mt'' which has a ``setdensities'' option.
In either case, it is common for tapes to be made using the default
settings of a tape drive, or its default densities. 

As a last resort in all tape reading situations, it is often possible to
use the Unix device-to-device copying program {\bf dd\/} to make an image
of the entire tape on disk
{\small \begin{verbatim}
% dd if=/dev/rmtx of=filename bs=32767 conv=noerror
\end{verbatim}}\noindent
where ``/dev/rmtx'' is replaced with your tapedrive device and ``filename''
is some file name you choose.
If this works, then the next step is to try using {\bf segyread\/} as above,
with ``tape=filename.''
If {\bf dd\/} fails, then it is likely that the hardware format
of your tapedrive is not compatible with your tape.

Of course, the best way to prevent tape reading problems is to
make sure that you talk to the person who is writing the tape
before they write it. On SGI systems, in particular, there are
so many possible choices for the type of tape format, that the
person who is making the tape must have information about the platform
that the tape is intended to be read on, before they can make a
tape that is guaranteed to be readable.

\subsection{SEG-Y abuses}

Unfortunately, there are formats which are called ``SEGY'' but which
are not true to the SEG's standards for SEGY.
One common variation is to honor most of the SEGY convention, but
have the traces be in an IEEE format.

Such data would be read via:
{\small\begin{verbatim}
% segyread tape=/dev/rmt0 verbose=1 endian=1 conv=0 | segyclean > data.su
\end{verbatim}}\noindent
where the ``conv=0'' tells the program not to attempt the IBM to float
conversion.

\subsubsection{DOS SEGY}
There is also a ``DOS SEGY'' format which is similar to the previous
format, with the exception that the traces and headers are all written
in a little-endian format. On a big-endian machine,
the command to read such a dataset would be
{\small\begin{verbatim}
% segyread tape=/dev/rmt0 verbose=1 endian=0 conv=0 | segyclean > data.su
\end{verbatim}}\noindent
will read the data. Note, that endian=0 is set to swap the bytes.
(All of the bytes, header and data are in the swapped format.)
On a little-endian machine, the procedure is 
{\small\begin{verbatim}
% segyread tape=/dev/rmt0 verbose=1 endian=1 conv=0 | segyclean > data.su
\end{verbatim}}\noindent
with endian=1, in this case preventing byteswapping.

In each case, if we had a diskfile with some `filename', we would
use ``tape=filename.''

\subsubsection{Landmark BCM2D format in SEISWORKS}

Commercial seismic software can be sources of non-standard SEGY formats.
In addition to non-standard usage of the official header fields, 
commercial variations on SEGY may employ definitions of parts of the 
optional SEGY header fields.

A remedy for this problem, supplied by Matthias Imhoff of Virginia Tech,
is a remapping feature in segyread, which allows nonstandard fields
to be remapped into compatible locations in the SU header.  The
options remap=  allows the destination SU header fields to be specified,
while byte= specifies the byte location in the SEGY trace header, and
its data type.

For the example of Landmark BCM2D format, header fields 73 and 77 are
floats, but these are int's in the standard SEGY format and are hence
also int's in the SU format. BCM2D also has to header fields set as longs
at bytes 181 and 185. The following usage of segyread 

{\small\begin{verbatim}
% segyread tape=... remap=d1,d2,gelev,selev byte=73f,77f,181l,185l > ...
\end{verbatim}}\noindent

The floats at 73 and 77 are mapped to d1 and d2, while the long integers at 
181 and 185 are mapped to gelev and selev, which are integers in the
SU format. By selecting compatible destination fields, no precision
is lost.

\subsection{SEGYWRITE - Writing an SEGY Tape or Diskfile}
The companion program to {\bf segyread\/} is {\bf segywrite}. This program
permits data to be written either to a tape or a diskfile in
a number of variations on the SEGY format.
This program is useful for putting data into a form that can be
read by commercial seismic packages.
Before showing examples of how {\bf segywrite\/} is used, there
are several preprocessing steps that must be discussed in preparation
for actually writing a tape.

\subsection{SEGYHDRS - make SEG-Y ascii and binary headers for segywrite}

To write a tape in exactly the SEGY format as specified by the
SEG's Digital Tape Standards book, you will need to supply
the ASCII and binary tape reel header files which will become
the EBCDIC and binary tape reel headers in the SEGY tape or file.
These are the files ``header'' and ``binary'' created by segywrite. 

If you don't have the ``binary'' and ``header'' files, then you
must create them with the program {\bf segyhdrs\/} (pronounced SEG Y headers)
The command
{\small\begin{verbatim}
% segyhdrs < data.su
\end{verbatim}}\noindent
will write the files  ``header'' and ``binary'' in the current working
directory.
As an example, make some test data with {\bf suplane\/} and then
run {\bf segyhdrs\/} on it
{\small\begin{verbatim}
% suplane > data.su
% segyhdrs < data.su
\end{verbatim}}\noindent
You will see the files {\em binary\/} and {\em header\/} appear in
your current working directory.

The program has options that will allow you to set the
values of binary header fields. 
These fields may be seen by typing:
{\small\begin{verbatim}
% sukeyword jobid

        int jobid;      /* job identification number */

        int lino;       /* line number (only one line per reel) */

        int reno;       /* reel number */

        short ntrpr;    /* number of data traces per record */

        short nart;     /* number of auxiliary traces per record */

        unsigned short hdt; /* sample interval in micro secs for this reel */

        unsigned short dto; /* same for original field recording */   
...
\end{verbatim}}\noindent
where ``jobid'' is the first binary header field.

The file ``header'' is an ASCII file which may be edited with a
normal text editor. You can put anything in there, as long as the
format is 40 lines of 80 characters each. {\bf Segywrite\/} will
automatically convert this program 
The default header file created by {\bf segyhdrs\/} looks like this
{\small\begin{verbatim}
C      This tape was made at the                                               
C                                                                              
C      Center for Wave Phenomena                                               
C      Colorado School of Mines                                                
C      Golden, CO, 80401                                                       
C                                                                              
...                                                                              
C                                                                              
C                                                                              
\end{verbatim}}\noindent

\subsection{BHEDTOPAR, SETBHED - Editing the binary header file}
The binary header file must be converted to an ASCII form, to be
edited.  The program {\bf bhedtopar\/} permits the ``binary'' file
to be written in the form of a ``parfile''
{\small\begin{verbatim}
% bhedtopar  < binary outpar=binary.par
\end{verbatim}}\noindent
which for the case of the header file created for the test SU data
appears as follows
{\small\begin{verbatim}
jobid=1
lino=1
reno=1
ntrpr=0
nart=0
hdt=4000
...
\end{verbatim}}\noindent
The values that are assigned to the various header files may be
edited, and be reloaded into the header file via {\bf setbhed\/}
{\small\begin{verbatim}
% setbhed bfile=binary par=binary.par
\end{verbatim}}\noindent
Individual header field values may also be set. For example
{\small\begin{verbatim}
% setbhed bfile=binary par=binary.par lino=3
\end{verbatim}}\noindent
which uses the contents of binary.par but with the field lino set to 3.

Finally, the tape may be written via a command sequence like this
{\small \begin{verbatim}
% segywrite tape=/dev/rmtx verbose=1 < data.su
\end{verbatim}} \noindent
taking care to note that the files ``header'' and ``binary'' are
in the current working directory. You may use different names
for these files, if you wish. {\bf Segywrite\/} has a ``bfile='' 
and an ``hfile='' option to permit you to input the files by
the different names you choose.

\subsection{SEGDREAD - Other SEG formats}
There are other SEG formats (SEG-A, SEG-B, SEG-X, SEG-C, SEG-D, SEG-1,
and SEG-2).
Of these, SEG-D, SEG-B, and SEG-2 are the types that you will most
commonly encounter. 
There is a {\bf segdread\/} program which supports only 1 of the vast
number of variations on SEG-D.

In the directory \$CWPROOT/Third\_Party/   is a {\bf seg2segy\/} conversion
program which may be used to convert SEG-2 format to SEG-Y.
Future plans are to create a {\bf seg2read\/} program, which will
be similar to {\bf segyread\/} and {\bf segdread\/}.

\subsection{DT1TOSU - Non-SEG tape formats}

Currently, there is only one non-SEG tape format that is completely supported
in the SU package, and two others which are supported through third-party
codes which have not yet been integrated into the package.
This is the Sensors \& Software DT1 format,
via {\bf dt1tosu}, which is a GPR (ground penetrating radar) format.
In the \$CWPROOT/src/Third\_Party directory are two additional
non-SEG conversion programs, these are {\bf segytoseres\/} and 
{\bf bison2su}.
Future plans include incorporating each of these codes into the main
CWP/SU package.

\section{Data Format conversion}

Often, it is necessary to transfer data from other systems, or
to input data which may be in a variety of formats. A number
of tools and tricks are available in SU for dealing with these
issues.

The following programs may be useful for such conversion problems
\begin{itemize}
\item A2B - convert ascii floats to binary
\item B2A - convert binary floats to ascii
\item FTNSTRIP - convert Fortran floats to C-style floats 
\item FTNUNSTRIP - convert C-style floats to Fortran-style floats
\item H2B - convert 8 bit hexidecimal floats to binary
\item RECAST - RECAST data type (convert from one data type to another)
\item TRANSP - TRANSPose an n1 by n2 element matrix 
\item FARITH - File ARITHmetic -- perform simple arithmetic with binary files 
\item SUADDHEAD - put headers on bare traces and set the tracl and ns fields 
\item SUSTRIP - remove the SEGY headers from the traces 
\item SUPASTE - paste existing SEGY headers on existing data
\item SWAPBYTES - SWAP the BYTES of various  data types
\item SUSWAPBYTES - SWAP the BYTES in SU data to convert data from big endian
to little endian byte order, and vice versa
\end{itemize}

The purpose of this section is to discuss situations where these
programs may be used.
\subsection{A2B and B2A - ASCII to Binary, Binary to ASCII}

Of all of the formats of data, the most transportable (and most 
space consuming) is ASCII. No matter what system you are working
on, it is possible to transport ASCII data to and from that system.
Also, because text editors support ASCII, it is usually possible to
data entry and data editing in the simplest of text editors.

Such data probably come in a multicolumn format, separated either
by spaces or tabs. To convert such a, say 5 column, dataset into
binary floats, type:
{\small\begin{verbatim}
% a2b < data.ascii n1=5 > data.binary 
\end{verbatim}}\noindent
The reverse operation is
{\small\begin{verbatim}
% b2a < data.binary n1=5 > data.ascii 
\end{verbatim}}\noindent

\subsection{FTNSTRIP - Importing Fortran Data to C}
Often, because Fortran is a popular language in seismic data processing,
data may be obtained that was either created or processed in some
way with Fortran.
Binary data in Fortran are separated by beginning-of-record
and end-of-record delimiters. Binary data created by C programs do not have
any such delimiters.
To use Fortran data in a C program requires that the Fortran labels
be stripped off, via
{\small\begin{verbatim}
ftnstrip < fortdata > cdata
\end{verbatim}}\noindent
This will produce C-style floats, most of the time.
The program assumes that each record of fortran data is preceded
and followed by an integer listing the size of the record in bytes.
There have been fortran data types which have had only one or the
other of these integer markers, but having both a beginning-of-record
(BOR) and an end-of-record (EOR) markers seems to be standard today.

\subsection{Going from C to Fortran}

It is fairly easy to transport data made with Fortran code to C, however,
it may not be so easy to go the other way. On
the SGI Power Challenge, it is possible to read a file of C-floats
called ``infile'' via, open and read statements that look like:

{\small\begin{verbatim}
OPEN(99,file='infile',form='system') 

DO i=1,number
READ(99) tempnumber
Array(i)=tempnumber
END DO

CLOSE(99)
\end{verbatim}}\noindent

The statement "form='system'" does not work on all machines, however,
as it is likely that this is not standard Fortran.  The general format
command to read in binary is "form='unformatted'".  This may not work
on other systems, (for example, SUN). Indeed, it may not be generally
guaranteed that you can
read binary files in Fortran that have been created with a C-programs
(as in SU).

If you have problems with binary and the input files are not too
big you could convert to ASCII (using 'b2a') and use formatted I/0

{\small\begin{verbatim}
OPEN(99,file='infile')

DO i=1,number
READ(99,*) tempnumber
Array(i)=tempnumber
END DO

CLOSE(99)
\end{verbatim}}\noindent

\subsubsection{FTNUNSTRIP - convert C binary floats to Fortran style floats }

However, another possibility is to make ``fake'' Fortran floats
with a C-program. Such a program is {\bf ftnunstrip}.

This program assumes that the record length is constant
throughout the input and output files.
In fortran code reading these floats, the following implied
do loop syntax would be used:
        DO i=1,n2
                 READ (10) (someARRAY(j), j=1,n1)
        END DO
Here n1 is the number of samples per record, n2 is the number
of records, 10 is some default file (fort.10, for example) which
is opened as form='unformatted'. Here  ``someArray(j)'' is an array
dimensioned to size n1.

Please note that the Fortran style of having BOR and EOR markers
is smart, if used properly, but is stupid if used incorrectly. 
The Fortran READ statement finds out from the BOR marker how many 
bytes will follow, reads the number of values specified, keeping 
track of the number of bytes. When it finishes reading, it compares
the number of bytes read to the number of bytes listed in the EOR
value, and can effectively trap errors in bytes read, or premature
EOR.

Because there is an additional ``sizeof(int)'' (usually 4 bytes)
at the beginning and end of every record, it is smart to have as few
records as is necessary. The worst case scenario is to have every
value of data be a record, meaning that the size of the file could
be 2/3 BOR and EOR markers and 1/3 data!

\subsection{H2B - Importing 8 Bit Hexidecimal}

The issue of converting 8 bit hexidecimal may seem to be one
that would not come up very often.
However 8 bit hex is a common format for bitmapped images
(grayscale PostScript) and if you wish to take a scanned image
and turn it into floats for further processing, then it will
come up.\footnote{This issue came up originally, when a student
had destroyed an original dataset, by accident, but only had
a bit-mapped PostScript image of the data. Using {\bf h2b}
it was possible to recover the data from the PostScript file.}

If you have a scanned image, written as a 256 level grayscale
bitmapped PostScript image, then the bitmap portion is in  8 bit hex.
By removing all of the PostScript commands, and leaving only the
bitmap then the command
{\small\begin{verbatim}
% h2b < hexdata > floatdata
\end{verbatim}}\noindent
will convert the bitmap into a form that can be viewed and processed
by programs in the CWP/SU package.

\subsection{RECAST - Changing Binary Data Types}

Of course, C supports a variety of types, and instead of having
a bunch of program to convert each type into every other type,
there is a program called ``recast'' that will do the job for a
large collection of these types. 

Types supported by recast for input and output:
\begin{itemize}
\item float - floating point
\item double - double precision
\item int - (signed) integer
\item char - character
\item uchar - unsigned char
\item short - short integer
\item long - long integer
\item ulong - unsigned long integer
\end{itemize}
For example, to convert integers to floats
{\small\begin{verbatim}
% recast < data.ints  in=int out=float > data.floats
\end{verbatim}}\noindent
The name of this program derives from the fact that an explicit
type conversion in the C-language is called a ``cast.''

\subsection{TRANSP - Transposing Binary Data}

In the course of any of the operations, it is often necessary
to transpose datasets. In particular, data which are represented
in a multi-column ASCII format, will likely need to be transposed
after being converted from ASCII to binary with {\bf a2b}.
The reason for this is that it is convenient to have the fast
dimension of the data be the time (or depth) dimension
for seismic purposes. 

Our example above was a 5 column dataset, which you might think
of as  5 seismic traces, with the same number of samples in each
trace, side by side in the file. The processing
sequence
{\small\begin{verbatim}
% a2b < data.ascii n1=5 > data.binary 
\end{verbatim}}\noindent
will result in a file with the fast dimension being in the 
trace number direction, rather than the number of samples direction
The additional processing sequence
{\small\begin{verbatim}
% transp < data.binary n1=5 > data.transp
\end{verbatim}}\noindent
will put the data in the desirable form of the fast dimension being
in the number of samples direction, so that each trace is accessed
successively.

\subsection{FARITH - Performs simple arithmetic on binary data.}

It is often necessary to perform arithmetical operations on files,
or between two files of binary data. The program {\bf farith\/} has
been provided for many of these tasks.

Some of the tasks for single files supported by {\bf farith\/} include
\begin{itemize}
\item scaling value,
\item polarity reversal,
\item signum function,
\item absolute value,
\item exponential,
\item logarithm,
\item square root,
\item square,
\item inverse (punctuated),
\item inverse of square (punctuated),
\item inverse of square root (punctuated).
\end{itemize}
Binary operations (operations involving two files) include
value by value
\begin{itemize}
\item addition,
\item subtraction,
\item multiplication,
\item division,
\item cartesian product.
\end{itemize}
Seismic operations (which assume files consist of wavespeeds) include
\begin{itemize}
\item slowness perturbation (difference of inverses of files),
\item sloth perturbation (difference of inverses of files),
\end{itemize}
Examples of using {\bf farith\/} are
{\small \begin{verbatim}
% farith in=data.binary op=pinv out=data.out.bin
% farith in=data1.binary in2=data2.binary op=add > data.out2.bin
\end{verbatim}} \noindent


\section{Trace Header Manipulation}

The data format of SU inherits the seismic trace headers
from SEGY data. If your data are not SEGY, but are created
by conversion from some other type, there is a minimum collection
of headers that you will need to set, for the data to be compatible
with commonly used SU programs.

The issues of interest in this section are:
\begin{itemize}
\item adding trace headers,
\item removing trace headers,
\item pasting trace headers back on,
\end{itemize}

\subsection{SUADDHEAD - Adding SU Headers to Binary Data}
Once data have been put in the correct form, that is to say, 
an array of C-style floats organized with the fast dimension in
the direction of increasing sample number per trace, then
it is necessary to add headers so that these data may be accessible
to other SU programs.

If the data are SEGY, SEGD, DT1, or Bison/Geometrics data, 
read from a tape or diskfile, then the programs {\bf segyread},
{\bf segdread}, {\bf dt1tosu}, or {\bf bison2su} will have set
the trace header values so that the output will be ``SU data.''

For data read from some other means, such as an ASCII dataset
by {\bf a2b}, 
headers have to be added, this is done by using {\bf suaddhead}.
If our dataset consists of a file of binary C-style floats with,
say 1024 samples per trace, then the command sequence

{\small\begin{verbatim}
% suaddhead < data.bin ns=1024 > data.su
\end{verbatim} } \noindent
will yield the SU datafile ``data.su.''

\subsection{SUSTRIP - Strip SU headers SU data}
The inverse operation to {\bf suaddhead} is {\bf sustrip}.
Sometimes, you will have data with SU headers on it, but you may
want to export the data to another program that does not understand
SU headers. The command sequence
{\small\begin{verbatim}
% sustrip < data.su head=data.headers  >data.bin
\end{verbatim}}\noindent
will remove the SU headers and save them in the file ``data.headers.''

\subsection{SUPASTE - Paste SU Headers on to Binary Data}

Having performed an operation on the binary data, we may want to
paste the headers back on. This is done with {\bf supaste}.
The command sequence
{\small\begin{verbatim}
% supaste < data.bin head=data.headers  >data.su
\end{verbatim}}\noindent
will paste the headers contained in data.headers back on to the
data.

\section{Byte Swapping}
Even the best of human intentions can be circumvented. This is
the case with the IEEE floating point data format.
The implementation of the IEEE standard by chip manufacturers
has resulted in two types of commonly encountered data types.
These are called ``big-endian'' (high-byte) or ``little-endian''
(low-byte) types. Little-endian machines are Intel or Dec-based,
whereas big-endian is every other chip manufacturer.

Transporting data (either regular floating point, or SU data)
requires that the bytes be swapped, in order for the data to
be read. Two issues are necessary to address. These are
the issues of swapping the bytes in
\begin{itemize}
\item normal floating point data,
\item SU data.
\end{itemize}

Two programs are provided to perform these tasks. These are
\begin{itemize}
\item SWAPBYTES - SWAP the BYTES of various  data types
\item SUSWAPBYTES - SWAP the BYTES in SU data to convert data from big endian
\end{itemize}

\subsection{SWAPBYTES - Swap the Bytes of Binary (non-SU) Data}

If you transport binary data (data without SU headers) between 
platforms of a different ``endian'' (that is to say, the IEEE byte-order),
from the one you are working on, then you will have to swap the
bytes to make use of that data.

This problem arises because the order of the mantissa and exponent
of binary data comes in two different possible order
under the IEEE data standard.
The so-called ``big-endian'' or high-byte IEEE format is found on SGI,
SUN, IBM RS6000, and all Motorola chip-based systems. 
The ``little-endian'' or low-byte systems are systems that are based
on Intel and Dec chips.

The program {\bf swapbytes\/} is provided to do this for a variety of
data format types, but not SU data. For example,
{\small\begin{verbatim}
% swapbytes < data.bin in=float  >data.swap
\end{verbatim}}\noindent
will swap the bytes in a file of containing C-style floating point numbers.

\subsection{SUSWAPBYTES - Swap the Bytes of SU Data}

If you transport SU data (data with SU headers) between systems,
you have to swap both the data and the SU headers. The program
{\bf suswapbytes\/} is provided to do this.
The command sequence
{\small\begin{verbatim}
% suswapbytes < data.su format=0 > data.su.swapped
\end{verbatim}}\noindent
will swap data in the SU format from a machine of the reverse endian
to the byte order of your system.

The command sequence
{\small\begin{verbatim}
% suswapbytes < data.su format=1 > data.su.swapped
\end{verbatim}}\noindent
will swap data in the SU format that is in your system's byte
order to the opposite byte order.
You would use this if you were transporting your SU data from
you system to another system of the opposite byte order.

\section{Setting, Editing, and Viewing Trace Header Fields}

Seismic data can have a large number of parameters associated with
it. In SU data (following the example of the SEG-Y format) the
values of these parameters are stored in the header fields
of the traces.
There are a number of programs which access the header fields
allowing you to set, view, and modify those fields for a variety
of purposes.

The tasks of interest are
\begin{itemize}
\item adding SU headers,
\item stripping SU headers off of data and then pasting them back on, 
\item identifying SU keywords,
\item viewing the range of SU header values,
\item setting specified SU header fields,
\item computing a third header field from the values of two given fields,
\item getting the values of header fields,
\item editing specific header fields,
\end{itemize}
all of which are necessary, and may come under the heading of 
``geometry setting.''

The following list of programs will be discussed in this chapter
\begin{itemize}
\item SUADDHEAD - put headers on bare traces and set the tracl and ns fields 
\item SUSTRIP - remove the SEGY headers from the traces 
\item SUPASTE - paste existing SEGY headers on existing data
\item SUKEYWORD -- guide to SU keywords in segy.h 
\item SURANGE - get max and min values for non-zero header entries
\item SUSHW - Set one or more Header Words using trace number, mod and
integer divide to compute the header word values or input the header word
values from a file 
\item SUCHW - Change Header Word using one or two header word fields 
\item SUGETHW - Get the Header Word(s) in SU Data
\item SUEDIT - examine segy diskfiles and edit headers 
\item SUXEDIT - examine segy diskfiles and edit headers 
\end{itemize}

Some of these items were discussed in the previous section, but 
for completeness, let's recap what these programs do,
with a bit more information.

\subsection{SUADDHEAD - add SU (SEGY-style) Trace Headers}

To add headers to a datafile consisting of C-style binary floating
point numbers do:
{\small\begin{verbatim}
% suaddhead < data.bin ns=1024 > data.su
\end{verbatim}}\noindent
or for some other type, such as integers, use {\bf recast\/}
{\small\begin{verbatim}
% recast < data.ints in=int out=float | suaddhead ns=1024 > data.su
\end{verbatim}}\noindent
Here we have used a pipe $|$ to cascade the processing flow.

If the data are integers were originally from Fortran, then this is a likely
processing flow:
{\small\begin{verbatim}
% ftnstrip < data.fortran | recast in=int out=float | suaddhead ns=1024 > data.su
\end{verbatim}}\noindent

Other variations are obviously possible.
\subsection{SUSTRIP and SUPASTE - Strip and Paste SU Headers}
{\small\begin{verbatim}
% sustrip < data.su head=data.head > data.strip
... other processing that doesn't change the number of traces ...
% supaste < datanew.strip head=data.head > datanew.su
\end{verbatim}}\noindent
See the discussion of {\bf sustrip\/} and {\bf supaste\/} above.

\subsection{SUKEYWORD - See SU Keywords \label{sukeyword}}
The next 5 programs that are discussed in this section have
the option ``key='' in their selfdocs and the reference to
the trace header field ``keywords.''
As has been discussed in Chapter~2, {\bf sukeyword\/} is used to
determine just what these header keywords are.
Typing:
{\small\begin{verbatim}
% sukeyword -o
\end{verbatim}}\noindent
shows this list. (You may also see this list in Appendix~\ref{app:B}.)
However, of the 80+ fields which are defined in the SU header,
only a relatively small subset are used most of the time. These 
fields are  given by the {\bf sukeyword\/} listing as
{\small\begin{verbatim}
	int tracl;	/* trace sequence number within line */
	int tracr;	/* trace sequence number within reel */
	int cdp;	/* CDP ensemble number */
	int cdpt;	/* trace number within CDP ensemble */
 ...
	short trid;	/* trace identification code:
 ...			1 = seismic data

	int offset;	/* distance from source point to receiver
 ...
	int  sx;	/* X source coordinate */
	int  sy;	/* Y source coordinate */
	int  gx;	/* X group coordinate */
	int  gy;	/* Y group coordinate */
	short counit;	/* coordinate units code:
 ...
	short delrt;	/* delay recording time, time in ms between
			   initiation time of energy source and time
			   when recording of data samples begins
			   (for deep water work if recording does not
			   start at zero time) */
	unsigned short ns;	/* number of samples in this trace */
	unsigned short dt;	/* sample interval; in micro-seconds */

 ...
	/* local assignments */
	float d1;	/* sample spacing for non-seismic data */

	float f1;	/* first sample location for non-seismic data */

	float d2;	/* sample spacing between traces */

	float f2;	/* first trace location */
\end{verbatim}}\noindent
It is a good idea to be aware of this collection when using
data derived either from modeling programs or from field datasets.

\subsection{SURANGE - Get the Range of Header Values}

A useful piece of information about trace headers is to see
the range of values of the headers in a given dataset.
Typing, 
{\small\begin{verbatim}
% surange < data.su
\end{verbatim}}\noindent
will return the ranges of all SU header fields that are nonzero.
For example:
{\small\begin{verbatim}
% suplane | surange
32 traces:
 tracl=(1,32)  tracr=(1,32)  offset=400 ns=64 dt=4000
\end{verbatim}}\noindent
The program {\bf suplane\/} generates a test pattern whose header
values simulate a common-offset dataset. The default parameters for
{\bf suplane\/} are to make 32 traces, with 64 samples per trace
at 4 ms (4000 microseconds by SEG convention) sampling, with
offset=400. The output consists of three intersecting lines
of spikes.

Please note that corrupt data may show really strange values for
a large number of the header fields. Detecting such a problem
is one of the primary uses of {\bf surange}.

\subsection{SUGETHW - Get the Values of Header Words in SU Data}

Having header fields on data means that there are many additional
pieces of information to be kept track of, besides just the seismic
data, themselves. 
If you read data from a SEGY format tape, {\bf segyread\/} will
preserve the trace header information in the main part of the
SEGY header.
We saw above that {\bf surange\/} would permit us to see the min
and max header values over an entire dataset.
However, we often need to see the values of trace header
fields, trace by trace, and in an order that we choose.

The program {\bf sugethw\/} (pronounced, SU get header word)
is just such a utility. For example, the command sequence:
{\small\begin{verbatim}
% sugethw < data.su key=keyword1,keyword2,... | more
\end{verbatim} } \noindent
Will dump the values of each of the header fields specified by
the keywords listed.

A more tangible example, using {\bf suplane\/} data input via a pipe $|$
is
{\small\begin{verbatim}
% suplane | sugethw key=tracl,tracr,offset,dt,ns | more
 tracl=1         tracr=1        offset=400          dt=4000         ns=64       

 tracl=2         tracr=2        offset=400          dt=4000         ns=64       

 tracl=3         tracr=3        offset=400          dt=4000         ns=64       

 tracl=4         tracr=4        offset=400          dt=4000         ns=64       

 tracl=5         tracr=5        offset=400          dt=4000         ns=64       

 tracl=6         tracr=6        offset=400          dt=4000         ns=64       

 tracl=7         tracr=7        offset=400          dt=4000         ns=64       
...
\end{verbatim} } \noindent
There is no requirement regarding the order of the key words specified,
or the number of keywords, as long as at least one is specified.

If, for some reason, you need to dump the values in binary format,
an example, again using {\bf suplane\/} data
{\small\begin{verbatim}
% suplane | sugethw key=tracl,tracr,offset,dt,ns output=binary >  file.bin
\end{verbatim} } \noindent
outputs the values sequentially in order of keyword given, trace
by trace.

For ``geometry setting,'' you may want to use a command sequence
(again illustrated by piping suplane data into sugethw)

{\small\begin{verbatim}
% suplane | sugethw key=tracl,tracr,offset,dt,ns output=geom >  hdrfile
\end{verbatim} } \noindent

The contents of  hdrfile may be viewed via
{\small\begin{verbatim}
% more hdrfile

1 1 400 4000 64
2 2 400 4000 64 
3 3 400 4000 64 
4 4 400 4000 64 
5 5 400 4000 64 
...
\end{verbatim} } \noindent
where only the first 5 rows of data have been shown.
\subsection{SUSHW - Set the Header Words in SU Data}

The program {\bf sushw\/} (pronounced, SU set header word) is
an all purpose utility for setting the value of seismic trace
headers. This program permits the user to set one or more
trace header words.
A common use of sushw is to just set a particular field to
one value. For example, sometimes data don't have the ``dt''
field set. Let's say that the data are sampled at 2 ms,
By using {\bf sukeyword}, we see that dt is in microseconds
{\small\begin{verbatim}
% sukeyword dt

...skipping
        unsigned short ns;      /* number of samples in this trace */

        unsigned short dt;      /* sample interval; in micro-seconds */
...

\end{verbatim}} \noindent
This means that the following command sequence will set all dt values
to 2000 microseconds
{\small\begin{verbatim}
% sushw < data.su key=dt a=2000 > data.out.su
\end{verbatim}} \noindent
A more tangible example can be seen by piping {\bf suplane\/} data into
{\bf sushw\/}
{\small\begin{verbatim}
% suplane | sushw key=dt a=2000 | sugethw key=dt | more
161 wenzel> suplane | sushw key=dt a=2000 | sugethw key=dt | more

    dt=2000     

    dt=2000     

    dt=2000     

    dt=2000     

    dt=2000     
...
\end{verbatim}} \noindent

From the selfdoc for {\bf sushw\/} we see that the following optional
parameters are defined
{\small\begin{verbatim}
 Optional parameters ():						
 key=cdp,...			header key word(s) to set 		
 a=0,...			value(s) on first trace			
 b=0,...			increment(s) within group		
 c=0,...			group increment(s)	 		
 d=0,...			trace number shift(s)			
 j=ULONG_MAX,ULONG_MAX,...	number of elements in group		
\end{verbatim}} \noindent
These extra options permit more complicated operations
to be performed.
This is necessary, because often there is a relationship between
header fields and the position of the trace within the dataset.
The value of the header field is computed by the following formula
{\small\begin{verbatim}
 	i = itr + d							
 	val(key) = a + b * (i % j) + c * (i / j)			
 where itr is the trace number (first trace has itr=0, NOT 1)		
\end{verbatim}} \noindent
where percent \% indicates the ``modulo'' function and / is division.

For example if we want to 
set the sx field of the first 5 traces to 6400, the second 5 traces
    to 6300, decrementing by -100 for each 5 trace groups		
{\small\begin{verbatim}
% sushw < data.su key=sx a=6400 c=-100 j=5  > data.new.su
\end{verbatim}} \noindent
Again, piping in {\bf suplane\/} data into {\bf sushw\/}
{\small\begin{verbatim}
% suplane | sushw  key=sx a=6400 c=-100 j=5 | sugethw key=sx | more

    sx=6400     

    sx=6400     

    sx=6400     

    sx=6400     

    sx=6400     

    sx=6300     

    sx=6300     

    sx=6300     

    sx=6300     

    sx=6300     

    sx=6200     

    sx=6200     
...
\end{verbatim}} \noindent

As another example, if we wanted set the ``offset'' fields of each group
of 5 traces to 200,400,...,6400
{\small\begin{verbatim}
%  sushw  < data.su key=offset a=200 b=200 j=5 > data.out.su
\end{verbatim}} \noindent
As before, piping {\bf suplane\/} data into {\bf sushw\/} yields the following
{\small\begin{verbatim}
% suplane | sushw  key=offset a=200 b=200 j=5 | sugethw key=offset | more

offset=200      

offset=400      

offset=600      

offset=800      

offset=1000     

offset=200      

offset=400      

offset=600      

offset=800      

offset=1000     

offset=200      

...
\end{verbatim}} \noindent

We can perform all 3 operations with one call to {\bf sushw}, via:
{\small\begin{verbatim}
% sushw < data.su key=dt,sx,offset a=2000,6400,200 b=0,0,200 c=0,-100,0 j=0,5,5 > newdata.su
\end{verbatim}} \noindent
Or with {\bf suplane\/} data piped in
{\small\begin{verbatim}
% suplane | sushw key=dt,sx,offset a=2000,6400,200 b=0,0,200 c=0,-100,0 j=0,5,5 |
 sugethw key=dt,sx,offset | more

    dt=2000         sx=6400     offset=200      

    dt=2000         sx=6400     offset=400      

    dt=2000         sx=6400     offset=600      

    dt=2000         sx=6400     offset=800      

    dt=2000         sx=6400     offset=1000     

    dt=2000         sx=6300     offset=200      

    dt=2000         sx=6300     offset=400      

    dt=2000         sx=6300     offset=600      

    dt=2000         sx=6300     offset=800      

    dt=2000         sx=6300     offset=1000     

    dt=2000         sx=6200     offset=200      

    dt=2000         sx=6200     offset=400      
...
\end{verbatim}} \noindent
As you can see, it is natural to use pipes and redirects to control
job flow, but this becomes ungainly on a single command line.
Later in this document, we will see how to construct complicated
processing sequences in the controlled environment of shell scripts.

\subsection{Setting Geometry - Converting Observers' Logs to Trace Headers}
There is an often unpleasant task called ``setting geometry''
which must be performed on field data.
Often, a SEGY tape will have only a rudimentary set of header
fields set in the data.
The rest of the information (shot location, geophone location, etc...)
will be supplied in the form of observers' logs. 

For setting geometry, you may wish to dump a specific
collection of header fields of interest into a file,
read the file into a text editor or spreadsheet program
so that you can make changes.

For example, if you have some file ``sudata'' which has
some header fields set incorrectly or incompletely
then the following command sequence illustrates a
possible way of working with such data.
You begin by reading the selected header fields
into a file ``hdrfile''.
{\small\begin{verbatim}
% sugethw < sudata output=geom key=key1,key2,... > hdrfile 		
\end{verbatim} } \noindent
Now edit the ASCII file hdrfile with any editor, setting the fields	
appropriately. Convert hdrfile to a binary format via:		
{\small\begin{verbatim}
% a2b < hdrfile n1=nfields > binary_file				
\end{verbatim} } \noindent
were ``nfields'' is the number of header fields in the ``key=..''
list above.
Then load the new file of header fields via:					
{\small\begin{verbatim}
% sushw < sudata infile=binary_file key=key1,key2,... > sudata.edited
\end{verbatim} } \noindent
Again, ``key=key1,key2,...'' here is the same list as in the
{\bf sugethw\/} statement above.
The finished product is the file sudata.edited.

If you are just beginning to set the header fields,
you may build the ASCII header file  ``hdrfile''  any way you
want. This could be with your favorite text editor, or with
a spreadsheet program. It is not important how the ascii file is
created, as long as it is in multi-column ASCII format for
the sequence above.

Of course, if you have the header values in a file consisting
of C-style floats, (which you can make either from a C-program,
or from Fortran data with {\bf ftnstrip}) listed trace-by-trace, 
then you already have the ``binary\_file'' and need only execute the final
sequence.

\subsection{SUCHW - Change (or Compute) Header Words in SU Data}

Some header fields such as ``cdp'' may be computed from existing
header fields. The program {\bf suchw\/} provides this functionality.

From the selfdoc of {\bf suchw},
{\small \begin{verbatim}
...
 key1=cdp,...	output key(s) 						
 key2=cdp,...	input key(s) 						
 key3=cdp,...	input key(s)  						

...
 a=0,...		overall shift(s)				
 b=1,...		scale(s) on first input key(s) 			
 c=0,...		scale on second input key(s) 			
 d=1,...		overall scale(s)				
\end{verbatim}}\noindent
we can see that this program uses the values of 2 header fields,
key2 and key3, to compute a third, key3, via the equation 
{\small \begin{verbatim}
...
	val(key1) = (a + b * val(key2) + c * val(key3)) / d		
..
\end{verbatim}}\noindent

For example, to shift the values of the  cdp header field by a constant
amount, say $-1$
{\small \begin{verbatim}
% suchw <data >outdata a=-1					
\end{verbatim}}\noindent
or to add a constant amount, say 1000, to a header field, say ``tracr,''
{\small \begin{verbatim}
% suchw key1=tracr key2=tracr a=1000 <infile >outfile		
\end{verbatim}}\noindent

Another possible example is that of setting the ``gx'' 
field by summing the offset and ``sx'' (shot point) values
using {\bf sushw\/} and then computing the  ``cdp'' field
by averaging the ``sx'' and ``gx.''
Here, we are using the
actual cpp locations as the cdp numbers, instead of
the conventional 1, 2, 3, ... enumeration
{\small \begin{verbatim}
% suchw <indata key1=gx key2=offset key3=sx b=1 c=1 |			
% suchw key1=cdp key2=gx key3=sx b=1 c=1 d=2 >outdata			
\end{verbatim}}\noindent

It is possible to perform both operations in one call via:
{\small \begin{verbatim}
%  suchw<indata key1=gx,cdp key2=offset,gx key3=sx,sx b=1,1 c=1,1 d=1,2 >outdata
\end{verbatim}}\noindent

\subsection{SUEDIT and SUXEDIT - Edit the Header Words in SU Data}

Finally, it may be that you wish to examine, and possibly change
just  few headers. For this purpose, we have {\bf suedit\/} and
{\bf suxedit}.
the SU editing programs are executed via:
{\small \begin{verbatim}
% suedit diskfile  (open for possible header modification if writable)	
% suedit <diskfile  (open read only)					
\end{verbatim}}\noindent
and permit the interactive viewing and editing of the header
fields.

For example, making test data with {\bf suplane\/}
{\small \begin{verbatim}
% suplane > data.su
% suedit  data.su
\end{verbatim}}\noindent
yields the following 
{\small \begin{verbatim}
32 traces in input file
 tracl=32 tracr=32 offset=400 ns=64 dt=4000
>                          <------- prompt for interactive use 
\end{verbatim}}\noindent

The commands that may be used interactively in {\bf suedit\/} and 
{\bf suxedit\/}
may be seen by typing a question mark (?) at the prompt. For
example
{\small \begin{verbatim}
32 traces in input file
 tracl=32 tracr=32 offset=400 ns=64 dt=4000
> ?

 n              read in trace #n
 <CR>           step
 +              next trace;   step -> +1
 -              prev trace;   step -> -1
 dN             adv N traces; step -> N
 %              percentiles
 r              ranks
 p [n1 [n2]]    tabplot
 ! key=val      modify field
 ?              print this file
 q              quit
>
\end{verbatim}}\noindent
This program allows the user to view traces as a tabplot of
the data sample values or view or change individual header values

The program {\bf suxedit\/} is similar to {\bf suedit}, with the addition
of X-windows graphics for plotting traces
{\small \begin{verbatim}
% suxedit diskfile  (open for possible header modification if writable)	
% suxedit <diskfile  (open read only)					
\end{verbatim}}\noindent
{\small \begin{verbatim}
% suxedit data.su
32 traces in input file
 tracl=32 tracr=32 offset=400 delrt=5 ns=64 dt=4000

> ?

 n              read in trace #n
 <CR>           step
 +              next trace;   step -> +1
 -              prev trace;   step -> -1
 dN             adv N traces; step -> N
 %              percentiles
 r              ranks
 p [n1 [n2]]    tabplot
 g [tr1 tr2] ["opts"]   wiggle plot
 f              wig plot Fourier Transf
 ! key=val      modify field
 ?              print this file
 q              quit   
\end{verbatim}}\noindent
Again, the options of this program are largely self-explanatory.
Please note that the selfdoc is more informative than the help
menu given by typing the question mark ``?.''

\chapter{Viewing SU Data in X-Windows and PostScript}

The Seismic Unix package has a small collection of graphics utilities
for plotting data, both in general C-style float format, and in
SU format, both in the X-windows environment, for screen viewing
and in the PostScript form for hard copy.

The types of plotting that are available in SU are
\begin{itemize}
\item contour plots,
\item gray or colorscale image plots,
\item wiggle trace plots,
\item line or symbol graphs,
\item movies,
\item 3D cube plots (PostScript only).
\end{itemize}

These programs have lengthy selfdocs, reflecting the large number
of options for selecting the appearance and labeling of the plots.
However, functionality such as windowing data, should be done via
the programs {\bf subset\/} or {\bf suwind\/} as a preprocessing step
before the data are actually sent to the plotting program.
These plotting programs are purposely not designed to window data,
as raw seismic datasets are often huge.

Also, following the ``small is beautiful'' philosophy of Unix,
we have seprate, but more or less equivalent codes for generating
PostScript output for hardcopy plotting.

\section{X-Windows Plotting Programs}

X-windows provides a unified environment for the creation of
screen graphics routines, which can be quite portable,
provided that the code is written using only the items that
can be guaranteed to come with the general distributions of X.

Therefore, all of our X-windows codes are written using straight
X calls, or with the X-Toolkit. While coding in such widget sets
as Motif is easier in many respects, the code that results
is not nearly as portable. There are often great differences
between the implementation of commercial widget sets on various
platforms.

\subsection{Plotting General Floating Point Data}

The programs that are used for viewing general floating point
data (data without SU headers) in the X-window environment are
\begin{itemize}
\item XCONTOUR - X CONTOUR Plot of f(x1,x2) via vector plot call,
\item XIMAGE - X IMAGE plot of a uniformly-sampled function f(x1,x2),
\item XWIGB - X WIGgle-trace plot of f(x1,x2) via Bitmap,
\item XGRAPH - X GRAPHer Graphs n[i] pairs of (x,y) coordinates,
\item XMOVIE - image one or more frames of a uniformly sampled 
function f(x1,x2).
\end{itemize}

Try the following. Make some binary data by stripping the headers
off of some SU data. For example
{\small\begin{verbatim}
% suplane | sustrip > data.bin
n1=64 n2=32 d1=0.004000
nt=64 ntr=32 dt=0.004000
ns=64
\end{verbatim}\noindent
The items which follow indicate that the dimensions of the data
set are ``n1=64'' by ``n2=32.''
Now view these data in each of the plotting programs listed above
(except {\bf xgraph}) via:
{\small\begin{verbatim}
% xcontour < data.bin n1=64 n2=32 title="contour"  &
% ximage < data.bin n1=64 n2=32  title="image"  &
% xwigb < data.bin n1=64 n2=32  title="wiggle trace"  &
% xmovie < data.bin n1=64 n2=32  title="movie"  &
\end{verbatim}}\noindent
Please note that the ampersand ``\&'' is a Unix command
telling the working shell to run the program in background.

To test {\bf xgraph}, make an ASCII file containing  a double
column listing of pairs of data to be plotted such as the
following
{\small\begin{verbatim}
1 1
2 1.5
3 3
4 8
10 7
\end{verbatim}\noindent
Call this file ``data.ascii''
Then use {\bf a2b\/} to convert the file to binary and then plot it
with {\bf xgraph\/}
{\small\begin{verbatim}
% a2b < data.ascii n1=2 > data.bin
n=5
% xgraph < data.bin n=5
\end{verbatim}}\noindent
Note that the ``n=5'' that is echoed by a2b is the same as
the input to xgraph.

When you are finished, and wish to get rid of the windows,
you may click on the window and type the letter ``q,'' for quit.
On some systems, you may have to actually select the ``destroy''
option by clicking and dragging on the square in the upper left
hand side of the window frame.

Please note that all of these programs have a huge number of
options, reflecting a fairly large collection of functionalities.
See the selfdoc of each of these programs by typing
{\small\begin{verbatim}
% programname
\end{verbatim}}\noindent
or
{\small\begin{verbatim}
% sudoc programname
\end{verbatim}}\noindent

\subsection{X-Windows Plotting of SU Data}

To plot data that are in the SU format, a number of programs
(many of which have parallel functionality to those already discussed)
have been created.
These are
\begin{itemize}
\item SUXCONTOUR - X CONTOUR plot of Seismic UNIX tracefile via vector
plot call,
\item SUXIMAGE - X-windows IMAGE plot of an SU data set,
\item SUXWIGB - X-windows Bit-mapped WIGgle plot of an SU data set,
\item SUXGRAPH - X-windows GRAPH plot of an SU data set, 
\item SUXMOVIE - X MOVIE plot of an SU data set,
\item SUXMAX - X-windows graph of the MAX, min, or absolute max value on     
each trace of an SU data set.
\end{itemize}
Rather than maintain multiple codes, each of these programs actually
calls one or more of the X-windows graphics programs listed in the
previous subsection. Please note, that the selfdoc of the non-SU
version of a given graphics program also applies to the SU version,
meaning that, these programs also have a large functionality.

You may test each of these programs using suplane data via:
{\small\begin{verbatim}
% suplane | suxcontour title="contour"  &
% suplane | suximage  title="image"  &
% suplane | suxwigb  title="wiggle trace"  &
% suplane | suxgraph  title="graph" &
% suplane | suxmovie  title="movie" &
% suplane | suxmax    title="max"  &
\end{verbatim}}\noindent
Again, note that the ampersand ``\&'' is a Unix command
telling the working shell to run the program in background.
When you are finished, and wish to get rid of the windows,
you may click on the window and type the letter ``q,'' for quit.
On some systems, you may have to actually select the ``destroy''
option by clicking and dragging on the square in the upper left
hand side of the window frame.

\subsection{Special Features of X-Windows Programs}
To find out all of the many options of these programs, please
see their selfdocs, by typing the names of each of these programs
on the commandline:
{\small\begin{verbatim}
% suxcontour
% suxwigb
% suximage
% suxmovie
% suxmax
\end{verbatim}}\noindent

In addition, the names of the non-SU versions of some of these programs
may be typed to yield additional information
{\small\begin{verbatim}
% xcontour
% xwigb
% ximage
% xmovie
\end{verbatim}}\noindent

However there are some properties that these programs have that
can only be illustrated with examples.

\subsubsection{Plotting wiggle traces in true offset with SUXWIGB}
It is possible to plot wiggle traces in true offset, that is to
say, to take the values for the horizontal dimension of the wiggle
plot from the values in the header.
This is done with the ``key='' parameter.

For example, lets make some test data with {\bf suplane\/} and
plot it using the ``key=offset'' via:
{\small\begin{verbatim}
% suplane | suchw key1=offset key2=tracl a=0 b=100  | suxwigb key=offset   &
\end{verbatim}}\noindent
The result is a plot with the x2 axis labeled in the values of the offset
header field (which count by 100's).

\subsubsection{Making a movie with SUXMOVIE}

It is possible to make movies of seismic data with {\bf suxmovie\/}.
An example of this is to make several synthetic data panels with {\bf  suplane\/}
appending each successive panel with the double redirect sign ``>>''

{\small\begin{verbatim}
% suplane > junk1.su
% suplane | suaddnoise sn=20 >> junk1.su
% suplane | suaddnoise sn=15 >> junk1.su
% suplane | suaddnoise sn=10 >> junk1.su
% suplane | suaddnoise sn=5 >> junk1.su
% suplane | suaddnoise sn=3 >> junk1.su
% suplane | suaddnoise sn=2 >> junk1.su
% suplane | suaddnoise sn=1 >> junk1.su

% suxmovie < junk1.su n2=32 title="frame=%g" loop=1  &
\end{verbatim}}\noindent
The final command has ``n2=32'' set to show that there are 32 traces
per frame of data. The usage of ``\%q'' permits the frame
number to be listed as part of the title, and ``loop=1'' runs the
movie in a continuous loop.

To make the movie go faster or slower, simply enlarge or shrink the
window by clicking and dragging on the lower right corner of the plot.
Clicking the far-right mouse button once will freeze the frame, and clicking
it a second time will start the movie again.

\subsection{PostScript Plotting Programs}

To complement our collection of X-Windows plotting utilities
are a collection of very similar PostScript codes.
The idea was to create PostScript plotting codes which would
correspond to each of the X-Windows codes listed above.

\subsection{PostScript Plotting of General Floating Point Data}

The programs that are used for PostScript plotting of general
floating point data (data without SU headers) are
\begin{itemize}
\item PSCONTOUR - PostScript CONTOURing of a two-dimensional function f(x1,x2),
\item PSIMAGE - PostScript IMAGE plot of a uniformly-sampled function f(x1,x2),
\item PSCUBE - PostScript image plot of a data CUBE,
\item PSGRAPH - PostScript GRAPHer Graphs n[i] pairs of (x,y) coordinates,
\item PSMOVIE - PostScript MOVIE plot of a uniformly-sampled function f(x1,x2,x3),
\item PSWIGB - PostScript WIGgle-trace plot of f(x1,x2) via Bitmap,
\item PSWIGP - PSWIGP - PostScript WIGgle-trace plot of f(x1,x2) via Polygons.
\end{itemize}

Again, you may create binary data to test these programs by stripping
off the headers of some suplane data.
{\small\begin{verbatim}
% suplane | sustrip > data.bin
n1=64 n2=32 d1=0.004000
nt=64 ntr=32 dt=0.004000
ns=64
\end{verbatim}\noindent
The dimensions of the data are n1=64 samples per trace by n2=32 traces.
{\small\begin{verbatim}
% pscontour < data.bin n1=64 n2=32 title="contour" > data1.eps
% psimage < data.bin n1=64 n2=32  title="image"  > data2.eps
% pscube < data.bin n1=64 n2=32  title="cube plot"  > data4.eps
% pswigb < data.bin n1=64 n2=32  title="bitmap wiggle trace"  > data3.eps
% pswigp < data.bin n1=64 n2=32  title="wiggle trace"  > data4.eps
% psmovie < data.bin n1=64 n2=32  title="movie"  > data5.eps
\end{verbatim}}\noindent
The output files contain Adobe Level 2 Encapsulated PostScript.
You should be able to view these files with any standard X-windows
PostScript previewer (such as ``ghostview'').

Please note, that the output from ``psmovie'' may not work on your
system. This output works under NeXTStep, but is multi-page Encapsulated
PostScript, which is not generally supported by PostScript devices.

To test psgraph, make an ascii file containing  a double
column listing of pairs of data to be plotted such as the
following
{\small\begin{verbatim}
1 1
2 1.5
3 3
4 8
10 7
\end{verbatim}\noindent
Call this file ``data.ascii''
Then use ``a2b'' to convert the file to binary and then plot it
with psgraph
{\small\begin{verbatim}
% a2b < data.ascii n1=2 > data.bin
n=5
% psgraph < data.bin n=5 > data6.eps
\end{verbatim}}\noindent
Note that the ``n=5'' that is echoed by {\bf a2b\/} is the same as
the input to {\bf psgraph}.
This permits the following trick to be used
{\small\begin{verbatim}
% a2b < data.ascii outpar=junk.par n1=2 > data.bin
% psgraph < data.bin par=junk.par > data6.eps
\end{verbatim}}\noindent
to yield the same output.

\subsection{PostScript Plotting of SU Data}

Just as there are X-Windows codes for plotting SU data, there
are also codes for making PostScript plots of these data.
The programs for PostScript graphics are
\begin{itemize}
\item SUPSCONTOUR - PostScript CONTOUR plot of an SU  data set 
\item SUPSIMAGE - PostScript IMAGE plot of an SU data set
\item SUPSCUBE - PostScript CUBE plot of an SU data set
\item SUPSGRAPH - PostScript GRAPH plot of an SU data set
\item SUPSWIGB - PostScript Bit-mapped WIGgle plot of an SU data set
\item SUPSWIGP - PostScript Polygon-filled WIGgle plot of an SU data set
\item SUPSMAX - PostScript of the MAX, min, or absolute max value on
each trace of a SU data  set
\end{itemize}

We can use {\bf suplane\/} data to test each of these programs
as we did with the X-Windows codes
{\small\begin{verbatim}
% suplane > junk.su
\end{verbatim}}\noindent
{\small\begin{verbatim}
% supscontour < junk.su title="contour" > data1.eps
% supsimage < junk.su title="image" label1="sec" label2="trace number"  > data2.eps
% supscube < junk.su  title="cube plot"  > data4.eps
% supswigb < junk.su title="bitmap wiggle trace"  > data3.eps
% supswigp < junk.su title="wiggle trace"  > data4.eps
% supsmovie  < junk.su title="movie"  > data5.eps
% supsmax < junk.su title="max"  > data5.eps
\end{verbatim}}\noindent
The output files contain Adobe Level 2 Encapsulated PostScript,
and is compatible with TeX, LaTeX, and many draw tools.
You will need to use a PostScript previewer, such as GhostScript
or Ghostview to view these files on the screen.

Please note again, that programs for which there is a non-SU version,
have selfdoc information which applies to these codes, as well.

\section{Additional PostScript Support}
There are several additional tools for supporting PostScript
operations in SU. These are
\begin{itemize}
\item PSBBOX - change BoundingBOX of existing PostScript file
\item PSMERGE - MERGE PostScript files 
\item MERGE2 - MERGE2 PostScript figures onto one page 
\item MERGE4 - MERGE4 figures onto one page
\item PSLABEL - output PostScript file consisting of a single TEXT string
on a specified background. (Use with psmerge to label plots.)
\item PSMANAGER - printer MANAGER for HP 4MV and HP 5Si Mx Laserjet
PostScript printing 
\item PSEPSI - add an EPSI formatted preview bitmap to an EPS file
\end{itemize}

The programs {\bf psbbox}, {\bf pslabel}, {\bf psmerge}, {\bf merge2},
and {\bf merge4\/} are designed to help in constructing figures
made from SU-style graphics programs, and are not guaranteed to
work with EPS files generated by other means.

\subsection{PSBBOX - Changing the BoundingBox}
For example, create test PostScript data with {\bf suplane\/} and
{\bf supswigb\/} via
{\small\begin{verbatim}
% suplane | supswigb > junk1.eps
\end{verbatim}}\noindent
where the ``.eps'' extension is chosen as a reminder that this is
encapsulated PostScript.
Let's say that there is too much white-space surrounding this figure.
To fix this problem, we want to change the size of the BoundingBox
at the top of the file. For example
{\small\begin{verbatim}
% more junk1.eps
\end{verbatim}}\noindent
shows the dimensions of the BoundingBox
{\small\begin{verbatim}
%!PS-Adobe-2.0 EPSF-1.2
%%DocumentFonts:
%%BoundingBox: 13 31 603 746  
...
\end{verbatim}}\noindent
We could manually edit this, but with {\bf psbbox\/} we can
type:
{\small\begin{verbatim}
% psbbox < junk1.eps llx=40 lly=80 urx=590 ury=730 > junk2.eps
Original:  %%BoundingBox: 13 31 603 746
Updated:   %%BoundingBox: 40 80 590 730 
\end{verbatim}}\noindent
to yield a smaller BoundingBox, with correspondingly less white
space.

\subsection{PSMERGE, MERGE2, MERGE4 - Merging PostScript Plots}

It is often useful to merge several plots to make a compound
figure.
The program {\bf psmerge\/} is the general tool in SU provided
for this. 
There are two additions shell scripts, which call {\bf psmerge\/}
called {\bf merge2\/} and {\bf merge4}.
If we make a couple of test datasets
{\small\begin{verbatim}
% suplane > junk.su
% suplane | sufilter > junk1.su
\end{verbatim}}\noindent
and display these by various means
{\small\begin{verbatim}
% supswigb < junk.su title="Wiggle trace" label1="sec" label2="trace number" > junk1.eps
% supsimage < junk.su title="Image Plot" label1="sec" label2="trace number" > junk2.eps
% supscontour < junk.su title="Contour Plot" label1="sec" label2="trace number" > junk3.eps
% supswigb < junk1.su title="Filtered" label1="sec" label2="trace number" > junk4.eps
\end{verbatim}}\noindent
we now have 4 PostScript files which can be merged to make new plots.

Merging 2 plots may be done via:
{\small\begin{verbatim}
% merge2 junk1.eps junk2.eps > junk.m2.eps
\end{verbatim}}\noindent
while merging all 4 plots may be done via 
{\small\begin{verbatim}
% merge2 junk1.eps junk2.eps junk3.eps junk4.eps > junk.m4.eps
\end{verbatim}}\noindent
Of course, neither {\bf merge2\/} nor {\bf merge4\/} are robust
enough to handle all plot sizes, so you may need to manually 
merge plots with psmerge. Also, if you want to overlay plots,
such as a graph on top of a wiggle trace plot, then you will
also need to use {\bf psmerge}.

Here is an example of creating plots and merging them with {\bf psmerge\/}.
Because the command sequence is ungainly for typing on the commandline
it is expressed as a shell script

{\small\begin{verbatim}
#! /bin/sh
# shell script for demonstrating PSMERGE

# make data
suplane > junk.su
suplane | sufilter > junk1.su

# make PostScript Plots of data
supswigb < junk.su wbox=6 hbox=2.5 \
 title="Wiggle trace" label1="sec" label2="traces" > junk1.eps
supscontour < junk.su wbox=2.5 hbox=2.5  \
 title="Contour Plot" label1="sec" label2="traces" > junk3.eps
supswigp < junk1.su wbox=2.5 hbox=2.5 \
 title="Filtered" label2="traces" > junk4.eps

# merge PostScript plots
psmerge in=junk1.eps translate=0.,0. \
 in=junk3.eps translate=0.0,3.7 \
 in=junk4.eps translate=3.3,3.7 > junk5.eps

echo "You may view the files: junk1.eps, junk3.eps, junk4.eps, junk5.eps"
echo "with your PostScript Previewer"

exit 0
\end{verbatim}}\noindent
In this case, the original files were made small to fit within an
8-1/2'' by 11'' window.
However, {\bf psmerge\/} has the capability of scaling plots.
(This is how the {\bf merge2\/} and {\bf merge4\/} shells work.
You can examine the texts of these for further information by typing
{\small\begin{verbatim}
% more $CWPROOT/bin/merge2
or
% more $CWPROOT/bin/merge4
\end{verbatim}}\noindent
An additional example of merging 3 plots of different sizes is given
by the following shell script
{\small\begin{verbatim}
#! /bin/sh
# shell script for demonstrating PSMERGE

# make data
suplane > junk.su
suplane | sufilter > junk1.su

# make PostScript Plots of data
supswigb < junk.su wbox=7 hbox=4 \
 title="Wiggle trace" label1="sec" label2="traces" > junk1.eps
supscontour < junk.su \
 title="Contour Plot" label1="sec" label2="traces" > junk3.eps
supswigp < junk1.su label1="sec" \
 title="Filtered" label2="traces" > junk4.eps

# merge PostScript plots
psmerge in=junk1.eps translate=0.,0. scale=.6,.6 \
 in=junk3.eps scale=.4,.4 translate=0.0,3.7 \
 in=junk4.eps scale=.4,.4 translate=3.3,3.7 > junk5.eps

echo "You may view the files: junk1.eps, junk3.eps, junk4.eps, junk5.eps"
echo "with your PostScript Previewer"

exit 0
\end{verbatim}}\noindent

In this case, the plots are of normal size, and are then scaled to
fit within an 8-1/2'' by 11'' window.

\section{Trace Picking Utilities}
For lack of a better location to discuss this is the subject,
we will now list``trace picking'' utilities.
The X-Windows wiggle trace, image, and contour plotting programs
all have the attribute that by placing the cursor on the point to
be picked and typing the letter `s' the coordinates of the point
are saved in memory. When the letter `q' is typed, the values
are saved in a user-specified file ``mpicks.''

There are two additional programs which are dedicated to picking
issues. These are
\begin{itemize}
\item XPICKER - X wiggle-trace plot of f(x1,x2) via Bitmap with PICKing 
\item SUXPICKER - X-windows  WIGgle plot PICKER of an SU data set
\item SUPICKAMP - pick amplitudes within user defined and resampled window 
\end{itemize}
with ``xpicker'' being the non-SU data version which ``suxpicker''
calls. The xpicker/suxpicker programs are interactive tools for picking.
The program ``supickamp'' is a simple automated picking program,
which seeks the largest amplituded within a user-specified window.

See demos in \$CWPROOT/src/demos/Picking
for more information about ``supickamp.''

\section{Editing SU Data}

Once the data are read in, and the headers are set properly,
then there will often be manipulation and data editing issues
to be dealt with.

It is often necessary to perform tasks of varying so
that data may be
\begin{itemize}
\item windowed,
\item sorted,
\item truncated,
\item tapered,
\item zeroed,
\item made uniform in number of samples,
\item concatenated,
\end{itemize}
which are dataset editing issues.

This section, therefore will deal with the following programs
\begin{itemize}
\item SUWIND - window traces by key word 
\item SUSORT - sort on any segy header keywords 
\item SURAMP - Linearly taper the start and/or end of traces to zero. 
\item SUTAPER - Taper the edge traces of a data panel to zero. 
\item SUNULL - create null (all zeroes) traces 
\item SUZERO -- zero-out data within a time window
\item SUKILL - zero out traces 
\item SUMUTE - mute above (or below) a user-defined polygonal curve with
the distance along the curve specified by key header word
\item SUVLENGTH - Adjust variable length traces to common length 
\item SUVCAT - append one data set to another (trace by trace)
\end{itemize}
which provide a variety of trace editing utilities.

\subsection{SUWIND - window traces by key word}

It is very common to view or process only a subset of a
seismic dataset. Because seismic data have a number of parameters
that we may want to window the data about, ``suwind'' has been
written.

\subsubsection{Windowing by trace header field}
In its simplist usage, ``suwind'' permits the user to set
min and max values of a specific header field 
{\small \begin{verbatim}
       key=tracl       Key header word to window on (see segy.h)       
       min=LONG_MIN    min value of key header word to pass            
       max=LONG_MAX    max value of key header word to pass            
\end{verbatim}}\noindent

For example, windowing suplane data by trace number yields
{\small \begin{verbatim}
% suplane  | suwind key=tracl  min=5 max=10 | sugethw key=tracl | more

 tracl=5

 tracl=6

 tracl=7

 tracl=8

 tracl=9

 tracl=10   
\end{verbatim}}\noindent
On a large dataset, the ``count'' parameter should be used,
instead of setting the max value. If you set an explicit
``max'' value, suwind will have to go through the entire
dataset to capture all possible traces with values between
the min and max value, because the program assumes that multiple
occurrences of trace labeling are possible.
For example, compare
{\small \begin{verbatim}
% suplane ntr=100000 | suwind key=tracl min=5 max=10 | sugethw tracl | more
\end{verbatim}}\noindent
(it's ok to type ``control-c'' after a few minutes) with
{\small \begin{verbatim}
%suplane ntr=100000 | suwind key=tracl min=5 count=5 | sugethw tracl | more                                                               
\end{verbatim}}\noindent
where suplane has been set to create 100000 traces in each case.

More sophisticated windowing, (decimating data, for example)
{\small \begin{verbatim}
       j=1             Pass every j-th trace ...                       
       s=0             ... based at s  (if ((key - s)%j) == 0)         
\end{verbatim}} \noindent
can be illustrated with suplane data by showing every 2nd trace
{\small \begin{verbatim}
% suplane  | suwind key=tracl j=2 | sugethw key=tracl | more

 tracl=2

 tracl=4

 tracl=6

 tracl=8

 tracl=10
...
\end{verbatim}} \noindent
or by every 2nd trace, based at 1
{\small \begin{verbatim}
% suplane  | suwind key=tracl j=2 s=1 | sugethw key=tracl | more

 tracl=1

 tracl=3

 tracl=5

 tracl=7

 tracl=9         

...
\end{verbatim} } \noindent

Accepting and rejecting traces is also possible with ``suwind.''

{\small \begin{verbatim}
...
       reject=none     Skip traces with specified key values           
...    accept=none     Pass traces with specified key values(see notes)
\end{verbatim} } \noindent
The reject parameter is a straightforward rejecting of numbered
traces.
For example
{\small \begin{verbatim}
suplane | suwind key=tracl reject=3,8,9 | sugethw key=tracl | more
 tracl=1

 tracl=2

 tracl=4

 tracl=5

 tracl=6

 tracl=7

 tracl=10

 tracl=11

 tracl=12    
\end{verbatim} } \noindent
traces 3, 8, and 9 are rejected.

The accept option is a bit strange--it does {\em not\/} mean
accept {\em only\/}
the traces on the accept list!  It means accept these traces,   
even if they would otherwise be rejected.
For example:

{\small \begin{verbatim}
suplane | suwind key=tracl reject=3,8,9 accept=8 | sugethw key=tracl
| more
 tracl=1

 tracl=2

 tracl=4

 tracl=5

 tracl=6

 tracl=7

 tracl=8

 tracl=10

 tracl=11   
....
\end{verbatim} } \noindent

If you want to {\em accept only \/} the traces listed, then you
need to set ``max=0''

{\small \begin{verbatim}
% suplane | suwind key=tracl accept=8 max=0 | sugethw key=tracl | more
 tracl=8 
\end{verbatim} } \noindent
Only trace 8 is passed in this example.

The count parameter overrides the accept    
parameter, so you can't specify count if you want true          
unconditional acceptance.

See the demos in \$CWPROOT/src/demos/Selecting\_Traces for specific
examples.

\subsubsection{Time gating}

The second issue windowing is time gating.
In fact, when people want to window data, they often want to
do both trace and time gating.
{\small \begin{verbatim}
 Options for vertical windowing (time gating):                         
       tmin = 0.0              min time to pass                        
       tmax = (from header)    max time to pass                        
       itmin = 0               min time sample to pass                 
       itmax = (from header)   max time sample to pass                 
       nt = itmax-itmin+1      number of time samples to pass          
\end{verbatim}} \noindent
The result of setting either ``itmin and itmax'' or ``tmin
and tmax'' will be to create a time gate which is to the
nearest sample. If you want to time gate to an arbitrary
(inter-sample) window, then this is a problem in data resampling
and ``suresamp'' is the program of choice.

\subsection{SUSORT - sort on any SEGY header keywords}

One of the advantages of working in the Unix operating system is
that when a superior Unix system call for a particular operation
exists, it is advantageous to use that call to perform the necessary
task. Such a task is sorting, and the Unix ``sort'' command is
just such a utility.

Susort takes advantage of the Unix system sort command to permit
the sorting of traces by header field key work.

For example, sorting data (with values in ascending order)
for two fields (cdp and offset) would be done via:
{\small\begin{verbatim}
% susort <indata.su >outdata.su cdp offset			
\end{verbatim}} \noindent
In decending order for, offset, and ascending order for cdp
{\small\begin{verbatim}
% susort <indata.su >outdata.su cdp -offset			
\end{verbatim}} \noindent
 Note:	Only the following types of input/output are supported	
Disk input to any output, but  pipe input to disk output.				

Please see the demo in \$CWPROOT/src/demos/Sorting\_Traces
for specific examples of trace sorting.

\subsection{SURAMP and SUTAPER - tapering data values}
Many seismic processing algorithms will show spurious artifacts
resulting from sharp edges on a dataset.  Tapering the amplitudes 
on the edges of a dataset is the one of the easiest ways of 
suppressing these artifacts.
For this purpose, we have  ``sutaper'' to taper the edges of
the dataset (for example here, linear taper over 5 traces on each
end of the dataset)
{\small\begin{verbatim}
% sutaper <diskfile >stdout ntaper=5   
\end{verbatim}} \noindent
and ``suramp'' to smooth the beginning and/or end of traces
(for the example here, ramp up from 0 to tmin=.05 seconds,
and ramp down from tmax=1.15 seconds to the end of the traces
{\small\begin{verbatim}
% suramp <diskfile tmin=.05 tmax=1.15 >stdout 
\end{verbatim}} \noindent

\subsection{SUKILL, SUZERO, SUNULL, SUMUTE - zeroing out data}

It is often useful to zero out noisy traces, or traces
on the edge of a dataset (abrupt analog to tapering), or to create
null traces as separators to be used between successive panels of data
in plotting.

\subsubsection{SUKILL - zero out traces}

To zero out a block of traces type,
{\small \begin{verbatim}
% sukill <stdin >stdout min=MIN_TRACE count=COUNT				
\end{verbatim}}\noindent
where COUNT is the number of traces to be zeroed, and MIN\_TRACE
is the minimum trace number in the block of traces.

\subsubsection{SUNULL - Create a Panel of Empty traces}
It is sometimes necessary to create a panel of zero value traces.
To create a panel of traces of NTR traces with NT time samples 
{\small \begin{verbatim}
% sunull nt=NT ntr=NTR <stdin >stdout min=MIN_TRACE count=COUNT				
\end{verbatim}}\noindent

\subsubsection{SUZERO -- zero-out data within a time window}
To zero out data within a time window, use  ``suzero''
{\small \begin{verbatim}
% suzero itmin=MIN_TIME_SAMPLE itmax=MAX_TIME_SAMPLE <indata.su > outdata.su				
\end{verbatim}}\noindent

\subsubsection{SUMUTE - Surgically Muting Data}
The last collection of programs perform a form of crude muting of
the data.
For a more precise muting operation, there is ``sumute''
which can perform surgical muting of SU data.
The program may be used by specifying trace header field
to mute on via the ``key='' parameter,
{\small\begin{verbatim}
% sumute <indata.su >outdata.su key=KEYWORD xmute=x1,x2,x3,... tmute=t1,t2,t3,... 		
\end{verbatim}} \noindent
An suplane data example of this is to compare
original suplane data, made by
{\small\begin{verbatim}
% suplane | suxwigb &
\end{verbatim}} \noindent
with surgically muted data
{\small\begin{verbatim}
% suplane | sumute key=tracl xmute=1,10,12 tmute=.06,.1,.11 | suxwigb &
\end{verbatim} } \noindent
This says to mute every arrival above the polygonal curve defined
by the curve defined by the xmute= and  tmute= values.

The program also permits the x,t values to be input from binary
files by setting the options ``nmute, xfile, and tfile''
as listed in a portion of the selfdoc for this program
{\small\begin{verbatim}
...
 nmute=		number of x,t values defining mute		
 xfile=		file containing position values as specified by	
 			the `key' parameter				
 tfile=		file containing corresponding time values (sec)	
...
 				=tracl  use trace number instead	
 ntaper=0		number of points to taper before hard		
			mute (sine squared taper)			
 below=0		=1 to zero BELOW the polygonal curve		
...
\end{verbatim} } \noindent
Please note also that ``above'' and ``below'' refer to the appearance
on a seismic plot, such as the suxwigb plot and not to the time
values.

\subsection{SUVCAT and CAT - Concatenating Data}
There are two possible ways of appending one dataset onto another
(concatenating). The first way would be to append one dataset
to another, so that the traces from the second file simply follow
the traces of the first file. This is done with the Unix ``cat''
command via:
{\small\begin{verbatim}
% cat data1.su data2.su > data3.su
\end{verbatim}} \noindent
In addition, it may be necessary to renumber the traces via:
{\small\begin{verbatim}
% cat data1.su data2.su | sushw key=tracl a=1 > data3.su
\end{verbatim}} \noindent
so that the tracl parameter is monotonically increasing.

The second way of appending one dataset to another is to ``vertically''
append each trace of the second dataset to the end of the first dataset.
This is done via ``suvcat.''
{\small\begin{verbatim}
% suvcat data1.su data2.su > data3.su
\end{verbatim}} \noindent
In this case, no modification of the header fields should be necessary.

\subsection{SUVLENGTH - Adjust Variable Length Traces to a Common Number of Samples}

Sometimes the data consists of traces which have different numbers
of samples on each trace. The program ``suvlength''  We can construct an example 
of this with suplane data 
{\small \begin{verbatim}
% suplane nt=64 > data1.su
% suplane nt=32 > data2.su
% cat data1.su data2.su > data3.su
\end{verbatim}} \noindent
Attempts to use any SU program on the resulting file ``data3.su''
will probably result in failure, because most SU programs require
that the number of samples be constant on a panel of data.
Applying ``suvlength'' fixes this problem
{\small \begin{verbatim}
% suvlength ns=64 < data3.su > data4.su
% suxwigb < data4.su title="Test of suvlength"  &
\end{verbatim}} \noindent
by making all of the traces the same length.

\chapter{General Operations on SU Data}
Beyond the task of editing SU data are operations level codes
which perform 
\begin{itemize}
\item gaining,
\item resampling,
\item unary operations (arithmetic operations involving a single data file),
\item binary operations (arithmetic operations involving two data files),
\end{itemize}

The purpose of this section, therefore will be to discuss the programs
\begin{itemize}
\item SUADDNOISE - add noise to traces,
\item SUGAIN - apply various types of gain to display traces,
\item SUOP - do unary arithmetic operation on segys,
\item SUOP2 - do a binary operation on two data sets,
\end{itemize}
that perform these operations

\subsection{SUADDNOISE - Add noise to SU data}

While it may seem that having a program that {\em adds\/} noise
to seismic data is counterproductive, it is often useful
for testing purposes to have the capability of simulating
noise.

It is also useful for demonstration purposes, and many of the
demos below use suaddnoise to ``fill in the blanks'' in the
suplane testpattern. A couple of examples of the output of this
program, using suplane data

{\small\begin{verbatim}
% suplane | suxwigb title="no noise" &
% suplane | suaddnoise | suxwigb title="noise added" &
% suplane | suaddnoise sn=2 | suxwigb title="noise added" &
\end{verbatim}}\noindent

\subsection{SUGAIN - Gaining to SU data}

There are numerous operations which come under the heading of
gaining, which ``sugain'' performs.
These operations include
\begin{itemize}
\item scaling the data,
\item multiplying the data by a power of time,
\item taking the power of the data,
\item automatic gain control,
\item trapping noise spiked traces,
\item clipping specified amplitudes or  quantiles,
\item balancing traces by quantile clip, rms value, or mean,
\item scaling the data,
\item biasing or debiasing the data.
\end{itemize}
The heirarchy of the operations is stated by the following equation
{\small\begin{verbatim}
out(t) = scale * BAL{CLIP[AGC{[t^tpow * exp(epow * t) * ( in(t)-bias )]^gpow}]}
\end{verbatim}}\noindent

You may see what sugain does by running the following examples
using suplane data. Noise has been added with ``suaddnoise'' to
make the affects of AGC apparent. 
Type only the items following the percent \%.
Create some SU data (with noise added, via)
{\small\begin{verbatim}
% suplane | suaddnoise > data.su
\end{verbatim}}\noindent

{\small\begin{verbatim}
% suxwigb < data.su title="Ungained Data"  &
% sugain < data.su scale=5.0 | suxwigb title="Scaled data"  &
% sugain < data.su agc=1 wagc=.01 | suxwigb title="AGC=1 WAGC=.01 sec &
% sugain < data.su agc=1 wagc=.2 | suxwigb title="AGC=1 WAGC=.1 sec &
% sugain < data.su pbal=1 | suxwigb title="traces balanced by rms" &
% sugain < data.su qbal=1 | suxwigb title="traces balanced by quantile" &
% sugain < data.su mbal=1 | suxwigb title="traces balanced by mean" &
% sugain < data.su tpow=2 | suxwigb title="t squared factor applied" &
% sugain < data.su tpow=.5 | suxwigb title="square root t factor applied" &
\end{verbatim}}\noindent
Please note, on your terminal window, there will be a message
with ``clip=" some number, for example:
{\small\begin{verbatim}
xwigb: clip=1
\end{verbatim}}\noindent
This indicates the amplitude value above which traces are clipped.
You may think of this as the value of the maximum on the trace.

\subsection{SUOP - Unary Arithmetic Operations on SU Data}

Occassionally we want to apply mathematical functions or
other operations which go beyond gaining to data. Such
operations might include
\begin{itemize}
\item absolute value,
\item signed square root,
\item square,
\item signed square,			
\item signum function,			
\item exponential,
\item natural logarithm,
\item signed common logarithm,		
\item cosine,				
\item sine,				
\item tangent,
\item hyperbolic cosine,		
\item hyperbolic sine,
\item hyperbolic tangent,
\item divide trace by Max. Value,	
\item express trace values in decibels:  20 * slog10 (data)		
\item negate values,
\item pass only positive values,	
\item pass only negative values.
\end{itemize}
Operations involving logarithms are ``punctuated'' meaning
that if the  contains 0 values,	0 values are returned.		

Examples of suop using SU plane data may be easily run
{\small\begin{verbatim}
% suplane | suaddnoise > data.su
% suop < data.su op=abs | suxwigb title="absolute value" &
% suop < data.su op=ssqrt | suxwigb title="signed square root" &
% suop < data.su op=sqr | suxwigb title="signed square" &
...
\end{verbatim}}\noindent

Please type:
{\small\begin{verbatim}
% suop
\end{verbatim}}\noindent
to see the selfdoc and the other options.

\subsection{SUOP2 - Binary Operations with SU data}

To perform operations two SU datasets, the progam ``suop2''
has been provided.
Some of the opperations supported are to compute the
\begin{itemize}
\item difference,
\item sum,
\item product,
\item quotient,
\item difference of a panel and a single trace,
\item sum of a panel and a single trace,
\item product of a panel and a single trace,
\item quotient of a panel and a single trace.
\end{itemize}
The first 4 options assume that there are the same number
of traces in each SU datafile. In the last 4,
it is assumed that there is only a single trace in the
second file.

From the selfdoc of ``suop2'' please note that there are
8 equivalent shell scripts commands which perform these
operations 
{\small\begin{verbatim}
...
 	susum file1 file2 == suop2 file1 file2 op=sum			
 	sudiff file1 file2 == suop2 file1 file2 op=diff			
 	suprod file1 file2 == suop2 file1 file2 op=prod			
 	suquo  file1 file2 == suop2 file1 file2 op=quo			

 For:  panel "op" trace  operations: 				
 	suptsum  file1 file2 == suop2 file1 file2 op=ptsum		
 	suptdiff file1 file2 == suop2 file1 file2 op=ptdiff		
 	suptprod file1 file2 == suop2 file1 file2 op=ptprod		
 	suptquo  file1 file2 == suop2 file1 file2 op=ptquo		
...
\end{verbatim}}\noindent
All of these call ``suop2'' to perform the computation.

Try the following. Make two files of SU data with ``suplane''
{\small\begin{verbatim}
% suplane > junk1.su
% suxwigb < junk1.su | suxwigb title="Data without noise" &
% suplane | suaddnoise > junk2.su
% suxwigb < junk2.su  | suxwigb title="Data with noise added" &
% suop2 junk2.su junk1.su op=diff | suxwigb title="difference" &
\end{verbatim}}\noindent
Note, that the filenames must appear before the ``op=.'' 

\section{Transform and Filtering Operations}

A major aspect of seismic research and seismic processing is
related to operations which are based on mathematical {\em transform\/} 
methods.
In particular, much of seismic processing would not exist without
numerical Fourier transforms.
Filtering is a related subject, because the majority of filters
are applied in the frequency domain, or are at least representable
mathematically as frequency domain operations.

In addition to standard Fourier transforms, there are a couple
of other transforms, such as the Hilbert transform, and the Gabor
transform which are also included in the SU package.
These items may find more use as educational tools, rather than
processing tools.

\subsection{Fourier Transform Operations}

There are Fourier transform operations for both 1D and 2D applications
in the SU package.
The 1D transforms provide spectral (either amplitude or phase)
information about each trace in a panel of seismic data.

The 2D transforms include the seismic F-K variety,
assuming that the fast dimension of the input data is temporal, and
the second dimension is spatial, and the non-seismic  K1-K2
variety, in which the input is assumed to be purely spatial (x1,x2)
data.

\subsection{1D Fourier Transforms}
\begin{itemize}
\item SUFFT - fft real time traces to complex frequency traces
\item SUIFFT - fft complex frequency traces to real time traces
\item SUAMP - output amp, phase, real or imag trace from (frequency, x)
\item SUSPECFX - Fourier SPECtrum (T to F) of traces 
domain data
\end{itemize}

The program ``sufft'' produces the output of the Fourier transform
operation as a complex data type.
The program ``suifft'' is designed to accept complex input as
would be generated by sufft to perform the inverse Fourier transform.
The cascade of these operations is not quite an non-operation,
because there is zeropadding which is automatically implemented for
the transforms. Also, the header fields will not quite be
right for seismic data. For example, try:
{\small \begin{verbatim}
% suplane | suxwigb title="Original Data"  &
% suplane | sufft | suifft | sushw key=d1,dt a=0,4000 | suxwigb  &
\end{verbatim}}\noindent
The result is the same as the input, except there are more samples
on the traces due to zero-padding required for the transform.

To view the amplitude and phase spectra, and the real
and imaginary parts of the of the output of sufft, do the following
{\small \begin{verbatim}
% suplane | sufft | suamp mode=amp | suxwigb title="amplitude" &
% suplane | sufft | suamp mode=phase | suxwigb title="phases" &
% suplane | sufft | suamp mode=real | suxwigb title="real" &
% suplane | sufft | suamp mode=imag | suxwigb title="imaginary" &
\end{verbatim}}\noindent
SU data has a format which allows for the storage of the real
and imaginary parts of data in a complex datatype.
To see the header field settings for that format, type:
{\small \begin{verbatim}
% suplane | sufft | surange
sufft: d1=3.571428
32 traces:
 tracl=(1,32)  tracr=(1,32)  trid=11 offset=400 ns=72
 dt=4000 d1=3.571428 
\end{verbatim}}\noindent
You will notice that the setting for the trace id (trid)
is 11. Typing:
{\small \begin{verbatim}
% sukeyword trid
...
                        11 = Fourier transformed - unpacked Nyquist
                             xr[0],xi[0],...,xr[N/2],xi[N/2]   
...
\end{verbatim}}\noindent
shows that trid=11 how the data are arranged in the output of
the fft.

Of course, most of the time, we only want to have a quick look
at the amplitude spectrum of a seismic trace, or a panel
of seismic traces.
For these purposes use ``suspecfx''
{\small \begin{verbatim}
% suplane | suspecfx | suximage title="F-X Amplitude Spectrum"  &
\end{verbatim}}\noindent
which directly displays the amplitude spectra of each trace
of the input SU data.


\subsection{2D Fourier Transforms}

Seismic data are generally at least 2D datasets. If the data
really are data in (time, space) coordinates, then the output
should be in (frequency, wavenumber), the F-K domain.
However,
there are applications where we consider the data to be
in two spatial dimensions (x1,x2) meaning that the output
is in (k1,k2)

We have the programs
\begin{itemize}
\item SUSPECFK - F-K Fourier SPECtrum of data set 
\item SUSPECK1K2 - 2D (K1,K2) Fourier SPECtrum of (x1,x2) data set
\end{itemize}
For each of these cases. Examples using suplane data are easily
run using
{\small \begin{verbatim}
% suplane | suspecfk | suximage title="F-K Amplitude Spectrum"  &
% suplane | suspeck1k2 | suximage title="K1-K2 Amplitude Spectrum"  &
\end{verbatim}}\noindent
Please note, as these are {\em display\/} programs, so the intent is that 
plots of the output should appear correct when compared with a
plot of the original data. The effect of the 2D Fourier transform 
on a line of spikes is to produce a line of spikes normal to the
original line, if the (k1,k2) data are plotted on the same plot as
the (x1,x2) data, 

\section{Hilbert Transform, Trace Attributes, and Time-Frequency Domain}

A number of classical techniques for representing instantaneous trace
attributes have been created over the years. Many of these techniques
involve the construction of a quadrature trace to be used as the
imaginary part of a ``complex trace'' (with the real part, being 
the real data). The quadrature trace is created with the Hilbert
transform following the construction of the so-called ``allied function.''
This representation permits ``instantaneous amplitude, phase,
and frequency'' information to be generated for a dataset, by
taking the modulus, phase, and time derivative of the phase, respectively.
An alternate approach is to perform multi-filter analysis on data,
to represent it as a function of both time and frequency.

Tools in SU which perform these operations are
\begin{itemize}
\item SUHILB - Hilbert transform 
\item SUATTRIBUTES - trace ATTRIBUTES instantanteous amplitude, phase, or frequency 
\item SUGABOR -  Outputs a time-frequency representation of seismic data via
the Gabor transform-like multifilter analysis,
\end{itemize}
To generate the Hilbert transform of a test dataset, try
{\small \begin{verbatim}
% suplane | suhilb | suxwigb title="Hilbert Transform"  &
\end{verbatim}}\noindent
This program is useful for instructional and testing purposes.

To see an example of ``trace attributes'' with ``suattributes''
it is necessary to make data which has a strong time-frequency
variability. This is done here with ``suvibro'' which makes a
synthetic vibroseis sweep. Compare the following:
{\small \begin{verbatim}
% suvibro | suxgraph title="Vibroseis sweep" &
% suvibro | suattributes mode=amp | suxgraph title="Inst. amplitude"  &
% suvibro | suattributes mode=phase unwrap=1.0 | suxgraph title="Inst. phase"  &
% suvibro | suattributes mode=freq | suxgraph title="Inst. frequency"  &
\end{verbatim}}\noindent
which show, respectively, instantaneous amplitude, phase, and frequency.

To see the synthetic vibroseis trace in the time-frequency domain,
try the following:
{\small \begin{verbatim}
% suvibro | sugabor | suximage title="time frequency plot"  &
\end{verbatim}}\noindent
The result is an image which shows the instantaneous or apparent
frequency increasing from 10hz to 60hz, with time, exactly as
stated by the default parameters of ``suvibro.''

See the demos in \$CWPROOT/src/demos/Time\_Freq\_Analysis
and \$CWPROOT/src/demos/Filtering/Sugabor
for further information.

\section{Radon Transform - Tau\_P Filtering}

The Radon or ``tau-p'' transform, as it is sometimes called in
geophysical literature is useful for a variety of multiple suppression,
and other ``surgical'' data manipulation tasks.
The programs
\begin{itemize}
\item SUTAUP - forwared and inverse T-X and F-K global slant stacks
\item SUHARLAN - signal-noise separation by the invertible linear
transformation method of Harlan, 1984
\item SURADON - compute forward or reverse Radon transform or remove multiples
by using the parabolic Radon transform to estimate multiples and subtract.
\item SUINTERP - interpolate traces using automatic event picking 
\end{itemize}
are provided to make use of this transform.
You may tests using suplane data as we have with other programs
to see the output from each of these codes. Both ``suinterp'' and ``suradon''
have some sophisticated options which my require some experimentation.

See also the demos in \$CWPROOT/src/demos/Tau\_P  for examples.

\section{1D Filtering Operations}

A large part of what is called seismic processing, may be
thought of as being ``filtering.''
There are filtering operations for 1D applications in
the SU package that span simple filtering tasks, to more
sophisticated tasks including deconvolution and wavelet shaping operations.
These operations are 1D, in that they are applied trace by trace.

Several types of filtering operations that arise in seismic
processing are
\begin{itemize}
\item zero bandpass, bandreject, lowpass, and highpass , and notch filtering,
\item minimum or zero phase Butterworth filtering,
\item Wiener prediction error (deconvolution),
\item Wiener wavelets shaping,
\item convolution,
\item crosscorrelation,
\item autocorrelation,
\item data resampling with sinc interpolation,
\item fractional derivatives/integrals,
\item median filtering,
\item time varying filtering.
\end{itemize}

The programs that are provided to meet these needs are:
\begin{itemize}
\item SUFILTER - applies a zero-phase, sine-squared tapered filter 
\item SUBFILT - apply Butterworth bandpass filter
\item SUACOR - auto-correlation
\item SUCONV, SUXCOR - convolution, correlation with a user-supplied filter
\item SUPEF - Wiener predictive error filtering
\item SUSHAPE - Wiener shaping filter 
\item SURESAMP - Resample in time,
\item SUFRAC -- take general (fractional) time derivative or integral of
data, plus a phase shift.  Input is TIME DOMAIN data.
\item SUMEDIAN - MEDIAN filter about a user-defined polygonal curve with     
the distance along the curve specified by key header word 
\item SUTVBAND - time-variant bandpass filter (sine-squared taper)
\end{itemize}

\subsection{SUFILTER - applies a zero-phase, sine-squared tapered filter}
The program {\bf sufilter\/} provides a general purpose zero-phase filtering
capability for the usual tasks of bandpass, bandreject, lowpass, highpass,
and notch filtering.  Examples of each of these using {\bf sufilter\/}
data are provided
{\small \begin{verbatim}
% suplane | sufilter f=10,20,30,60 amps=0,1,1,0 | suxwigb title="10,20,30,60 hz bandpass"  &
\end{verbatim}}\noindent
{\small \begin{verbatim}
% suplane | sufilter f=10,20,30,60 amps=1,0,0,1 | suxwigb title="10,20,30,60 hz bandreject"  &
\end{verbatim}}\noindent
{\small \begin{verbatim}
% suplane | sufilter f=10,20,30,60 amps=1,1,0,0 | suxwigb title="10,20 hz lowpass"  &
\end{verbatim}}\noindent
{\small \begin{verbatim}
% suplane | sufilter f=50,60,70 amps=1,0,1 | suxwigb title="60 hz notch"  &
\end{verbatim}}\noindent

The filter is polygonal, with the corners of the polygon defined by
the vector of frequency values defined by array of {\bf f=\/} values,
and a collection of amplitude values, defined by the array of {\bf amps=\/}
values. The amplitudes may be any floating point numbers greater than,
or equal to zero. The only rule is that there must be the same number
of {\bf amps\/} values, as {\bf f\/} values.  The segments are sine-squared
tapered between {\bf f\/} values of different amplitude. It is
best to select {bf f\/} values such that tapering to zero is done
over an octave to prevent ringing.

With {\bf suplane\/}, {\bf sufilter\/} provides a way of easily
generating bandlimited testpattern data in SU format.

\subsection{SUBFILT - apply Butterworth bandpass filter}

An alternative to {\bf sufilter\/} is {\bf subfilt\/}, which
applies a Butterworth filter to data. 

{\small \begin{verbatim}
% suplane | subfilt fstoplo=10 fpasslo=20 fpasshi=30 fstophi=60 | suxwigb title="10,20,30,60 hz  bandpass bfilt"  &
\end{verbatim}}\noindent
{\small \begin{verbatim}

\subsection{SUACOR - auto-correlation}
This program is used to compute the autocorrelation of a trace.
This process is useful to see the size of a wavelet, or the repetitions
of the wavelet, as an aid in choosing the {\bf maxlag\/} parameter
of {\bf supef}. {\bf Suacor\/} is also useful for determining the
frequency range of data in terms of the power spectrum. For example
try
{\small \begin{verbatim}
% suplane | sufilter | suacor | suspecfx | suxwigb &
\end{verbatim}}\noindent

\subsection{SUCONV, SUXCOR - convolution, correlation with a user-supplied filter}

The standard operations of convolution and cross correlation may be
performed with {\bf suconv\/} and {\bf suxcor\/}, respectively.
The filter may be supplied as a vector input on the commandline,
or as a file containing a single trace in SU format. In addition
to taking the input as a single trace, it is possible to supply
a panel of filters, each to be used trace by trace on a panel of
SU data.

An example of correlating a vibroseis sweep may be seen by creating
vibroseis-like data, with {\bf suvibro\/}, {\bf suplane\/} and {\bf suconv\/}.
To make ``vibroseis'' {\bf suplane\/} data

{\small \begin{verbatim}
% suvibro > junk.vib.su
% suplane | suconv sufile=junk.vib.su > plane.vib.su
\end{verbatim}}\noindent

Because {\bf surange\/} tells us that
{\small \begin{verbatim}
% surange < junk.vib.su
1 traces:
 tracl=1 ns=2500 dt=4000 sfs=10 sfe=60
 slen=10000 styp=1
\end{verbatim}}\noindent

there are 2500 samples on the vibroseis sweep, we can do the following
correlation
{\small \begin{verbatim}
%  suxcor < plane.vib.su sufile=junk.vib.su |
     suwind itmin=2500 itmax=2563 | sushw key=delrt a=0.0 > data.su
(this line is broken to make it fit on the page here, the real
command is typed on a single line)
\end{verbatim}}\noindent
The value of {\bf itmin=sweeplength\/} and {\bf itmax=sweeplength+nsout\/}
where {\bf nsout\/} is the number of samples expected in the output.
The final step using {\bf sushw\/} is to set the trace delay to 0.
Choosing {\bf itmin=sweeplength\/} will ensure that the data start
at the correct value. Choosing {\bf nsout=sweeplength-nsin\/}, where
{\bf nsin\/} is the number of samples in the input, will yield the
correct number of samples to keep.

\subsection{SUPEF - Wiener predictive error filtering}
The prediction error filtering method, also known as Wiener filtering,
is the principle process of traditional Wiener-Levinson deconvolution.
The reason that this subsection is not headed ``deconvolution'' is
because there are two additional issues that have to be addressed
before prediction error filtering can be used for effective deconvolution.
These issues are preprocessing of the data, and postprocess filtering.
Indeed, much feedback has come back claiming that {\bf supef\/}
doesn't work properly, when in fact, it simply being used improperly.
(This is our fault, for not supplying sufficient documentation.)

Using {\bf supef\/}, itself, generally requires that the value
of {\bf maxlag\/} be set. This may be determined by first establishing the
size of the wavelet being spiked, through application {\bf suacor\/}.

As the preprocessing step, you will probably need to use {\bf sugain\/}
to remove any decay in amplitudes with time that may result from geometric
spreading. 

As a postprocessing step, you need to remove any increase in frequency
content which has occurred due to the whitening effect of the 
prediction error filter. The demos in the demos/Deconvolution  directory
demonstrate the functioning of the program. However, these examples
are a bit unrealistic for field data. For example, in the demos, the
data are spiked, and then reverberations are removed via a cascaded
of {\bf supef\/} calls with {\bf maxlag=.04\/} and 
{\bf minlag=.05 maxlag=.16\/}, respectively. However on field data,
we would probably only use a single pass of the filter to remove
reverberations, or to spike arrivals.

The prediction error filter has a spectral whitening effect, which
is likely to put high frequencies in the data that were not there
originally. These must be filtered out. It is a good idea to assume
that there is a {\em loss\/} of frequency information, and design
the parameters for the postprocessing filtering, using {\bf sufilter\/}
to slightly reduce the frequency content from its original values.

Also, an added feature of the Release~32 version of {\bf supef\/}
is the mixing parameter, which permits the user to apply a weighted
moving average to the autocorrelations that are computed as part of
the prediction error filter computation. This can provide additional
stability to the operation.

\subsection{SUSHAPE - Wiener shaping filter}

The demos in demos/Deconvolution also contain a demonstration of
the Wiener shaping filter {\bf sushape\/}.


\subsubsection{2D Filtering Operations}
Filtering in the (k1,k2) domain, and the (F,K) domain is
often useful for changing dip information in data. 
The programs
\begin{itemize}
\item SUKFILTER - radially symmetric K-domain, sin\^2-tapered, polygonal filter 
\item SUK1K2FILTER - symmetric box-like K-domain filter defined by the
cartesian product of two sin\^2-tapered polygonal filters defined in k1 and k2
\item SUKFRAC - apply FRACtional powers of i|k| to data, with phase shift 
\item SUDIPFILT - DIP--or better--SLOPE Filter in f-k domain
\end{itemize}
provide the beginnings of a set of K-domain and F-K-domain filtering
operations.

\subsection{SURESAMP - Resample Data in Time}
Often data need to be resampled to either reduce or increase
the number of samples for processing or data storage.
For seismic data, the smart way of doing this is by the 
method of sinc interpolation.
The program ``suresamp''
\begin{itemize}
\item SURESAMP - Resample in time 
\end{itemize}
performs this operation.

See the demos in \$CWPROOT/src/demos/Filtering   for further information
about filtering in SU.

\chapter{Seismic Modeling Utilities}

An important aspect of seismic exploration and research are programs
for creating synthetic data. Such programs find their use,
both in the practical problem of modeling real data, as well
as in the testing of new processing programs. A processing program
that will not work on idealized model data will likely not work
on real seismic data.

Another important aspect of modeling programs is the fact that many
seismic processing algorithms (such as migration) may be viewed
as {\em inverse processes\/}.  The first step in such an inverse 
problem may be to create a method to solve the forward problem,
and then formulate the solution to the inverse
problem as a ``backpropagation'' of the recorded data to its position
in the subsurface.

There are two parts to the seismic modeling task. The first part is the
construction of background wavespeed profiles, which may consist
of uniformly sampled arrays of floating point numbers.
The second part is the construction of the synthetic wave
information which propagates in that wavespeed profile.

Because of the intimate relationship between seismic modeling and
seismic processing, background wavespeed profiles created for
modelling tasks, may also be useful for processing tasks.

Of course, if some simple assumptions are made, it may be possible
for background wavespeed information to be built into the modeling
program.

\section{Background Wavespeed Profiles}

There are many approaches to creating background wavespeed profiles.
For many processes, it may be that a simple array of floating point
numbers, each representing the wavespeed, slowness (1/wavespeed), or
sloth (slowness squared) on a uniformly sampled grid will be sufficient.

However, more advanced techniques may involve wavespeed profile generation
in triangulated or tetrahedrized media.

\section{Uniformly Sampled Models}
In Seismic Unix there are several programs which may be used to generate
background wavespeed profile data. Often, such data need to be
smoothed.

These programs are:
\begin{itemize}
\item UNISAM - UNIformly SAMple a function y(x) specified as x,y pair
\item UNISAM2 - UNIformly SAMple a 2-D function f(x1,x2)
\item MAKEVEL - MAKE a VELocity function v(x,y,z)
\item UNIF2 - generate a 2-D UNIFormly sampled velocity profile from a 
layered model. In each layer, velocity is a linear function of position.
\item SMOOTHINT2 - SMOOTH non-uniformly sampled INTerfaces, via the damped
least-squares technique 
\item SMOOTH2 - SMOOTH a uniformly sampled 2d array of data, within a user-
defined window, via a damped least squares technique
\item SMOOTH3D - 3D grid velocity SMOOTHing by the damped least squares 
\end{itemize}

Please see the selfdoc of each of these programs for further information.
Also see the demos in \$CWPROOT/src/Velocity\_Profiles

\section{Synthetic Data Generators}

There are a number of programs for generating synthetic seismic 
and seismic-like data in the SU package.
These are
\begin{itemize}
\item SUPLANE - create common offset data file with up to 3 planes 
\item SUSPIKE - make a small spike data set
\item SUIMP2D - generate shot records for a line scatterer embedded 
in three dimensions using the Born integral equation 
\item SUIMP3D - generate inplane shot records for a point scatterer
embedded in three dimensions using the Born integral equation
\item SUFDMOD2 - Finite-Difference MODeling (2nd order) for acoustic 
wave equation
\item SUSYNCZ - SYNthetic seismograms for piecewise constant V(Z) function   
True amplitude (primaries only) modeling for 2.5D
\item SUSYNLV - SYNthetic seismograms for Linear Velocity function
\item SUSYNVXZ - SYNthetic seismograms of common offset V(X,Z) media via     
Kirchhoff-style modeling
\item SUSYNLVCW - SYNthetic seismograms for Linear Velocity function
for mode Converted Waves
\item SUSYNVXZCS - SYNthetic seismograms of common shot in V(X,Z) media via  
Kirchhoff-style modeling
\end{itemize}
Of these, only sufdmod2, susynvxz, and sysnvxzcs require an file
of input wavespeed. The other programs use commandline arguments
for wavespeed model input.

Please see the demos in \$CWPROOT/src/demos/Synthetic for further
information. Also, a number of the other demos use these programs
for synthetic data generation.

\section{Delaunay Triangulation}

More sophisticated methods of synthetic data generation use
assumptions regarding the nature of the medium (as represented
by the input wavespeed profile data format) to expedite computations
of the synthetic data. One such method is triangulation via the
Delaunay method. 

\subsection{Triangulated Model Building}
There are two way of making triangulated models. The first
is to explicitly input boundary coordinates and wavespeed (actually
sloth values) with ``trimodel.'' The second is to make a uniformly
sampled model, perhaps with one of the model building utilities
above, and then use ``uni2tri'' to convert the uniformly sampled
model to a triangulated model. (Please note, that this will work
better if the wavespeed model is smoothed, prior to conversion.)
These programs are
\begin{itemize} 
\item TRIMODEL - make a triangulated sloth (1/velocity squared) model
\item UNI2TRI - convert UNIformly sampled model to a TRIangulated model
\item TRI2UNI - convert a TRIangulated model to UNIformly sampled model
\end{itemize}

\subsection{Synthetic Seismic Data in Triangulated Media}

Several programs make use of triangulated models to create ray
tracing, or ray-trace based synthetic seismograms.
There is also a code to create Gaussian beam synthetic seismograms
in a triangulated medium. 
These programs are:
\begin{itemize} 
\item NORMRAY - dynamic ray tracing for normal incidence rays in a sloth model
\item TRIRAY - dynamic RAY tracing for a TRIangulated sloth model 
\item GBBEAM - Gaussian beam synthetic seismograms for a sloth model
\item TRISEIS - Synthetic seismograms for a sloth model 
\end{itemize}

There is a comprehensive set of demos located in the directory
\$CWPROOT/src/Synthetic/Tri and 
\$CWPROOT/src/Synthetic/Trielas

\section{Tetrahedral Methods}
Some new functionality will be entering SU in future releases
for tetrahedral model building and ray tracing in tetrahedral models.
One code that is in the current release is

\begin{itemize}
\item TETRAMOD - TETRAhedron MODel builder. In each layer, velocity gradient
is constant or a 2-D grid; horizons could be a uniform grid and/or added by
a 2-D grid specified.
\end{itemize}


\chapter{Seismic Processing Utilities}
There are a collection of operations which are uniquely seismic
in nature, representing operations which are designed to
perform some aspect of the involved process which takes seismic
data and converts it into images of the earth.

\begin{itemize}
\item stacking data,
\item picking data,
\item velocity analysis,
\item normal moveout correction,
\item dip moveout correction,
\item seismic migration and related operations.
\end{itemize}

\section{SUSTACK, SURECIP, SUDIVSTACK - Stacking Data}
\begin{itemize}
\item SUSTACK - stack adjacent traces having the same key header word, 
\item SURECIP - sum opposing offsets in prepared data,
\item SUDIVSTACK -  Diversity Stacking using either average power or peak   
power within windows 
\end{itemize}

\section{SUVELAN, SUNMO - Velocity Analysis and Normal Moveout Correction}
\begin{itemize}
\item SUVELAN - compute stacking velocity semblance for cdp gathers
\item SUNMO - NMO for an arbitrary velocity function of time and CDP 
\end{itemize}

Please see the demos in  \$CWPROOT/src/demos/Velocity\_Analysis
and \$CWPROOT/src/demos/NMO for further information. 

\section{SUDMOFK, SUDMOTX, SUDMOVZ - Dip Moveout Correction}

Dip-moveout is a data transformation which converts data recorded
with offset to zero offset data.
The following programs
\begin{itemize}
\item SUDMOFK - DMO via F-K domain (log-stretch) method for common-offset gathers
\item SUDMOTX - DMO via T-X domain (Kirchhoff) method for common-offset gathers
\item SUDMOVZ - DMO for V(Z) media for common-offset gathers 
\end{itemize}
perform this operation.

\section{Seismic Migration}
The subject of seismic migration is one of the most varied in
seismic data processing. Many algorthms have been developed
to perform this task. 
Methods include Kirchhoff, Stolt, Finite-Difference,
Fourier Finite-Difference, and several types of Phase-Shift
or Gazdag Migration.

Please see the demos in \$CWPROOT/src/demos/Migration
for further information.

\subsection{SUGAZMIG, SUMIGPS, SUMIGPSPI, SUMIGSPLIT - Phase Shift Migration}
\begin{itemize}
\item SUGAZMIG - SU version of Jeno GAZDAG's phase-shift migration for
zero-offset data. 
\item SUMIGPS - MIGration by Phase Shift with turning rays 
\item SUMIGPSPI - Gazdag's phase-shift plus interpolation migration         
for zero-offset data, which can handle the lateral velocity variation.
\item SUMIGSPLIT - Split-step depth migration for zero-offset data.
\end{itemize}

\subsection{SUKDMIG2D, SUMIGTOPO2D, SUDATUMK2DR, SUDATUMK2DS - 2D Kirchhoff Migration,
and Datuming}
\begin{itemize}
\item SUKDMIG2D - Kirchhoff Depth Migration of 2D poststack/prestack data 
\item SUDATUMK2DR - Kirchhoff datuming of receivers for 2D prestack data
(shot gathers are the input)
\item SUDATUMK2DS - Kirchhoff datuming of sources for 2D prestack data
(input data are receiver gathers)
\item SUMIGTOPO2D - Kirchhoff Depth Migration of 2D postack/prestack data
from the (variable topography) recording surface
\end{itemize}

\subsection{SUMIGFD, SUMIGFFD - Finite-Difference Migration}
\begin{itemize}
\item SUMIGFD - 45 and 60 degree Finite difference migration for zero-offset data.
\item SUMIGFFD - Fourier finite difference migration for zero-offset data. This method is a hybrid migration which
combines the advantages of phase shift and finite difference migrations.
\end{itemize}

\subsection{SUMIGTK - Time-Wavenumber Domain Migration}

This algorthm was created by Dave Hale ``on the fly'' and, as
far as we know, exists nowhere in else geophysical literature.

\begin{itemize}
\item SUMIGTK - MIGration via T-K domain method for common-midpoint stacked data
\end{itemize}

\subsection{SUSTOLT - Stolt Migration}
This is the classic F-K migration method of Clayton Stolt.
\begin{itemize}
\item SUSTOLT - Stolt migration for stacked data or common-offset gathers 
\end{itemize}

\chapter{Processing Flows with SU}

\section{SU and UNIX}
You need not learn a special seismic language to use
{\small\sf SU}.  If you know how
to use UNIX shell-redirecting and pipes, you are ready to start
using {\small\sf SU}---the seismic commands and options can be used just as you
would use the built-in UNIX commands.  In particular, you
can write ordinary UNIX shell scripts to combine frequent
command combinations into meta-commands (i.e., processing flows).
These scripts can be thought of as ``job files.''

\begin{table}[htbp]
\label{SU:tab:unix}
\caption{UNIX Symbols}
\begin{tabular}{||l||l||}  \hline\hline
process1 $<$ file1 & process1 takes input from file1 \\
process2 $>$ file2 & process2 writes on (new) file2 \\
process3 $>>$ file3 & process3 appends to file3  \\
process4 $|$ process5 & output of process4 is input to process5  \\
process6 $<<$ text & take input from following lines  \\ \hline \hline
\end{tabular}
\end{table}

So let's begin with a capsule review of the basic UNIX operators
as summarized in Table~\ref{SU:tab:unix}.
The symbols $<$, $>$, and $>>$ are known as ``redirection operators,''
since they redirect input and output into or out of the command
(i.e., process).
The symbol $|$ is called a ``pipe,'' since we can picture
data flowing from one process to another through the ``pipe.''
Here is a simple {\small\sf SU} ``pipeline'' with input ``indata'' and
output ``outdata'':

{\small\begin{verbatim}
sufilter f=4,8,42,54 <indata |
sugain tpow=2.0 >outdata
\end{verbatim}}\noindent
This example shows a band-limiting operation being ``piped'' into
a gaining operation.  The input data set \verb:indata: is directed into
the program {\bf sufilter\/} with the \verb:<: operator, and similarly, the output data set \verb:outdata: receives the data because of the \verb:>: operator.
The output of {\bf sufilter\/} is connected to the input of {\bf sugain\/} by use of the \verb:|: operator.

\label{SU:page:getpar}The strings with the \verb:=: signs illustrate
how parameters are passed to {\small\sf SU} programs.  The program {\bf sugain\/}
receives the assigned value 2.0 to its parameter \verb:tpow:, while
the program {\bf sufilter\/} receives the assigned four component {\em vector}
to its parameter \verb:f:.  To find out what the valid parameters are
for a given program, we use the self-doc facility.

By the way, space around the UNIX
redirection and pipe symbols is optional---the example shows
one popular style.  On the other hand, spaces around the \verb:=:
operator are {\em not} permitted.

The first four symbols in
Table~\ref{SU:tab:unix} are the basic grammar of UNIX;
the final $<<$ entry
is the symbol for the less commonly used ``here document'' redirection.
Despite its rarity in interactive use,
{\small\sf SU} shell programs are significantly enhanced by
appropriate use of the $<<$ operator---we will illustrate this below.

Many built-in UNIX commands do not have a self-documentation
facility like {\small\sf SU}'s---instead, most do have ``man'' pages.
For example,

{\small\begin{verbatim}
% man cat

CAT(1)              UNIX Programmer's Manual               CAT(1)



NAME
     cat - catenate and print

SYNOPSIS
     cat [ -u ] [ -n ] [ -s ] [ -v ] file ...

DESCRIPTION
     Cat reads each file in sequence and displays it on the stan-
     dard output.  Thus

                    cat file

     displays the file on the standard output, and

                    cat file1 file2 >file3
--More--
\end{verbatim}}\noindent
You need to know a bit more UNIX lore
to use {\small\sf SU} efficiently---we'll introduce these tricks of the trade in
the context of the examples discussed later in this chapter.



\section{Understanding and using SU shell programs}
The essence of good {\small\sf SU} usage is constructing (or cloning!)
UNIX shell programs to create and record processing flows.
In this section, we give some
annotated examples to get you started. 
\subsection{A simple SU processing flow example\label{SU:sec:Plotshell}}
Most {\small\sf SU} programs read from standard input and write to standard output.
Therefore, one can build complex processing flows by simply
connecting {\small\sf SU} programs with UNIX pipes.
Most flows will end with one of the {\small\sf SU} plotting programs.
Because typical processing flows are lengthy and involve many
parameter settings, it is convenient to put the {\small\sf SU} commands in a
shell file.

{\bf Remark}: All the UNIX shells, Bourne (sh), Cshell (csh),
Korn (ksh), \ldots, include a programming language.  In this document,
we exclusively use the Bourne shell programming language.

Our first example is a simple shell program called {\bf Plot}.
The numbers in square brackets at the
end of the lines in the following listing are not part of the
shell program---we added them as keys to the discussion
that follows the listing.

{\small\begin{verbatim}
#! /bin/sh                                              [1]
# Plot:   Plot a range of cmp gathers
# Author: Jane Doe
# Usage:  Plot cdpmin cdpmax

data=$HOME/data/cmgs                                    [2]

# Plot the cmp gather.
suwind <$data key=cdp min=$1 max=$2 |                   [3]
sugain tpow=2 gpow=.5 |
suximage f2=0 d2=1 \                                    [4]
        label1="Time (sec)" label2="Trace number" \
        title="CMP Gathers $1 to $2" \
        perc=99 grid1=solid &                           [5]
\end{verbatim}}\noindent
{\bf Discussion of numbered lines:}

\begin{enumerate}
\item The symbol \verb:#: is the comment symbol---anything on the remainder
of the line is not executed by the UNIX shell.  The combination
\verb:#!: is an exception to this rule: the shell uses the
file name following
this symbol as a path to the program that is to execute the remainder
of the shell program.

\item The author apparently intends that the shell be edited
if it is necessary to change the data set---she made this easier to
do by introducing the shell variable \verb:data: and assigning
to it the full pathname of the data file.  The assigned value
of this parameter is accessed as \verb:$data: within the shell program.
The parameter \verb:$HOME: appearing as the first component of the file
path name is a UNIX maintained environment variable
containing the path of the user's home directory.  In general,
there is no need for the data to be located in the user's home
directory, but the user would need ``read permission'' on the
data file for the shell program to succeed.

{\bf WARNING!\/}  Spaces are significant to the UNIX shell---it  uses
them to parse command lines.  So despite all we've learned about
making code easy to read, do {\em not} put spaces next to the \verb:=: symbol.
(Somewhere around 1977, one author's (Jack) first attempt to learn UNIX was
derailed for several weeks by making this mistake.)

\item The main pipeline of this shell code selects a certain set of cmp gathers with {\bf suwind}, gains this subset with {\bf sugain\/} and pipes the result into
the plotting program {\bf suximage}.  As indicated in the Usage comment,
the cmp range is specified by command line arguments.
Within the shell program, these arguments are
referenced as \verb:$1:, \verb:$2: (i.e., first argument, second argument).

\item The lines within the {\bf suximage\/} command are continued by the
backslash escape character.

\noindent{\bf WARNING!\/}  The line continuation backslash must be the {\em final}
character on the line---an invisible space or tab following the
backslash is one of the most common and frustrating bugs in UNIX
shell programming.

\item The final \verb:&: in the shell program
puts the plot window into ``background'' so we can continue
working in our main window.  This is the X-Windows
usage---the \verb:&: should {\em not} be used with the analogous PostScript
plotting programs (e.g., supsimage).  For example, with {\bf supsimage\/} in
place of {\bf suximage}, the \verb:&: might be replaced by \verb:| lpr:.

The {\small\sf SU} plotting programs are special---their self-doc doesn't
show all the parameters accepted.  For example, most of the parameters
accepted by {\bf suximage\/}
are actually specified in the self-documentation for the
generic {\small\sf CWP} plotting program {\bf ximage}.  This apparent flaw
in the self-documentation is actually a side
effect of a key {\small\sf SU} design decision.  The {\small\sf SU} graphics
programs call on the generic plotting programs to do the actual plotting.
The alternative design was to have tuned graphics programs
for various seismic applications.
Our design choice keeps things simple,
but it implies a basic limitation in {\small\sf SU}'s graphical capabilities.

The plotting programs are the vehicle for presenting your results.
Therefore you should take the time to carefully look
through the self-documentation for {\em both} the ``{\small\sf SU} jacket'' programs
({\bf suximage}, {\bf suxwigb}, \ldots) and the generic plotting
programs ({\bf ximage}, {\bf xwigb}, \ldots).

\end{enumerate}

\subsection{Executing shell programs}
The simplest way to execute a UNIX shell program is to give
it ``execute permission.''  For example, to make our above {\bf Plot} shell
program executable:
{\small\begin{verbatim}
chmod +x Plot
\end{verbatim}}\noindent
Then to execute the shell program:
{\small\begin{verbatim}
Plot 601 610
\end{verbatim}}\noindent
Here we assume that the parameters \verb:cdpmin=601:, \verb:cdpmax=610: are
appropriate values for the \verb:cmgs: data set.
Figure~\ref{fig:Plot} shows an output generated by the \verb:Plot: shell
program.
\begin{figure}[htbp]
\epsfysize 280pt
\centerline{\epsffile{Plot.eps}}
\caption{Output of the \protect\verb:Plot: shell program.}
\label{fig:Plot}
\end{figure}


\subsection{A typical SU processing flow\label{SU:sec:Dmoshell}}
Suppose you want to use {\bf sudmofk}.  You've read the self-doc, but
a detailed example is always welcome isn't it?  The place to look is
the directory {\bf su/examples}.  In this case, we are lucky and find
the shell program, {\bf Dmo}.  Again, the numbers in square brackets at the
end of the lines shown below are {\em not} part of the listing.
{\small\begin{verbatim}
#! /bin/sh
# dmo
set -x                                                            [1]

# set parameters
input=cdp201to800                                                 [2]
temp=dmocogs
output=dmocmgs
smute=1.7
vnmo=1500,1550,1700,2000,2300,2600,3000                           [3]
tnmo=0.00,0.40,1.00,2.00,3.00,4.00,6.00


# sort to common-offset, nmo, dmo, inverse-nmo, sort back to cmp
susort <$input offset cdp |                                       [4]
sunmo smute=$smute vnmo=$vnmo tnmo=$tnmo |                        [5]
sudmofk cdpmin=201 cdpmax=800 dxcdp=13.335 noffmix=4 verbose=1 |  [6]
sunmo invert=1 smute=$smute vnmo=$vnmo tnmo=$tnmo >$temp          [7]
susort <$temp cdp offset >$output                                 [8]
\end{verbatim}}\noindent
{\bf Discussion of numbered lines:}

The core of the shell program (lines 5-7) is recognized as the typical
dmo process: crude nmo, dmo, and then ``inverse'' nmo.
The dmo processing is surrounded by sorting operations
(lines 4 and 8).  Here is a detailed discussion of the shell program
keyed to the numbers appended to the listing (see also the discussion
above for the \verb:Plot: shell):

\begin{enumerate}
\item Set a debugging mode that asks UNIX
to echo the lines that are executed.  You can comment
this line off when its output is no longer of interest.  An
alternate debugging flag is \verb:set -v: which echos
lines as they are read by the shell interpreter.  You
can use both modes at once if you like.

\item This line and the next two lines set filenames that,
in this case, are in the same directory as the shell program itself.
Again, the reason for using parameters here is to make it easy
to ``clone'' the shell for use with other data sets.
Those of us who work with only a few data sets at any given time,
find it convenient to devote a directory to a given data set and
keep the shells used to process the data in that directory as
documentation of the processing parameters used.  ({\small\sf SU} does not have
a built-in ``history'' mechanism.)

\item The dmo process requires a set of velocity-time picks for
the subsidiary nmo processes.  Because these picks must be consistent
between the nmo and the inverse nmo, it is a good idea to make them
parameters to avoid editing mistakes.  Again, note the format
of {\small\sf SU} parameter vectors: comma-separated strings with no spaces.
The nmo program ({\bf sunmo}) will give an error message and abort
if the \verb:vnmo: and \verb:tnmo: vectors have different lengths.

\item Note that {\bf susort} allows the use of {\em secondary}
sort keys.  Do not assume that a secondary field that is
initially in the ``right'' order will remain in that order
after the sort---if you care about the order of some secondary
field, specify it (as this shell program does). In this line,
we sort the data according to increasing offsets and then, within
each offset, we sort according to increasing cdp number.

\item The forward nmo step.

\item The dmo step.

\item The inverse nmo step.

\item Sort back to cdp and have increasing offset within each cdp.
\end{enumerate}

If you want to thoroughly understand this shell program, your next
step is to study the self-docs of the programs involved:

{\small\begin{verbatim}
% sunmo

SUNMO - NMO for an arbitrary velocity function of time and CDP

sunmo <stdin >stdout [optional parameters]

Optional Parameters:
vnmo=2000         NMO velocities corresponding to times in tnmo
tnmo=0            NMO times corresponding to velocities in vnmo

...
\end{verbatim}}\noindent
Related shell programs are {\bf su/examples/Nmostack} and
{\bf su/examples/Mig}.

\section{Extending SU by shell programming}
Shell programming can be used to
greatly extend the reach of {\small\sf SU} without writing C code.
See, for example, {\bf CvStack}, {\bf FilterTest}, {\bf FirstBreak}, and
{\bf Velan} in {\bf su/examples}.

It is a sad fact that the UNIX shell is not
a high level programming language---consequently, effective shell
coding often involves arcane tricks.  In this section, we'll
provide some useful templates for some of the
common UNIX shell programming idioms.

We use {\bf CvStack} as an
illustration.  The core of this shell is a
double loop over velocities and cdps that produces
{\em velocity panels}---a concept
not contained in any single {\small\sf SU} program.

{\bf Remark}:  For most of us,
writing a shell like {\bf CvStack} from scratch is a time-consuming affair.
To cut down the development time,
your authors excerpt from existing shells to make new ones
even when we don't quite remember what every detail means.
We suggest that you do the same!

We won't comment on the lines already explained in our previous
two shell code examples
(see Sections~\ref{SU:sec:Plotshell} and~\ref{SU:sec:Dmoshell}),
but instead focus on the new features used in {\bf CvStack}.

{\small\begin{verbatim}
#! /bin/sh
# Constant-velocity stack of a range of cmp gathers
# Authors: Jack, Ken
# NOTE: Comment lines preceding user input start with  #!#
set -x

#!# Set input/output file names and data parameters
input=cdp601to610
stackdata=cvstack
cdpmin=601 cdpmax=610
fold=30
space=1         # 1 null trace between panels

#!# Determine velocity sampling.
vmin=1500   vmax=3000   dv=150

### Determine ns and dt from data (for sunull)
nt=`sugethw ns <$input | sed 1q | sed 's/.*ns=//'`                [1]
dt=`sugethw dt <$input | sed 1q | sed 's/.*dt=//'`

### Convert dt to seconds from header value in microseconds
dt=`bc -l <<END                                                   [2]
        scale=4
        $dt / 1000000
END`


### Do the velocity analyses.
>$stackdata  # zero output file                                   [3]
v=$vmin
while [ $v -le $vmax ]                                            [4]
do
        cdp=$cdpmin
        while [ $cdp -le $cdpmax ]                                [5]
        do
                suwind <$input \                                  [6]
                        key=cdp min=$cdp max=$cdp count=$fold |
                sunmo cdp=$cdp vnmo=$v tnmo=0.0 |
                sustack >>$stackdata
                cdp=`bc -l <<END                                  [7]                               
                        $cdp + 1
END`
        done
        sunull ntr=$space nt=$nt dt=$dt >>$stackdata              [8]
        v=`bc -l <<END
                $v + $dv
END`
done


### Plot the common velocity stacked data
ncdp=`bc -l <<END
        $cdpmax-$cdpmin+1
END`
f2=$vmin
d2=`bc -l <<END
        $dv/($ncdp + $space)                                      [9]
END`

sugain <$stackdata tpow=2.0 |

suximage perc=99 f2=$f2 d2=$d2 \
        title="File: $input  Constant-Velocity Stack " \
        label1="Time (s)"  label2="Velocity (m/s)" & 

exit                                                              [10]
\end{verbatim}}\noindent
{\bf Discussion of numbered lines:}

\begin{enumerate}
\item This elaborate construction gets some information
from the first trace header of the data set.  The program {\bf sugethw}
lists the values of the specified keys in the successive traces.  For
example,
{\small\begin{verbatim}
% suplane | sugethw tracl ns
 tracl=1            ns=64       

 tracl=2            ns=64       

 tracl=3            ns=64       

 tracl=4            ns=64       

 tracl=5            ns=64       

 tracl=6            ns=64    
   
 ...
\end{verbatim}}\noindent
Although {\bf sugethw} is eager to give the values for every trace in the
data set, we only need it once.  The solution is to use the UNIX stream
editor ({\bf sed}).  In fact, we use it twice.  By default, {\bf sed} passes
along its input to its output.  Our first use is merely to tell {\bf sed}
to quit after it puts the first line in the pipe.  The second pass through
{\bf sed} strips off the unwanted material before the integer.
In detail, the second {\bf sed} command reads: replace (or substitute)
everything up to the characters \verb:ns=: with nothing, i.e., delete
those characters.


\item We are proud of this trick.
The Bourne shell does not provide floating point
arithmetic.  Where this is needed, we use the UNIX built-in
{\bf bc} calculator program with the ``here document'' facility.
Here, we make the commonly needed conversion of sampling interval which
is given in micro-seconds in the {\sf SEG-Y} header,
but as seconds in {\small\sf SU} codes.  Note carefully the {\em back}quotes
around the entire calculation---we assign the result of this
calculation to the shell variable on the left of the equal sign,
here \verb:dt:.  The calculation may take several lines.
We first set the number of decimal places with \verb:scale=4:
and then do the conversion to seconds.  The characters \verb:END:
that follow the here document redirection symbol \verb:<<: are arbitrary,
the shell takes its input from the text in the shell file
until it comes to a line that contains the same
characters again.  For more information about {\bf bc}:
{\small\begin{verbatim}
% man bc
\end{verbatim}}\noindent

\item As the comment indicates, this is a special use of the output
redirection symbol that has the effect of destroying any pre-existing
file of the same name or opening a new file with that name.  In fact,
this is what \verb:>: always does as its first action---it's a dangerous
operator!  If you intend to {\em append}, then, as mentioned earlier, use
\verb:>>:.

\item This is the outer loop over velocities.
Another warning about spaces---the spaces around the bracket
symbols are essential.

{\bf Caveat}: The bracket notation is a nice
alternative to the older clunky \verb:test: notation:
{\small\begin{verbatim}
while test $v -le $vmax
\end{verbatim}}\noindent
Because the bracket notation is not documented on the typical {\bf sh} manual
page, we have some qualms about using it.  But, as far as we know,
all modern {\bf sh} commands support it---please let us know
if you find one that doesn't.

{\bf WARNING!}  OK, now you know that there is a UNIX command
called \verb:test:.  So don't use the name ``test'' for one of your
shell (or C) programs---depending on your \verb:$PATH: setting, you could
be faced with seemingly inexplicable output.

\item This is the inner loop over cdps.

\item Reminder: No spaces or tabs after the line continuation
symbol!

\item Notice that we broke the nice indentation structure by
putting the final \verb:END: against the left margin.  That's because
the {\bf sh} manual page says that the termination should contain
only the \verb:END: (or whatever you use).  In fact, most versions
support indentation.  We didn't think the added beautification was
worth the risk in a shell meant for export.  Also note that we used
{\bf bc} for an integer arithmetic calculation even though
integer arithmetic is built into the Bourne shell---why learn
two arcane rituals, when one will do?  See \verb:man expr:, if
you are curious.
\begin{figure}[htbp]
\epsfysize 300pt
\centerline{\epsffile{CvStack.eps}}
\caption{Output of the \protect\verb:CvStack: shell program.}
\label{fig:cvstack}
\end{figure}

\item {\bf sunull} is a program I (Jack) wrote to create all-zero traces
to enhance displays of the sort produced by \verb:CvStack:.
Actually, I had written this program many times, but this was the first
time I did it on purpose.  (Yes, that was an attempt at humor.)

\item An arcane calculation to get velocity labeling
on the trace axis.  Very impressive!  I wonder what it means?
(See last item.)

\item The \verb:exit: statement is useful because you might want
to save some ``spare parts'' for future use.  If so, just put them
after the \verb:exit: statement and they won't be executed.
\end{enumerate}

\noindent Figure~\ref{fig:cvstack} shows an output generated by \verb:CvStack:.

\chapter{Answers to Frequently Asked Questions}

This chapter addresses questions often asked by new {\small\sf SU} users.
Some answers refer to the directory {\tt CWPROOT}.  We use this
symbolic name for the directory that contains the {\sf CWP/SU} source code, include files, libraries, and executables.  You are asked to specify
this directory name during the {\small\sf SU} installation procedure.

\section{Installation questions}
Complete information about the installation
process is found in the {\sf README}
files supplied with the distribution.
Here we discuss only some commonly found installation problems.

\begin{question}
I get error messages about missing {\tt fgetpos} and {\tt fsetpos} routines, 
even though I am using the {\sf GCC} compiler.
How do I get around this problem?
\end{question}

\begin{rmans}
We've seen this problem most often with older 
{\sf SUN OS} 4.xx (pre-{\sf SOLARIS}). 
These {\sf SUN} systems may not have the {\tt fgetpos} and {\tt fsetpos} subroutines defined.
Because these two routines are not currently used in the {\small\sf SU} package,
we have modified the installation process to permit the
user to define a compile-time flag to circumvent this problem.
Please uncomment the {\sf OPTC} line in the paragraph in Makefile.config
that looks like this:
\begin{verbatim}
# For SUN installing with GCC compiler but without GLIBC libraries
#OPTC = -O -DSUN_A -DSUN
\end{verbatim}
and do a "make remake".
\end{rmans}

\begin{question}
I get error messages regarding missing
{\tt strtoul}, and/or {\tt strerror} routines, even though I am using
the {\sf GCC} compiler. How do I get around this problem?
\end{question}

\begin{rmans}
Again, this is  most often seen with the older {\sf SUN OS}. 
The fix is the same as for the previous question.
\end{rmans}

\begin{question}
\label{SU:q:gcc}
Why do I get missing subroutine messages about {\sf ANSI C} routines?
Isn't the {\sf GCC} compiler supposed to be an {\sf ANSI} compiler?
\end{question}

\begin{rmans}
The {\sf GCC} compiler is just that, a compiler. It
draws on the libraries that are present on the machine.
If the {\sf GNU} libraries (this is the "glibc" package)
have not been installed, then the
{\sf GCC} compiler will use the libraries that are native to the machine
you are running on. Because the four routines listed above are
not available in the {\sf SUN 4. OS}, {\sf GCC} does not recognize them.
However, installing the {\sf GNU} libraries will make the {\sf GCC} compiler
behave as a full {\sf ANSI C} compiler.
\end{rmans}

\begin{question}
\label{SU:q:bugs}
Why do I get missing subroutine messages about {\sf ANSI C} routines?
I can't get the code to compile because my compiler can't find
"bzero" or "bcopy", how can  I fix this?
\end{question}

\begin{rmans}
You really shouldn't be having this problem, because
we try to keep to the ANSI standard, but sometimes old
style function calls creep in. The problem of rooting these things out
is exacerbated because many systems still support the old style calls.

If you have trouble installing
because your compiler can't find "bcopy" or "bzero"
make the following replacements.

Replace all statements of the form

{\small \begin{verbatim}
bzero( a, b);
\end{verbatim}} \noindent

with statements of the form:

{\small \begin{verbatim}
memset( (void *) a , (int) '\0', b );
\end{verbatim}} \noindent

Please replace all instances
of statements of the form of:
{\small \begin{verbatim}
bcopy ( a , b, c);
\end{verbatim}} \noindent
with a statements of the form:
{\small \begin{verbatim}
memcpy( (void *) b, (const void *) a, c );
\end{verbatim}} \noindent
\end{rmans}

\section{Data format questions}

In this section, we address questions about converting data
that are in various formats into {\small\sf SU} format.

\begin{question}
What is the data format that {\small\sf SU} programs expect?
\end{question}

\begin{rmans}
The {\small\sf SU} data format is based on, (but is not exactly the same as)
the {\sf SEG-Y} format. The {\small\sf SU} format
consists of data traces each of which has a header.
The {\small\sf SU} trace header is identical to {\sf SEG-Y} trace header.
Both the header and the trace data are written in the
native binary format of your machine.
You will need to use {\bf segyread\/} to convert SEGY data to SU data.

\noindent{\bf Caution}: The optional fields
in the {\sf SEG-Y} trace header are used for different purposes
at different sites.  {\small\sf SU} itself makes use of certain of these fields.
Thus, you may need to use {\tt segyclean}---see the answer to
Question~\ref{SU:q:segyclean}.
{\small\sf SU} format does not have the binary and ebcdic tape headers that
are part of the {\sf SEG-Y} format.

After installing the package, you can get more information on the
{\sf SEG-Y}/{\small\sf SU} header by typing: 
{\small \begin{verbatim}
% sukeyword -o
\end{verbatim}}\noindent
This lists the include file {\tt segy.h} that defines the {\small\sf SU} trace header.
\end{rmans}

\begin{question}
Is there any easy way of adding necessary 
{\sf SEG-Y} information to our own modeled data to prepare
our data for processing using the {\small\sf SU} package?
\end{question}

\begin{rmans}
It depends on the details of how your data was written to the file:
\begin{enumerate}
\item If you have a `datafile'
that is in the form of binary floating point numbers of the type
that would be created by a C program, then use {\tt suaddhead} to
put {\small\sf SU} ({\sf SEG-Y}) trace headers on the data. Example:
{\small \begin{verbatim}
% suaddhead < datafile  ns=N_SAMP > data.su
\end{verbatim}}\noindent
Here, \verb:N_SAMP: is the (integer) number of samples per
trace in the data.

\item If your data are Fortran-style floats, then you would use:
{\small \begin{verbatim}
% suaddhead < datafile ftn=1 ns=NS > data.su
\end{verbatim}}\noindent
See also, Question~\ref{SU:q:fortran}.

\item If your data are {\sf ASCII}, then use:
{\small \begin{verbatim}
% a2b n1=N1 < data.ascii | suaddhead ns=NS > data.su
\end{verbatim}}\noindent
Here \verb:N1: is the number of floats per line in the file
{\tt data.ascii}.

\item If you have some other data type, then you may use:
{\small \begin{verbatim}
% recast < data.other in=IN out=float | suaddhead ns=NS > data.su
\end{verbatim}}\noindent
where \verb:IN: is the type (int, double, char, etc...) 
\end{enumerate}

\noindent
For further information, consult the self-docs of the programs
{\tt suaddhead}, {\tt a2b}, and~{\tt recast}.
\end{rmans}

\begin{question}
\label{SU:q:segyclean}
I used {\tt segyread} to read a {\sf SEG-Y} tape.
Everything seems to work fine,
but when I plot my data with suximage, the window is black.
What did I do wrong?
\end{question}

\begin{rmans}
When you read an {\sf SEG-Y} tape, you need to pipe the data through
{\tt segyclean} to zero the optional {\sf SEG-Y} trace header field.
If the {\small\sf SU} programs see nonzero values in certain parts
of the optional field, they try
to display the data as ``nonseismic data,'' using those values
to set the plot parameters.

Another possibility is that there are a few data values that are so
large that they are overwhelming the 256 gray scale levels in the
graphics.
The way to get around this problem is to set {\bf perc=99} in the
graphics program. For example:
{\small \begin{verbatim}
% suximage < sudata  perc=99 &
\end{verbatim}} \noindent This will clip data values with size in
the top 1 percentile of the total data.
\end{rmans}

\begin{question}
I am trying to plot data with the {\tt pswigb}
(or {\tt pswigp}, or {\tt xwigb}, or  \ldots)
program.  I know that I have data with
\verb:n1=NSAMP: and \verb:n2=NTRACES:,
but when I plot, I find that I have to set \verb:n1=NSAMP+60: for the plot
to look even remotely correct. Why is this?
\end{question}

\begin{rmans}
It is likely that you are trying to plot with the wrong tool.
The input data format of the programs,
{\tt pswigb}, {\tt pswigp}, {\tt pscontour}, {\tt pscube}, {\tt psmovie},
{\tt xwigb}, {\tt xgraph}, and~{\tt xmovie},
expect data to consist of simple floating point numbers.
If your data are {\small\sf SU} data ({\sf SEG-Y}) traces,
then there is an additional
header at the beginning of each trace,
which, on most computer architectures,
is the same number (240) of bytes
as the storage for 60 floats.
 
\sloppypar{To plot these data, use respectively:
{\tt supswigb}, {\tt supswigp}, {\tt supscontour}, {\tt supscube},
{\tt supsmovie}, {\tt suxwigb}, {\tt suxgraph}, or~{\tt suxmovie}.}

Also, it is not necessary to specify the dimensions of the data for these
latter programs.  The {\tt su}-versions of the codes determine
the necessary information from the appropriate header values.
(In fact, that is {\em all} they do---the actual graphics is
handled by the version without the {\tt su} prefix.)
\end{rmans}

\begin{question}
I want to check the size of a file to see if it has the right number
of values, but I am not sure how to take the header into account.
How is this done?
\end{question}

\begin{rmans}
If the file consists of simple floating point numbers, then the
size in bytes equals the size of a float times the number of
samples (\verb:SIZE = 4 * N_SAMP:).
The {\small\sf SU} data ({\sf SEG-Y} traces)
also have a header (240 bytes per trace)
giving the total number of bytes as:\\
\verb:(240 + 4 N_SAMP ) N_TRACES:.\\
\noindent
The byte count computed in this way
is the number that the UNIX command {\tt ls -l} shows.

{\bf Caveats}: The above calculations assume that you have
the conventional architecture and that the header definition
in {\tt segy.h} has not been altered.  Watch out as machines
with 64 bit word size become common!
\end{rmans}

\begin{question}
\label{SU:q:fortran}
I have some data in Fortran form and tried to convert it to {\small\sf SU} data
via the following:
{\small \begin{verbatim}
% suaddhead < data.fortran ns=N_SAMP ftn=1 > data.su
\end{verbatim}}\noindent
but this did not work properly. I am sure that my fortran data
are in unformatted binary floats. What should I do?
\end{question}

\begin{rmans}
There are different ways of interpreting the term ``unformatted''
with regard to fortran data.  Try:
{\small \begin{verbatim}
% ftnstrip < data.fortran | suaddhead ns=N_SAMP > data.su
\end{verbatim}}\noindent

The program {\tt ftnstrip} can often succeed in converting
your fortran data into C-like binary data, even when the
\verb:ftn=1: option in {\tt suaddhead} fails.
(Note: the program {\bf ftnunstrip\/} may be used to take C-style
binary data and convert it to Fortran form.)
\end{rmans} 

\begin{question}
I just successfully installed the {\sf CWP/SU} package, but when I
try to run the demo scripts, I get many error messages describing
programs that the shell script cannot find. How do I fix this?
\end{question}

\begin{rmans}
You need to put {\tt CWPROOT/bin} (where {\tt CWPROOT}
is {\tt /your/root/path} that
contains the {\sf CWP/SU} source code, include files,
libraries, and executables)
in your shell {\tt PATH}. This is done in your {\tt .cshrc} file
if you run under
{\tt csh} or {\tt tcsh}.
In Bourne shell ({\tt sh}), Born Again shell ({\tt bash}), or Korn shell
({\tt ksh}) the {\tt PATH} variable is in your {\tt .profile} file.
You also need
to type
{\small\begin{verbatim}
% rehash
\end{verbatim}}\noindent
if you are running C-shell {\tt /bin/csh} or  TC-shell {\tt /bin/tcsh}
as your working shell environment, if you have not relogged since 
you compiled the codes. 
\end{rmans} 

\begin{question}
How do I transfer data between {\small\sf SU} and a commercial package, such
as Promax.
\end{question}

\begin{rmans}
The short answer is that you make a SEGY tape on disk file.
To do convert a file called, say, "data.su" to a segy file
do the following:

{\small \begin{verbatim}
% segyhdrs < data.su
% segywrite tape=data.segy < data.su
\end{verbatim}} \noindent

Now use Promax to read data.segy. This file is a
"Promax tape-on-disk file in IBM Real format."
Choose Promax menus accordingly.

\noindent For other commercial packages, use the appropriate
commands to read a SEGY tape on disk file.

\noindent To go from the commercial package to {\small\sf SU} follow
the reverse steps. Create a file that is a SEGY tape image
and then use 
{\small \begin{verbatim}
% segyread tape=data.segy | segyclean > data.su
\end{verbatim}} \noindent
\end{rmans}
 
\begin{question}
I would like to strip the trace headers off of some SU data, perform
an operation of some type on the bare traces and put the headers
back on without losing any of the header information. How do I do this?
\end{question}

\begin{rmans}
Do the following:

{\small \begin{verbatim}
% sustrip < data.su head=headers > data.binary
\end{verbatim}} \noindent

(Do whatever was desired to data.binary to make data1.binary)

{\small \begin{verbatim}
% supaste < data1.binary head=headers > data1.su
\end{verbatim}} \noindent
\end{rmans}

\begin{question}
I have made some data on an IBM RS6000 and have transferred it to
my Linux-based PC system. The data looks ok on the RS6000,
but when I try to work with it on the PC, none of the SU programs seem
to work. What is wrong?
\end{question}

\begin{rmans}
The problem you have encountered is that there are two IEEE binary
formats called respectively `big endian` and `little endian` or,
alternately `high byte` and `low byte`. These terms refer to the
order of the bytes that represent the data. IBM RS6000, Silicon
Graphics, NeXT (black hardware), SUN, HP, PowerPC, any Motorola
chip-based platforms are `big endian` machines, whereas, Intel-based
PCs and Dec and Dec Alpha products are `little endian` platforms.

Two programs are supplied in the CWP/SU package for swapping
the bytes for data transfer. These are  {\bf swapbytes} and {\bf suswapbytes}.

The program {\bf swapbytes} is designed to permit the user to swap
the bytes on binary data that are all one type of data (floats, doubles,
shorts, unsigned shorts, longs, unsigned longs, and ints).

For data that are in the {\small\sf SU} format, the program {\bf suswapbytes} is
provided.

Furthermore, within the programs {\bf segyread} and {\bf segywrite}
there are ``swap='' flags that permit the user to specify whether
the platform they are working on are ``big endian'' or ``little endian''
platforms.

In older releases of {\small\sf SU} there were problems with the bitwise operations
that would be encountered in the wiggle-trace drawing routines. However,
these problems have been fixed via the ENDIANFLAG that appears in
Makefile.config.
\end{rmans}

\begin{question}
How do I convert data that are in the SEG-2 format to SEGY?
\end{question}

\begin{rmans}
In \$CWPROOT/src/Third\_Party/seg2segy   there are two programs
that have been made available to us by the University of Pau
in France, for this purpose. These should be easy to install
on any system where {\small\sf SU} has been installed.

Once you have converted   data.seg2  to  data.segy, you may
read it into the {\small\sf SU} format via:

{\small \begin{verbatim}
% segyread tape=data.segy > data.su
\end{verbatim}} \noindent
\end{rmans}

\section{Tape reading and writing}
This section contains frequently asked questions about reading
and writing {\sf SEG-Y} tapes with {\small\sf SU}.

\noindent Tape reading/writing is more of an art than a science.
Here are a few tips. 
\begin{enumerate}
\item Make sure your tape drive is set to be variable block
    length. If you are on an {\sf IBM RS6000}, this means you
    will need to use {\tt smit} to set {\tt blocksize=0} on your tape
    device. Having the tape drive set to some default
    constant blocksize (say blocksize=1024 or 512)
    will foil all attempts to read an {\sf SEG-Y} tape.
\item To read multiple tape files on a tape, use the non
     rewinding device. On an {\tt RS6000} this would be
      something like {\tt /dev/rmtx.1}, see {\tt man mt} for details.
\item If this still doesn't work, then try:
{\small \begin{verbatim}
% dd if=/dev/rmtx of=temps bs=32767 conv=noerror
\end{verbatim}}\noindent
Here, {\tt /dev/rmtx} (not the real name of the device,
it varies from system
to system) is your regular (rewinding) tape device.
In the option, {\tt bs=32767}, we gave the right blocksize ($2^{16}+1$)
for an {\tt IBM/RS6000}.  Try
\verb:bs=32765:  ($2^{16}-1$) on a {\sf SUN}. 
This will dump the entire contents of the tape onto
a single file.
\end{enumerate}


\begin{question}
How do I write multiple SEG-Y files onto a tape?
\end{question}


\begin{rmans}
Here is a shell script for writing multiple files on a tape:
{\small \begin{verbatim}
#! /bin/sh

DEV=/dev/nrxt0  # non rewinding tape device

mt -f $DEV rewind

j=0
jmax=40

while test "$j" -ne "$jmax"
do
        j=`expr $j + 1`
        echo "writing tape file  $j"
        segywrite tape=$DEV bfile=b.$j hfile=h.$j verbose=1 buff=0 < ozdata.$j
done

exit 0
\end{verbatim}}\noindent
\end{rmans}

\section{Geometry Setting}
\begin{question}
How do I do ``geometry setting'' in SU?
\end{question}
\begin{rmans}

There is a common seismic data manipulation task that often is 
called "geometry setting" in commercial packages in which the
user converts information in the survey observers' logs
into values in the trace headers.

\noindent The CWP/SU package does indeed, have provisions for getting and
setting header fields, as well as computing a third header field
from one or two other header fields. The programs that you need
to use for this are:


\vspace{1ex}
\indent sugethw    ("SU get header word") \\
\indent sushw      ("SU set header word") \\
\indent suchw      ("SU change or compute header word")
\vspace{1ex}

\noindent Type the name of each program to see the self
documentation of that code.

\vspace{1ex}
\noindent In addition, to find out what the header field "keywords"
mentioned in these programs are:  type:    sukeyword -o

\vspace{1ex}
\noindent You may have the information in a variety of forms.
The most common and least complicated assumptions of that form 
will be made here.

\vspace{1ex}
\noindent The task requires the following basic steps.

\vspace{1ex}
\begin{enumerate}
\item Get your data into SU format. The SU format is not exactly SEGY,
   but it does preserve the SEGY header information. If you are
   starting with SEGY data (either on tape, or on in the form of
   a diskfile) then you use "segyread" to read the data into an
   su file format.

   For tape:

{\small \begin{verbatim}
      % segyread tape=/dev/rmt0 bfile=data.1 header=h.1 | segyclean > data.su
\end{verbatim}}\noindent

   For diskfile

{\small \begin{verbatim}
      %  segyread tape=data.segy bfile=data.1 header=h.1 | segyclean > data.su
\end{verbatim}} \noindent
   The file   data.segy is assumed here to be a "tape image" of segy data.
   You have to be careful because some commercial software will write
   SEGY-like data, by mimicking the layout of the SEGY format, but 
   this format will not be in the true IBM tape format that SEGY is defined
   to be.  In Promax, if you write a SEGY file in IBM Real format, then this
   will be true SEGY tape image.
   working on.

\item If you have your data in the SU format, then you may view the
   ranges of the SEGY headers (headers that are not set will not
   be shown) via:
{\small \begin{verbatim}
   % surange < data.su
\end{verbatim}}

\item Data often comes with some fields already set. To dump these
   fields in a format that is convenient for geometry setting,
   you would use    sugethw  in the following way:

{\small \begin{verbatim}
   % sugethw < data.su  output=geom  key=key1,key2,... > hfile.ascii
\end{verbatim}}

   The strings "key1,key2,..." are the keywords representing the desired
   SEGY trace header fields. These keywords may be listed via:

{\small \begin{verbatim}
   % sukeyword -o
\end{verbatim}}

\item Once you have dumped the desired header fields  into  hfile.ascii
   then you may edit them with the editor of your choice. The point
   is that you may create a multi-column ascii file that lists the
   values of specific header fields (trace by trace, as they appear
   in data.su) by *any* method you wish. Each column will contain
   the value of a specific header field to be set.

\item Now that you have created the ascii file containing your header values,
   you may load these values into data.su via:

{\small \begin{verbatim}
   % a2b < hfile.ascii n1=N_columns > hfile.bin
\end{verbatim}} \noindent
   Here,  N\_columns is the number of columns in   hfile.ascii.
   This is to convert hfile.ascii to a binary file.

   Now use:
{\small \begin{verbatim}
   % sushw < data.su key=key1,key2,...  infile=hfile.bin > data1.su
\end{verbatim}} \noindent
   Here   key1,key2,... are the appropriate keywords representing
   the fields being set, listed in the exact order the values appear,
   column by column in hfile.ascii.

\item If you want to compute a third header field from two given header
   field values, then you may use: {\bf suchw} for this.
   Also, if the header fields that you want to set are
   systematic in some way (are constant for each trace or vary
   linearly across a gather), then you don't have to use the
   "infile=" option. You may simply give the
   necessary values to   sushw.   See the selfdocs for   sushw and
   suchw  for examples of these.
\end{enumerate}
\end{rmans}

\section{Technical Questions}
\begin{question}
I want to resample my data so that I have half as many traces, and
half as many samples. How do I do that?
\end{question}

\begin{rmans}
To resample data, you must do the following: 

\begin{enumerate}
\item Check that you won't have aliasing. Do this by viewing the amplitude
spectra of your data. Do this with {\bf suspecfx\/}
{\small \begin{verbatim}
    suspecfx < data.su | suxwigb
\end{verbatim}} \noindent
\item If the bandwidth of your data extends beyond the new nyquist frequency
of your data (which, in this example, will be half of the original nyquist
frequency) then you will have to filter your data to fit within
its new nyquist bandwidth. Do this with {\bf sufilter\/}
{\small \begin{verbatim}
     sufilter < data.su f=f1,f2,f3,f4  amps=0,1,1,0 > data.filtered.su
\end{verbatim}} \noindent
Here, the {\bf f1 f2 f3 f4\/} are the filter corner frequencies and
{\bf amps=0,1,1,0\/} indicate that the filter is a bandpass filter.
\item Now you may resample your data with suresamp:
{\small \begin{verbatim}
    suresamp < data.filtered.su  nt=NTOUT dt=DTOUT > data.resampled.su
\end{verbatim}} \noindent
\end{enumerate}
For your case, NTOUT is 1/2 of the original number of samples, and
DTOUT is twice the time sampling interval (in seconds) of that in
the original data.  Your output data should look quite similar to 
your input data, with the exception that the bandwidth will change.
\end{rmans}

\section{General}
This section addresses general questions about the {\small\sf SU} package.

\begin{question}
What are these funny words gelev, selev, fldr, etc. that I see
in various places?
\end{question}

\begin{rmans}
These are the "keywords" that are required for many of the codes.
They refer to SU (Segy) header fields.

\begin{verbatim}
   Type:   sukeyword -o                to see the whole list 
   Type:   sukeyword keyword           to see the listing for an individual
                                       keyword
\end{verbatim}
\end{rmans}

\begin{question}
What do the terms ``little endian'' and ``big endian'' and  mean?
\end{question}

\begin{rmans}
There are two IEEE binary formats, called respectively
'little endian' and 'big endian'. These are also called
'high byte' and 'low byte', respectively.
These refer to the byte order in the bitwise representation of
binary data. The following platforms are 'little endian': DEC and
Intel-based PC's. The other common platforms are "big endian":
IBM RS6000, Silicon Graphics, NeXT (black hardware), SUN,
HP, PowerPC, any Motorola chip-based platform.
\end{rmans}

\begin{question}
Why are {\sf CWP/SU} releases given by integers (22, 23, 24, etc...)
instead of the more familiar decimal release numbers (1.1, 1.3, etc...)?
\end{question}

\begin{rmans}
The {\sf CWP/SU} release numbers are chosen to correspond
to the {\tt SU NEWS} email messages.
The individual codes in the package have traditional decimal
release numbers (assigned by {\sf RCS}), but these are all different.
The package changes in incremental, but non-uniform ways, so the standard
notation seems inappropriate. However, the user may view 24 to be
2.4. We may adopt this convention in the future.

{\bf Remark}:  In the early days, we {\em did} use {\sf RCS} to
simultaneously update all the codes to 2.1, 3.1, \ldots .  This
practice died a natural death somewhere along the way.
\end{rmans}

\begin{question}
How often are the codes updated?
\end{question}

\begin{rmans}
The {\sf CWP/SU\/} package is updated at roughly 3-6 month intervals.
We mail announcements of these releases to all known users.  Since
we do not provide support for outdated versions, we urge you to remain current.
\end{rmans}

\begin{question}
I have a complicated collection of input parameters for a {\sf CWP/SU}
program.  I want to run the command from the command line of a terminal
window, but I don't want to retype the entire string of input parameters.
What do I do?
\end{question}

\begin{rmans}
{\sf CWP/SU} programs that take their input parameters from the command
line also have the feature of being able to read from a
``parameter file.''   This is invoked by setting
the parameter \verb:par=parfile:, where {\tt parfile} is a file containing
the desired commandline string.

For example:
{\small \begin{verbatim}
suplane ntr=20 nt=40 dt=.001 | ...
\end{verbatim}}\noindent
is completely equivalent to the command:
{\small \begin{verbatim}
suplane par=parfile | ...
\end{verbatim}}\noindent
if the string
{\small \begin{verbatim}
ntr=20 nt=40 dt=.001
\end{verbatim}}\noindent
is contained  in `parfile.'
\end{rmans}

\begin{question}
I can't find an {\bf sudoc\/} entry for the function "ints8r," yet the
SU manual says that all library functions have online documentation?
What am I doing wrong?
\end{question}

\begin{rmans}
The proper search procedure for a library function (such as ints8r) is:
{\small \begin{verbatim}
% sufind ints8r
\end{verbatim}} \noindent
Which yields:

\begin{verbatim}
INTSINC8 - Functions to interpolate uniformly-sampled data via 8-coeff. sinc
                approximations:

ints8c  interpolation of a uniformly-sampled complex function y(x) via an


For more information type: "sudoc program_name <CR>"
\end{verbatim}

The name INTSINC8 is the name of the file that contains the
library function ins8c. You may now use {\bf sudoc\/} to find out more
information via:

{\small \begin{verbatim}
% sudoc intsinc8
\end{verbatim}} \noindent

Which yields:

\begin{verbatim}
In /usr/local/cwp/src/cwp/lib: 
INTSINC8 - Functions to interpolate uniformly-sampled data via 8-coeff. sinc
                approximations:

ints8c  interpolation of a uniformly-sampled complex function y(x) via an
         8-coefficient sinc approximation.
ints8r  Interpolation of a uniformly-sampled real function y(x) via a
                table of 8-coefficient sinc approximations

Function Prototypes:
void ints8c (int nxin, float dxin, float fxin, complex yin[], 
        complex yinl, complex yinr, int nxout, float xout[], complex yout[]);
void ints8r (int nxin, float dxin, float fxin, float yin[], 
        float yinl, float yinr, int nxout, float xout[], float yout[]);

Input:
nxin            number of x values at which y(x) is input
dxin            x sampling interval for input y(x)
fxin            x value of first sample input
yin             array[nxin] of input y(x) values:  yin[0] = y(fxin), etc.
yinl            value used to extrapolate yin values to left of yin[0]
yinr            value used to extrapolate yin values to right of yin[nxin-1]
nxout           number of x values a which y(x) is output
xout            array[nxout] of x values at which y(x) is output

Output:
yout            array[nxout] of output y(x):  yout[0] = y(xout[0]), etc.

Notes:
Because extrapolation of the input function y(x) is defined by the
left and right values yinl and yinr, the xout values are not restricted
to lie within the range of sample locations defined by nxin, dxin, and
fxin.

The maximum error for frequiencies less than 0.6 nyquist is less than
one percent.

Author:  Dave Hale, Colorado School of Mines, 06/02/89

\end{verbatim}
\end{rmans}

\begin{question}
I have written my own SU programs and would like them to appear 
in the {\bf suname\/} and {\bf sudoc\/} listings. How do I do this?
\end{question}

\begin{rmans}
Run {\bf updatedocall\/} (source code located in CWPROOT/par/shell).
If you have put this code under a new path, then you must add
this path to the list of paths in the updatedoc script.
For the selfdoc information to be captured by the updatedoc script,
you will need to have the following marker lines at the beginning
and end of the selfdoc and additional information portion of the 
source code of your program.
\begin{verbatim}
/*********************** self documentation **********************/
/**************** end self doc ********************************/
\end{verbatim}
Be sure to clone these directly out of an existing SU program, rather
than typing them yourself, so that the pattern is the exact one
expected by the updatedoc script.
\end{rmans}

\begin{question}
I have a gray scale (not color) PostScript file made with psimage
and would like to convert it to a color PostScript format, but do
not have the original binary data that I made the file from. How
do I do this?
\end{question}

\begin{rmans}
You have to restore the binary file to make the new color PostScript
file.  Here is how you do it. (Here, we are assuming a bit-mapped
graphic as would be produced by psimage or supsimage).
\begin{enumerate}
\item Make a backup of your PostScript file.
\item edit the PostScript file removing everything but the
    hexidecimal binary image that makes up the majority of
    the file. (Note, in the line preceeding the hexidecimal data
    portion of the file will be a pair of numbers that represents
    the dimensions of the data. You will need these numbers for
    later steps.)
\item use    h2b   to convert the hexidecimal file to binary
\item You will find that the file is flipped from the original
    input file.  Use   transp   to flip the data. Note that the
    n1 and n2 values that are used by transp are the dimensions
    of the input data, which are the reverse of the output data.
    (The n1 value, is {\em not\/} the total number of samples, that
    is returned by h2b, instead  
$ \mbox{total  no. values} = \mbox{n1}\times \mbox{n2}$.)
\item You now have a 0-255 representation of your binary data
    which you should be able to plot again any way you desire.
\end{enumerate}

This method may be used to convert scanned images to {\small\sf SU\/} format,
as well, with the next step in the procedure to be putting {\small\sf SU\/}
headers on the data with  {\bf suaddhead}.
\end{rmans}


\chapter{How to Write an SU Program}

\section{Setting up the Makefile}

The CWP/SU package uses a sophisticated Makefile structure, that you may also
use when you develop new code. You should begin any new code writing project by
creating a local directory in your working area.
You should then copy the Makefile from \$CWPROOT/src/su/main into that directory
and make the following changes
\begin{verbatim}
Change:

D = $L/libcwp.a $L/libpar.a $L/libsu.a

LFLAGS= $(PRELFLAGS) -L$L -lsu -lpar -lcwp -lm $(POSTLFLAGS)

to:

D = $L/libcwp.a $L/libpar.a $L/libsu.a

B = .

OPTC = -g

LFLAGS= $(PRELFLAGS) -L$L -lsu -lpar -lcwp -lm $(POSTLFLAGS)

Change:


PROGS =                 \
         $B/bhedtopar    \
         $B/dt1tosu      \
         $B/segyclean    \
         $B/segyhdrs     \
         $B/segyread     \
        ...
         ...

to:

PROGS =                 \
         $B/yourprogram
\end{verbatim}
where the source code of your program is called  ``yourprogram.c'' and resides in
this directory.


You should then  be able to  simply type ``make'' and ``yourprogram'' will be compiled.

As a test you can try copying one of the existing SU programs, from \$CWPROOT/src/su/main
into your local working directory, modifying the Makefile accordingly and typing: make.

Indeed, because all new SU programs may be viewed as beginning as clones of existing SU
programs of a similar structure, this is perhaps the best way to begin any new coding
venture.


\section {A template SU program\label{SU:sec:template}}
Although variations are usually needed, a template for a typical {\small\sf SU} program
looks like the program listing below (we excerpted lines from the program {\tt sumute} to 
build this template).  The numbers in square brackets at the end of the lines in the 
listing are not part of the listing---we added them to facilitate discussion of the 
template.  The secret to efficient {\small\sf SU} coding is finding an existing program 
similar to the one you want to write.  If you have trouble locating the right code or 
codes to ``clone,'' ask us---this can be the toughest part of the job!
 
{\small\begin{verbatim}
/* SUMUTE: $Revision: 1.21 $ ; $Date: 2019/07/26 18:00:07 $      */  [1]

#include "su.h"                                                     [2]
#include "segy.h"

/*********************** self documentation **********************/ [3]
char *sdoc[] = {
"                                                                ",
" SUMUTE - ......                                                ",
"                                                                ",
" sumute <stdin >stdout                                          ",
"                                                                ",
" Required parameters:                                           ",
"         none                                                   ",
"                                                                ",
" Optional parameters:                                           ",
"        ...                                                     ",
"                                                                ",
" Trace header fields accessed: ns                               ",
" Trace header fields modified: none                             ",
"                                                                ",
NULL};
/**************** end self doc ***********************************/

/* Credits:
 *
 *        CWP: Jack Cohen, John Stockwell
 */


segy tr;                                                             [4]

main(int argc, char **argv)
{
        int ns;                /* number of samples          */      [5]
        ...


        /* Initialize */                 
        initargs(argc, argv);                                        [6]
        requestdoc(1);                                               [7]

        /* Get parameters */
        if (!getparint("ntaper", &ntaper))        ntaper = 0;        [8]

                                                
        /* Get info from first trace */
        if (!gettr(&tr)) err("can't read first trace");              [9]
        if (!tr.dt) err("dt header field must be set");              [10]

        /* Loop over traces */
        do {                                                         [11]
                int nt     = (int) tr.ns;                            [12]

                if (below == 0) {                                    [13]
                        nmute = NINT((t - tmin)/dt);
                        memset((void *) tr.data, (int) '\0', nmute*FSIZE);
                        for (i = 0; i < ntaper; ++i)
                                tr.data[i+nmute] *= taper[i];
                } else {
                        nmute = NINT((nt*dt - t)/dt);
                        memset((void *) (tr.data+nt-nmute),
                                        (int) '\0', nmute*FSIZE);
                        for (i = 0; i < ntaper; ++i)
                                tr.data[nt-nmute-1-i] *= taper[i];
                }
                puttr(&tr);                                           [14]
        } while (gettr(&tr));                                         [15]
        
        return EXIT_SUCCESS;                                          [16]
}
\end{verbatim}}\noindent
{\bf Discussion of numbered lines:}

\begin{enumerate}
\item We maintain the internal versions of the codes with the UNIX utility {\sf RCS}.  This item shows the string template for {\sf RCS}.
\item The file {\tt su.h} includes (directly or indirectly) all our locally defined 
macros and prototypes.  The file {\tt segy.h} has the definitions for the trace header 
fields.
\item The starred lines delimit the ``self-doc'' information---include them exactly as 
you find them in the codes since they are used by the automatic documentation shells.  
The style of the self-doc shown is typical except that often additional usage information 
is shown at the bottom and, of course, often there are more options.  Look at some 
existing codes for ideas.
\item This is an external declaration of an {\small\sf SU} ({\sf SEG-Y}) trace buffer.  
It is external to avoid wasting stack space.
\item We usually describe the global variables at the time of declaration.
Examine codes related to yours to increase consistency of nomenclature
(there is no official {\small\sf SU} naming standard).
\item The {\tt initargs} subroutine sets {\small\sf SU}'s command line passing 
facility (see page~\pageref{SU:page:getpar}).
\item The {\tt requestdoc} subroutine call specifies the circumstances under which 
self-doc will be echoed to the user.  The argument `1' applies to the  typical program that uses only standard input (i.e. \verb+<+) to read an {\small\sf SU} trace file.  Use `0' for 
codes that create synthetic data (like {\tt suplane}) and `2' for codes that require 
two input files (we could say ``et cetera,'' but there are no existing {\small\sf SU} 
mains that require {\em three} or more input files).
\item This is typical code for reading `parameters from the command line.  
Interpret it like this: ``If the user did not specify a value, then use the default value.''  The subroutine must be type-specific, here we are getting an {\em integer} parameter.
\item Read the first trace, exit if empty.  The subroutine {\tt fgettr} ``knows about'' 
the {\small\sf SU} trace format.  Usually the trace file is read from standard input and then we use {\tt gettr} which is a macro based on {\tt fgettr} defined in {\tt su.h}.  Note 
that this code implies that the first trace is read into the trace buffer (here 
called {\tt tr}), therefore we will have to process this trace before the next 
call to {\tt fgettr}.
\item We've read that first trace because, we need to get some trace parameters from the 
first trace header.  Usually these are items like the number of samples ({\tt tr.ns}) 
and/or the sampling interval ({\tt tr.dt}) that, by the {\sf SEGY-Y} standard, are the 
same for all traces.
\item Since the first trace has been (typically) read before the main processing loop starts, we use a ``do-while'' that reads a new trace at the {\em bottom} of the loop.
\item We favor using {\em local} variables where permitted.
\item This is the seismic algorithm--here incomplete.  We've left in some of the actual {\tt sumute} code because it happens to contains lines that will be useful in the new code, we'll be writing below.  You may want to call a subroutine here to do the real work.
\item {\tt fputtr} and {\tt puttr} are the output analogs of {\tt fgettr} and {\tt gettr}.
\item The loop end.  {\tt gettr} returns a 0 when the trace file is exhausted and the processing then stops.
\item This is an {\sf ANSI-C} macro conventionally used to indicate successful program termination.
\end{enumerate}

\section{Writing a new program: {\tt suvlength}}

A user asked about {\small\sf SU} processing for variable length traces.  At his
institute, data are collected from time of excitation to a variable
termination time.  The difficulty is that {\small\sf SU} processing is based on
the {\sf SEG-Y} standard which mandates that all traces in the data set
be of the same length.  Rather than contemplating changing all of {\small\sf SU},
it seems to us that the solution is to provide a program that converts
the variable length data to fixed length data by padding with zeroes
where necessary at the end of the traces---let's name this new program
{\tt suvlength}.  We can make the length of the output traces a user
parameter.  If there is a reasonable choice, it makes sense to provide
a default value for parameters.  Here, using the length of the first
trace seems the best choice since that value can be ascertained before
the main processing loop starts.

So far, so good.  But now our plan runs into a serious snag: the
fundamental trace getting facility, {\tt gettr}, itself assumes fixed
length traces (or perhaps we should say that {\tt gettr} deliberately
enforces the fixed length trace standard).  But, if you think about
it, you'll realize that {\tt gettr} itself has to take special measures
with the {\em first} trace to figure out its length.  All we have to do
is make a new trace getting routine that employs that first trace
logic for {\em every} trace.  Here, we'll suppress the details of
writing the ``fvgettr'' subroutine and turn to converting the
template above into the new {\tt suvlength} code:

{\small\begin{verbatim}
/* SUVLENGTH: $Revision: 1.21 $ ; $Date: 2019/07/26 18:00:07 $   */

#include "su.h"
#include "segy.h"

/*********************** self documentation **********************/
char *sdoc[] = {
"                                                                ",
" SUVLENGTH - Adjust variable length traces to common length     ",
"                                                                ",
" suvlength <variable_length_traces >fixed_length_traces         ",
"                                                                ",
" Required parameters:                                           ",
"         none                                                   ",
"                                                                ",
" Optional parameters:                                           ",
"        ns      output number of samples (default: 1st trace ns)",
NULL};
/**************** end self doc ***********************************/

/* Credits:
 *        CWP: Jack Cohen, John Stockwell
 */

/* prototype */
int fvgettr(FILE *fp, segy *tp);

segy tr;

main(int argc, char **argv)
{
        int ns;        /* number of samples on output traces  */


        /* Initialize */                 
        initargs(argc, argv);
        requestdoc(1);
 
        /* Get parameters */
        ...
        
        /* Get info from first trace */
        ...

        ...

        return EXIT_SUCCESS;                                          [16]
}

/* fvgettr code goes here */
        ...

\end{verbatim}}\noindent
Now we run into a small difficulty.  Our only parameter has a default
value that is obtained only after we read in the first trace.  The
obvious solution is to reverse the parameter getting and the trace
getting in the template.  Thus we resume:
{\small\begin{verbatim}
        /* Get info from first trace and set ns */ 
        if (!fvgettr(stdin, &tr))  err("can't get first trace"); 
        if (!getparint("ns", &ns))    ns = tr.ns;

        /* Loop over the traces */
        do {
                int nt = tr.ns;
\end{verbatim}}\noindent
Now comes the actual seismic algorithm---which is rather trivial in
the present case:  add zeroes to the end of the input trace if the
output length is specified greater than the input length.  We could
write a simple loop to do the job, but the task is done most
succinctly by using the {\sf ANSI-C} routine {\tt memset}.  However, we
confess that unless we've used it recently, we usually forget how to
use this routine.  One solution is to {\tt cd} to the {\tt su/main}
directory and use {\tt grep} to find other uses of {\tt memset}.  When
we did this, we found that {\tt sumute} had usage closest to what we
needed and that is why we started from a copy of that code.  Here is
the complete main for {\tt suvlength}:
{\small\begin{verbatim}
/* SUVLENGTH: $Revision: 1.21 $ ; $Date: 2019/07/26 18:00:07 $        */

#include "su.h"
#include "segy.h"

/*********************** self documentation **********************/
char *sdoc[] = {
"                                                                 ",
" SUVLENGTH - Adjust variable length traces to common length      ",
"                                                                 ",
" suvlength <vdata >stdout                                        ",
"                                                                 ",
" Required parameters:                                            ",
"         none                                                    ",
"                                                                 ",
" Optional parameters:                                            ",
"          ns     output number of samples (default: 1st trace ns)",
NULL};
/**************** end self doc ***********************************/

/* Credits:
 *        CWP: Jack Cohen, John Stockwell
 *
 * Trace header fields accessed:  ns
 * Trace header fields modified:  ns
 */

/* prototype */
int fvgettr(FILE *fp, segy *tp);

segy tr;

main(int argc, char **argv)
{
        int ns;                /* samples on output traces        */


        /* Initialize */
        initargs(argc, argv);
        requestdoc(1);


        /* Get info from first trace */ 
        if (!fvgettr(stdin, &tr))  err("can't get first trace"); 
        if (!getparint("ns", &ns))    ns = tr.ns;


        /* Loop over the traces */
        do {
                int nt = tr.ns;
                                
                if (nt < ns) /* pad with zeros */
                        memset((void *)(tr.data + nt), '\0', (ns-nt)*FSIZE);
                tr.ns = ns;
                puttr(&tr);
        } while (fvgettr(stdin, &tr));
        
        return EXIT_SUCCESS;
}


#include "header.h"

/* fvgettr - get a segy trace from a file by file pointer (nt can vary)
 *
 * Returns:
 *        int: number of bytes read on current trace (0 after last trace)
 *
 * Synopsis:
 *        int fvgettr(FILE *fp, segy *tp)
 *
 * Credits:
 *        Cloned from .../su/lib/fgettr.c
 */

int fvgettr(FILE *fp, segy *tp)
   ...
\end{verbatim}}\noindent
{\bf Remark}: In the actual {\small\sf SU}, the subroutine {\tt fvgettr} has been
extracted as a library function and we also made a convenience macro
{\tt vgettr} for the case of standard input.  But these are secondary
considerations that don't arise for most applications.

For any new {\small\sf SU} code, one should provide an example shell program to show how
the new code is to be used.  Here is such a program for X Windows graphics:
{\small\begin{verbatim}
#! /bin/sh
# Trivial test of suvlength with X Windows graphics

WIDTH=700
HEIGHT=900
WIDTHOFF=50
HEIGHTOFF=20

>tempdata
>vdata
suplane >tempdata  # default is 32 traces with 64 samples per trace
suplane nt=72 >>tempdata
suvlength <tempdata ns=84 |
sushw key=tracl a=1 b=1 >vdata

# Plot the data 
suxwigb <vdata \
        perc=99 title="suvlength test"\
        label1="Time (sec)" label2="Traces" \
        wbox=$WIDTH hbox=$HEIGHT xbox=$WIDTHOFF ybox=$HEIGHTOFF &

# Remove #comment sign on next line to test the header
#sugethw <vdata tracl ns | more
\end{verbatim}}\noindent

\appendix
\chapter{Obtaining and Installing SU  \label{app:A}}
The {\small\sf SU} package contains seismic processing programs along with
libraries of scientific routines, graphics routines and
routines supporting the {\small\sf SU} coding conventions.
The package is available by anonymous ftp at the site
ftp.cwp.mines.edu (138.67.12.4). The directory path is pub/cwpcodes.
The package may also be obtained on the World Wide Web at
http://www.cwp.mines.edu/cwpcodes.
Take the files:
\begin{enumerate}
\item README\_BEFORE\_UNTARRING
\item untar\_me\_first.xx.tar.Z
\item cwp.su.all.xx.tar.Z
\end{enumerate}
Here the {\tt xx} denotes the number of the current release.
An incremental update is also available for updating the
previous release {\tt yy} to the current release {\tt xx}.  Take the files:

\begin{enumerate}
\item README\_BEFORE\_UNTARRING
\item README\_UPDATE
\item untar\_me\_first.xx.tar.Z
\item update.yy.to.xx.tar.Z
\item update.list
\end{enumerate}

\noindent If you find that {\tt ftp} times out during the transmission of the
files, the package is available in smaller pieces in the subdirectory
{\tt outside\_usa}.

\noindent For readers who are not familiar with anonymous ftp,
an annotated transaction listing follows in section~\ref{SU:sec:anonftp}.

\section{Obtaining files by anonymous ftp\label{SU:sec:anonftp}}
\begin{tabular}{lll}
Type: & &  \\
\% ftp 138.67.12.4        & --- &  138.67.12.4 is our ftp site  \\
username: anonymous       & --- &   your username is ``anonymous''     \\
password: yourname@your.machine.name   & --- &  type anything here  \\
& &  \\
ftp$>$     & --- & this is the prompt you see \\
& & when you are in ftp 
\end{tabular}

\indent You are now logged in via ftp to the CWP anonymous ftp site.
You may type:

\begin{tabular}{lll}
ftp$>$ ls             & --- & to see the contents of the directories \\
ftp$>$ cd dirname     & --- & to change directories to ``dirname'' \\
ftp$>$ binary         & --- & to set ``binary mode'' for transferring files \\
        & &            You must do this before you try to transfer any \\
        & &            binary file. This includes all files with the form \\
        & &            some\_name.tar.Z extension. \\
ftp$>$ get filename   & --- & to transfer  ``filename'' from our site to your machine \\
ftp$>$ mget pattern*  & --- & to transfer all files with names of the ``pattern*'' \\
For example: & & \\
&& \\
ftp$>$ mget *.tar.Z   & --- & will transfer all files with the form of name.tar.Z \\
& &              to your machine. You will be asked whether you  \\
& &              really want each file of this name pattern transferred, \\
& &               before ftp actually does it.  \\
ftp$>$ bye            & --- & to exit from ftp 
\end{tabular}

\section{Requirements for installing the package}
The only requirements for installing the package are:
\begin{enumerate}
\item A machine running the UNIX operating system.
\item An {\sf ANSI C} compiler.
\item A version of make which supports include files
\item 16-60 megabytes (depending on system) of disk space for
the source and compiled binary. If space is an issue, then the
compiled binaries may be ``stripped'' by cd'ing to \$CWPROOT/bin
and typing "strip *".
\end{enumerate}

\noindent
The package has been successfully installed on:
\begin{itemize}
%update
\item IBM RS6000
\item SUN SPARC STATIONS
\item HP 9000 series machines
\item HP Apollo
\item NeXT
\item Convex
\item DEC
\item Silicon Graphics
\item PC's running LINUX, PRIME TIME, SCO, FREE BSD, ESIX, and NeXTSTEP 486
\end{itemize}

There are README files in the distribution with special notes about some
of these platforms.  We depend on the {\small\sf SU} user community to alert us to 
installation problems, so if you run into difficulties, please let us know.

The distribution contains a series of files that detail the installation 
process.  Read them in the following order:

{\small\begin{verbatim}
LEGAL_STATEMENT --- license,  legal statement
README_BEFORE_UNTARRING --- initial information
README_FIRST --- general information
README_TO_INSTALL --- installation instructions
Portability/README_*    --- portability information for various platforms
README_GETTING_STARTED --- how to begin using the codes
\end{verbatim}}\noindent
Many of these files are contained within untar\_me\_first.xx.tar.Z.

\section{A quick test}
\begin{figure}
\epsfxsize 250pt
\centerline{\epsffile{suplane.eps}}
\caption{Output of the \protect\verb:suplane: pipeline.}
\label{fig:suplane}
\end{figure}

Once you have completed the installation, here is a quick test you can make
to see if you have a functioning seismic system.
For an X-windows machine, the ``pipeline''
\begin{verbatim}
suplane | suxwigb &
\end{verbatim}
should produce the graphic shown in Figure~\ref{fig:suplane}. 
If you have a PostScript printer, then you should get a hard copy version
with the pipeline
\begin{verbatim}
suplane | supswigb | lpr
\end{verbatim}
If you have Display PostScript, or a PostScript previewer, then to get
a screen display, replace the \verb:lpr: command in the pipeline by
the command that opens a PostScript file, for example:
\begin{verbatim}
suplane | supswigb | ghostview -
\end{verbatim}

Another set of test pipelines are
\begin{verbatim}
susynlv | supsimage | lpr
\end{verbatim}
\begin{verbatim}
susynlv | supswigb | ghostview -
\end{verbatim}

\begin{figure}
\epsfxsize 300pt
\centerline{\epsffile{susynlv.eps}}
\caption{Output of the \protect\verb:susynlv: pipeline.}
\label{fig:susynlv}
\end{figure}

\chapter{Help Facililties \label{app:B}}
\section{Suhelp}
The full text of the output from:
\begin{verbatim}
% suhelp
\end{verbatim} \noindent

\input suhelp.tex

\section{Suname}
The full text of the output from:
\begin{verbatim}
% suname
\end{verbatim}\noindent
\input suname.tex

\section{Suhelp.html}
This is the text listing of Chris Liner's SU Help page.

\input suhelphtml.tex

\section{SUKEYWORD - list the SU datatype (SEGY) Keywords}
The following is the listing put out by

\input sukeyword.tex

\chapter{DEMOS - a brief descriptions of the demos}

\input demos.tex


\end{document}
