\documentstyle[11pt,epsf]{book}

% make text fill more of the page
%\nofiles
%\scrollmode
\textwidth 6.25in
\textheight 8.75in
\oddsidemargin .125in
\evensidemargin .125in
\topmargin -.5in

\def\releasenumber{32\ }
\def\currentyear{1998\ }
\def\ftpsite{ftp.cwp.mines.edu}
\def\ipaddress{138.67.12.4}
\newtheorem{question}{Question}
\newtheorem{answer}{Answer}
\newenvironment{rmans}{\begin{answer} \em}{\end{answer}}

\begin{document}

\input title.tex
\input legal.tex

\tableofcontents
\newpage

\chapter*{Acknowledgments}
%\addtocontents{toc}{{\bf Acknowledgments}}
\addtocontents{toc}{\protect \contentsline {chapter}{\protect \numberline {}Acknowledgements}{v}}
\markboth{ACKNOWLEDGMENTS}{}

The Seismic Unix project is partially supported by the Gas Research
Institute (GRI), by the Society of Exploration Geophysicists
(SEG), and by the Center for Wave Phenomena (CWP) Department,
of Mathematical and Computer Sciences, Colorado School of Mines.


That you GRI, SEG, and CWP for your continued and expanded support!

The sponsors of the CWP Consortium Project have long been partners
in the  SU project and we are pleased to explicitly acknowledge that
relationship here.  In addition, we wish to acknowledge extra support
supplied in the past by IBM Corporation and by the Center for
Geoscientific Computing at the Colorado School of Mines during the
period when SU was ported to the modern workstation from its
previous incarnation on graphics terminals.

So many faculty and students, both at our Center and elsewhere, have
contributed to SU, that it is impossible to list them all here.
However, certain people have made such important contributions that they
deserve explicit mention.

Einar Kjartansson wrote the first draft of what is now called SU
(the SY package) in cooperation with Shuki Ronen while both were 
at Stanford University.
In turn, some of the fundamental concepts they implemented were
formulated by their mentor, Jon Claerbout, Director of the Stanford
Exploration Project.  Ronen brought this work to our Center during a
two-year stay here and, during this time, aided Cohen in
turning SU into a supportable and exportable product.

Chris Liner, while a student at the Center, wrote most of the graphics
codes used in the pre-workstation (i.e, graphics terminal) age of  SU.
Liner's broad knowledge of seismology and seismic processing enabled
him to make a positive and continuing influence on the SU coding
philosophy.

Craig Artley, now at Golden Geophysical, made major contributions to
the graphics codes while a student at CWP and continues to make
significant contributions to the general package.

Dave Hale wrote several of the ``heavy duty'' processing codes as well
as most of the core scientific and graphics libraries.  His knowledge
of good C-language coding practice helped make our package a good
example for applied computer scientists.

Ken Larner contributed many user interface ideas based on his
extensive knowledge of seismic processing in the ``real world.''

John Scales showed how to use  SU effectively in the classroom in his
electronic text, Theory of Seismic Imaging, Samizdat Press, 1994.
This text is available from the CWP anonymous ftp site.

John Stockwell is largely responsible for the easy installation and
portability of the package, and continues to be the main contact
for the project since the first public release in September of 1993
(Release 18). 

The project has also has had extensive technical help from the
worldwide SU user community.
Among those who should be singled out for mention are Tony Kocurko at
Memorial University in Newfoundland, Toralf Foerster of the Institut
of Baltic Research in Warnemuende Germany, Stewart A. Levin, John
Anderson, and Joe Oravetz at Mobil Oil, Joe Dellinger at Amoco, Matthew
Rutty in Australia, Jens Hartmann in Germany, Alexander Koek in Delft,
Wenying Cai at the University of Utah, and Torsten Shoenfelder of Germany.
Our apologies in advance
for undoubtedly omitting mention of other deserving contributors to
 SU---we promise to include you in future updates of this manual!

We especially thank Barbara McLenon for her detailed suggestions on the text
and also for her excellent design of this document, as well as the
SU pamplets, and other materials, which we distribute at public meetings.

\section*{In Memorium}
Dr. Jack K. Cohen passed away in October 1996. The Seismic Unix
package exists owing to his knowledge of seismic processing,
his creativity, and his desire to make a lasting contribution
to the scientific community.  He will surely be missed by all
who had contact with him in the mathematical, geophysical, and 
SU-user communities.

\chapter*{Preface}
In the 3 years that have elapsed since the first version of
this manual was distributed, there have been numerous changes
in the SU package.
These changes have been a result of
both in-house activities here at CWP and, equally important,
to the many contributions of code, bug fixes, extentions,
and suggestions from SU users outside of CWP.
After reviewing these many changes and extensions, and after
much discussion with members of the worldwide SU user commuinity,
it has become apparent that a new manual was necessary.

This new version began in preparation for a short course on
SU at the CREWES project at the University of Calgary.
Many of the items that were in the original manual,
such as information about obtaining and installing the package,
have been have been moved to appendicies.
Additional sections describing the demos have been added,
and the entire manual has been expanded.

The intent is that you will be able to find your way around
the package, via the help mechanisms, that you will learn to
run the programs by running the demos, and finally will be
able to see how to begin writing SU code, drawing on the
source code of the package as examples.

\chapter{About SU}
\pagenumbering{arabic}

In 1983, Jack K. Cohen and Shuki Ronen of the Center for Wave Phenomena (CWP)
at the Colorado School of Mines (CSM)
conceived a bold plan. This  plan was to create a seismic processing 
environment for Unix-based systems,
written in the C language, that would extend the Unix operating 
system to seismic processing and research tasks.
Furthermore, they intended that the package be freely available as
full source code to anyone who would want it.
They began with a package called SY, created by Einar Kjartansson
and Ronen, while Ronen was a student at Jon Claerbout's Stanford 
Exploration Project (SEP).
This package was a sharp departure from seismic processing software
that was available at the time,
because the industry standard at the time was to use Fortran programs on
VAX VMS based systems.

By the time that Cohen and Ronen created the first version of
SU (c. 1984) the sponsors of CWP had already begun showing interest
in the package.
The availability of Unix-based workstations combined with the influx of
Unix-literate geophysicists from the major academic institutions,
shifted the industry to using primarily Unix-based systems for research
and for processing, increasing the interest in Unix-based software,
including SU.

Until September of 1993, SU was used primarily in-house at CWP. 
Earlier versions of SU which had been ported to CWP sponsor companies
became the foundation for in-house seismic processing packages at
those companies.
Once the package was generally available on the Internet, it began
to be used by a much broader community.
The package used by exploration geophysicists,
earthquake seismologists, environmental engineers, software developers
and others. It is used by scientific staff in both small geotechnical
companies and major oil and gas companies, and by academics and government
researchers, both as a seismic data processing and software development
environment.

\section{What SU is}

The SU package is {\em free software}, meaning that you may have
unrestricted use of the codes for both processing and software development,
provided that you honor the license that appears at the beginning of 
this manual.
The package is maintained and expanded periodically, with
each new release appearing at 3 to 6 month intervals, depending
on changes that accumulate in the official version here at CWP.
The package is distributed with the full source code, so that users
can alter and extend its capabilities.  The philosophy behind the package
is to provide both a free processing and development environment
in a proven structure that can be maintained and expanded to suit
the needs of a variety of users.

The package is not necessarily restricted to seismic processing tasks,
however.  SU is intended as an extension of the Unix operating system,
and therefore shares many characteristics of the Unix, including  Unix
flexibility and expandibility. 
The fundamental Unix philosophy is that all operating system commands
are programs run under that operating system.
The idea is that individual tasks be identified, and that small programs
be written to do those tasks, and those tasks alone.
The commands may have options that permit variations on those tasks,
but the fundamental idea is one-program, one-task.
Because Unix is a multi-tasking operating system, multiple processes
may be strung together in a cascade via ``pipes'' ($|$).

This decentralization has the advantage of minimizing overhead
by not launching single ``monster'' applications that try to do
everything, as is seen in Microsoft operating systems, or
in some commercial seismic applications, for example.

Unix has the added feature of supporting a variety of shell languages,
making Unix, itself, a language. Seismic Unix benefits from all
of these attributes as well. In combination with standard Unix
programs, Seismic Unix programs may be used in shell scripts to
extend the functionality of the package.

Of course, it may be that no Unix or Seismic Unix program will fulfil
a specific task. 
This means that new code has to be written.
The availability of a standard set of source code, designed
to be readable by human beings, can expedite the process of extending
the package through the addition of new source code.

\section{What SU is not}

The SU package is not a graphical user interface driven utility
at this time, though there are several possibilities including Java
and TCL/TK scripts, which may be exploited for this purpose in the future.
Because most commmercial seismic processing packages are GUI-based,
it is unavoidable that users will expect SU to be similar.
However, it is not a fair comparision, for several reasons. 

As mentioned above, SU is an extension of the Unix operating system.
Just as there are no GUI driven Unix operating systems that give full
access to all of the capabilities of Unix from menus, it is not 
reasonable to expect full access to Seismic Unix through such an 
interface, if and when one is developed.   At most, any SU interface
will give limited access to the capabilities of the package.

SU is not a replacement for commercial seismic packages. Commercial
seismic software packages are industrial-strength utilities designed
for the express purpose of production-level seismic processing.
If you do commercial level processing, and have dedicated, or plan
to dedicate funds to purchasing one or more license of such commercial
software, then it is unlikely that you will be able to substitute
SU for the commercial utility.

However, SU can be an important adjunct to the commercial package
you use, however. Where commercial packages are used for 
production work, SU often has found a place as a prototyping package. 
Also, if new code needs to be written, SU can provide a starting base,
new software applications.

Another thing that SU is not, at least in its current version,
is a 3D package. However, it is not, expressly a 2D package, either,
as there are numerous filtering and trace manipulation tasks that
are the same in 2D as in 3D. It is likely, however, that there will
be 3D applications in future releases of SU, though these may not be
the most efficient of codes.

\section{Obtaining and Installing SU}

Because the coding standards of SU stress portability, the
package will install on any system that is running some form
of the Unix operating system, and has a decent version of ``make''
and an ANSI C compiler.
The programs GNU Make and GCC may be substituted for these respective
programs on most systems.

New releases of SU are issued at 3 to 6 month intervals, depending
upon the accumulation of changes in the home version at CWP.
Old releases are not supported. However, if materials appear to break
between releases, please contact me (John Stockwell) by email
so that we can fix the problem.

Instructions for obtaining and installing SU may be found in
Appendix~\ref{app:A} of this manual.

\chapter{Help facilities}

Like the Unix operating system, Seismic Unix may be thought of
as a language. As with any language, a certain amount of
vocabulary must be mastered before useful operations may be
performed.
Because the SU contains many programs,
there must be a ``dictionary'' to permit the inevitable
questions about vocabulary can be answered.

SU does not have ``man pages,'' in the same way that
Unix does, but it does have equivalent internal documentation
mechanisms.
There is a general level help utilities which give an
overview of what is available.
For information about specific aspects of a particular code,
the majority of the programs contain a ``selfdoc''---a
selfdocumentation paragraph, which will appear when the name
of the program is typed on the commandline, with no options.

(In all of the examples that follow, the percent sign \%
indicates a Unix commandline prompt, and is not typed
as part of the command.)

The following tools provide internal documentation at various
levels of detail for the main programs, shell scripts, and
library functions in the package:

\begin{itemize}
\item SUHELP - list the CWP/SU programs and shells
\item SUNAME - get name line from self-docs 
\item The selfdoc is an internal documentation utility which exists
in the majority of executable mains and shell scripts. The selfdoc
is seen by typing the name of the program or shell script on the commandline
with no arguments, and without redirection of input or output via
pipes $|$ or Unix redirects $>$ $<$,
For nonexecutables (library routines) and for programs without the
selfdoc feature, there is a dummy selfdoc included which provides a
database of information about those items, as well,
\item SUDOC - get DOC listing for code 
\item SUFIND - get info from self-docs 
\item GENDOCS - generate complete list of selfdocs in latex form 
\item suhelp.html - is an HTML global overview of SU programs by subject
matter,
\item SUKEYWORD -- guide to SU keywords in segy.h 
\end{itemize}

This chapter discusses each of these utilities, with the intent
of showing the reader how to get from the most general to the
most specific information about SU programs.


\section{Suhelp} 

For a general listing of the contents of SU, which includes each
executable (that is, each main program and shell script) in the package
type:

{\small\begin{verbatim}
% suhelp

CWP PROGRAMS: (no self-documentation)
ctrlstrip       fcat            maxints         t  
downfort        isatty          pause           upfort  

PAR PROGRAMS: (programs with self-documentation)
a2b             kaperture       resamp          transp          vtlvz  
b2a             makevel         smooth2         unif2           wkbj  
farith          mkparfile       smoothint2      unisam  
ftnstrip        prplot          subset          unisam2  
h2b             recast          swapbytes       velconv  


press return key to continue
....
\end{verbatim}}\noindent
Please type ``suhelp'' or see Appendix~{\ref{app:B} for the full text.

Another useful command sequence is:
\begin{verbatim}
% suhelp | lpr
\end{verbatim} \noindent
which will the output from `suhelp' to the local print.


\section{Suname}

{\small\begin{verbatim}
 -----  CWP Free Programs -----   
CWPROOT=/usr/local/cwp

Mains: 

In CWPROOT/src/cwp/main:
* CTRLSTRIP - Strip non-graphic characters
* DOWNFORT - change Fortran programs to lower case, preserving strings
* FCAT - fast cat with 1 read per file 
* ISATTY - pass on return from isatty(2)
* MAXINTS - Compute maximum and minimum sizes for integer types 
* PAUSE - prompt and wait for user signal to continue
* T - time and date for non-military types
* UPFORT - change Fortran programs to upper case, preserving strings

In CWPROOT/src/par/main:
A2B - convert ascii floats to binary 				
B2A - convert binary floats to ascii				
DZDV - determine depth derivative with respect to the velocity	",  
FARITH - File ARITHmetic -- perform simple arithmetic with binary files
FTNSTRIP - convert a file of floats plus record delimiters created 	
H2B - convert 8 bit hexidecimal floats to binary		
KAPERTURE - generate the k domain of a line scatterer for a seismic array
MAKEVEL - MAKE a VELocity function v(x,y,z)				
MKPARFILE - convert ascii to par file format 				
PRPLOT - PRinter PLOT of 1-D arrays f(x1) from a 2-D function f(x1,x2)
RAYT2D - traveltime Tables calculated by 2D paraxial RAY tracing	
RECAST - RECAST data type (convert from one data type to another)	
REGRID3 - REwrite a [ni3][ni2][ni1] GRID to a [no3][no2][no1] 3-D grid
RESAMP - RESAMPle the 1st dimension of a 2-dimensional function f(x1,x2)
...
\end{verbatim}}\noindent

Please type ``suhelp'' or see Appendix~{\ref{app:B} for the full text.


\section{The Selfdoc}

There are no Unix man pages for SU. To some people that seems
to be a surprise (even a disappointment) as this would seem to be
a standard Unix feature, which Seismic Unix should emulate.
The package does contain an equivalent mechanism called a 
``selfdoc," or self documentation.

This is a paragraph that is written into every program, and set
up so that if the name of the program is typed on the commandline,
with no options or redirects to or from files, the paragraph
is printed to standard error (the screen).
For example:

{\small\begin{verbatim}
% sustack
                                                                
 SUSTACK - stack adjacent traces having the same key header word
                                                                
     sustack <input >output [Optional parameters]
                                                                
 Required parameters:                                           
        none                                                    
                                                                
 Optional parameters:                                           
        key=cdp         header key word to stack on             
        normpow=1.0     each sample is divided by the           
                        normpow'th number of non-zero values    
                        stacked (normpow=0 selects no division) 
        verbose=0       verbose = 1 echos information           
                                                                
 Note:  The offset field is set to zero on the output traces.   
        Sushw can be used afterwards if this is not acceptable. 
\end{verbatim}}\noindent
The first line indicates the name of the program (SUSTACK) and
a short description of what the program does. 
This is the same line that appears for the listing of  sustack
in the suname listing.
The second line
\begin{verbatim}
     sustack <stdin >stdin [Optional parameters]
\end{verbatim}\noindent
indicates how the program is to be typed on the commandline, with
the words ``stdin'' and ``stdout'' indicating that the input
is from the standard input and standard output, respectively.
What this means in Unix terms is that the user could be inputting
and outputting data via diskfiles or the Unix ``redirect in'' $<$ 
and ``redirect out'' $>$ symbols, or via pipes $|$.

The paragraphs labeled by ``Required parameters:'' and ``Optional parameters''
indicate the commandline parameters which are required for the operation
of the program, and those which are optional. The default values of
the Optional parameters are given via the equality  are the values that the program assumes
for these parameters when data are supplied,  with no additional commandline
arguments given. For example: ``key=cdp'' indicates that sustack will
stack over ``cdp.'' (The shell script ``sukeyword'' tells what keywords
are expected in programs that have a ``key='' option.) 

\section{Sudoc}
As has been alluded to in previous sections of this manual, there
is a database of selfdocumentation items that is available for each
main program, shell script, and library function.
This database exists in the directory \$CWPROOT/src/doc  and is
composed of all of the selfdocumentation paragraphs of all of the
items in SU.

Because not all all items with selfdocs are executable, an additional
mechanism is necessary to see the ``selfdoc'' for these items.
For example, information about the Abel transform routines, located in
\$CWPROOT/src/cwp/lib/abel.c (on the system at CWP,
 CWPROOT=/usr/local/cwp) is obtained via

{\small\begin{verbatim}
% sudoc abel

In /usr/local/cwp/src/cwp/lib: 
ABEL - Functions to compute the discrete ABEL transform:

abelalloc	allocate and return a pointer to an Abel transformer
abelfree 	free an Abel transformer
abel		compute the Abel transform

Function prototypes:
void *abelalloc (int n);
void abelfree (void *at);
void abel (void *at, float f[], float g[]);

Input:
ns		number of samples in the data to be transformed
f[]		array of floats, the function being transformed

Output:
at		pointer to Abel transformer returned by abelalloc(int n)
g[]		array of floats, the transformed data returned by 
		abel(*at,f[],g[])

Notes:
The Abel transform is defined by:

	         Infinity
	g(y) = 2 Integral dx f(x)/sqrt(1-(y/x)^2)
		   |y|

Linear interpolation is used to define the continuous function f(x)
corresponding to the samples in f[].  The first sample f[0] corresponds
to f(x=0) and the sampling interval is assumed to be 1.  Therefore, the
input samples correspond to 0 <= x <= n-1.  Samples of f(x) for x > n-1
are assumed to be zero.  These conventions imply that 

	g[0] = f[0] + 2*f[1] + 2*f[2] + ... + 2*f[n-1]

References:
Hansen, E. W., 1985, Fast Hankel transform algorithm:  IEEE Trans. on
Acoustics, Speech and Signal Processing, v. ASSP-33, n. 3, p. 666-671.
(Beware of several errors in the equations in this paper!)

Authors:  Dave Hale and Lydia Deng, Colorado School of Mines, 06/01/90
\end{verbatim}}\noindent
shows information about the routines, including their names, usage
information (via the function prototype), some theory of how the
items are used, published references, and finally the author's names.

As an another example, type:

{\small \begin{verbatim}
% sugabor
                                                                        
 SUGABOR -  Outputs a time-frequency representation of seismic data via
                the Gabor transform-like multifilter analysis technique 
                presented by Dziewonski, Bloch and  Landisman, 1969.    
                                                                        
    sugabor <stdin >stdout [optional parameters]                        
                                                                        
 Required parameters:                                                   
        if dt is not set in header, then dt is mandatory                
                                                                        
 Optional parameters:                                                   
        dt=(from header)        time sampling interval (sec)            
        fmin=0                  minimum frequency of filter array (hz)  
        fmax=NYQUIST            maximum frequency of filter array (hz)  
        beta=3.0                ln[filter peak amp/filter endpoint amp] 
        band=.05*NYQUIST        filter bandwidth (hz)                   
        alpha=beta/band^2       filter width parameter                  
        verbose=0               =1 supply additional info               
                                                                        
 Notes: This program produces a muiltifilter (as opposed to moving window)
 representation of the instantaneous amplitude of seismic data in the   
 time-frequency domain. (With Gaussian filters, moving window and multi-
 filter analysis can be shown to be equivalent.)                        
                                                                        
 An input trace is passed through a collection of Gaussian filters      
 to produce a collection of traces, each representing a discrete frequency
 range in the input data. For each of these narrow bandwidth traces, a 
 quadrature trace is computed via the Hilbert transform. Treating the narrow
 bandwidth trace and its quadrature trace as the real and imaginary parts
 of a "complex" trace permits the "instantaneous" amplitude of each
 narrow bandwidth trace to be compute. The output is thus a representation
 of instantaneous amplitude as a function of time and frequency.        
                                                                        
 Some experimentation with the "band" parameter may necessary to produce
 the desired time-frequency resolution. A good rule of thumb is to run 
 sugabor with the default value for band and view the image. If band is
 too big, then the t-f plot will consist of stripes parallel to the frequency
 axis. Conversely, if band is too small, then the stripes will be parallel
 to the time axis.                                                      
                                                                        
 Examples:                                                              
    suvibro | sugabor | suximage                                        
    suvibro | sugabor | suxmovie n1= n2= n3=                            
     (because suxmovie scales it's amplitudes off of the first panel,  
     may have to experiment with the wclip and bclip parameters        
    suvibro | sugabor | supsimage | ... ( your local PostScript utility)

\end{verbatim}} \noindent

If you compare this output to the output from typing:

{\small \begin{verbatim}
% sudoc sugabor 
\end{verbatim}\noindent
you will see the same output as above, with the additional paragraphs

{\small \begin{verbatim}

 Credits:

	CWP: John Stockwell, Oct 1994

 Algorithm:

 This programs takes an input seismic trace and passes it
 through a collection of truncated Gaussian filters in the frequency
 domain.

 The bandwidth of each filter is given by the parameter "band". The
 decay of these filters is given by "alpha", and the number of filters
 is given by nfilt = (fmax - fmin)/band. The result, upon inverse
 Fourier transforming, is that nfilt traces are created, with each
 trace representing a different frequency band in the original data.

 For each of the resulting bandlimited traces, a quadrature (i.e. pi/2
 phase shifted) trace is computed via the Hilbert transform. The 
 bandlimited trace constitutes a "complex trace", with the bandlimited
 trace being the "real part" and the quadrature trace being the 
 "imaginary part".  The instantaneous amplitude of each bandlimited
 trace is then computed by computing the modulus of each complex trace.
 (See Taner, Koehler, and Sheriff, 1979, for a discussion of complex
 trace analysis.

 The final output for a given input trace is a map of instantaneous
 amplitude as a function of time and frequency.

 This is not a wavelet transform, but rather a redundant frame
 representation.

 References: 	Dziewonski, Bloch, and Landisman, 1969, A technique
		for the analysis of transient seismic signals,
		Bull. Seism. Soc. Am., 1969, vol. 59, no.1, pp.427-444.

		Taner, M., T., Koehler, F., and Sheriff, R., E., 1979,
		Complex seismic trace analysis, Geophysics, vol. 44,
		pp.1041-1063.

 		Chui, C., K.,1992, Introduction to Wavelets, Academic
		Press, New York.

 Trace header fields accessed: ns, dt, trid, ntr
 Trace header fields modified: tracl, tracr, d1, f2, d2, trid, ntr

\end{verbatim}}\noindent

There is more information in the sudoc listing,
than in the selfdoc listing. The selfdoc is intended as a quick reference,
whereas the sudoc listing can provide additional information
that we do not necessarily want to see everytime we are, say, simply wanting
to know what a particular parameter means, for example.
 
\section{Sufind}

The ``doc'' database may also be searched for specific 
strings, or topics.
For example,
{\small\begin{verbatim}
% sufind fft

 FFTLAB - Motif-X based graphical 1D Fourier Transform

 Usage:  fftlab


HANKEL - Functions to compute discrete Hankel transforms

hankelalloc     allocate and return a pointer to a Hankel transformer
hankelfree      free a Hankel transformer

PFAFFT - Functions to perform Prime Factor (PFA) FFT's, in place

npfa            return valid n for complex-to-complex PFA
npfar           return valid n for real-to-complex/complex-to-real PFA

 SUAMP - output amp, phase, real or imag trace from             
        (frequency, x) domain data                              

 suamp <stdin >stdout mode=amp                                  

 SUFFT - fft real time traces to complex frequency traces       

 suftt <stdin >sdout sign=1                                     


 SUFRAC -- take general (fractional) time derivative or integral of     
            data, plus a phase shift.  Input is TIME DOMAIN data.       

 sufrac power= [optional parameters] <indata >outdata                   

 SUIFFT - fft complex frequency traces to real time traces      

 suiftt <stdin >sdout sign=-1                                   


 SUMIGPS - MIGration by Phase Shift with turning rays                   

 sumigps <stdin >stdout [optional parms]                                


 SUMIGTK - MIGration via T-K domain method for common-midpoint stacked data

 sumigtk <stdin >stdout dxcdp= [optional parms]                 


 SURADON - forward generalized Radon transform from (x,t) -> (p,tau) space.

 suradon <stdin >stdout [Optional Parameters]                           


For more information type: "program_name <CR>"
\end{verbatim}}\noindent
The final line of this output ends with a symbol meant to indicate that the 
user is to type a carriage return.\footnote{The 
phrase ``carriage return'' refers to an older technology, the typewriter.
Ask your parents (or grandparents) for further details.}

\section{Gendocs}
The ultimate shell script for exploiting the sudoc database is
"gendocs". Typing:
\begin{verbatim}
% gendocs -o
\end{verbatim}\noindent
will generate the 528+ page document ``selfdocs.tex'', which is
in LaTeX format.
Obviously, you must {\em really\/} be sure that you want to print this
document, considering its size.
However, it does contain all of the selfdocumentations for all
CWP/SU programs, library routines, and shell scripts, and may be
a useful reference.

\section{Suhelp.html}
Long-time SU contributor, Dr. Christopher L. Liner of the University
of Tulsa, created the following  document which may be accessed
from the CWP/SU web site, or from his location of:

\begin{verbatim}
http://douze.utulsa.edu/~cll/suhelp/suhelp.html
\end{verbatim}

{\small\begin{verbatim}
                                SeismicUn*x

                          Version 31 (October 1997)

                            An HTML Help Browser

   * This is a help browser for the SeismicUn*x free software package
     developed and maintained by the Center for Wave Phenomena at the
     Colorado School of Mines. The SU project is directed at CWP by John
     Stockwell.
   * The author of this help facility is Dr. Christopher Liner (an alumnus
     of CWP) who is a faculty member in the Department of Geosciences at The
     University of Tulsa.
   * Last updated January 16, 1998



        o The arrangement below is by funtionality
        o Clicking on a program name pulls up the selfdoc for that item
        o Your web browser's Find capability is useful if you have a
          fragment in mind (e.g. sort or nmo)
        o While programs may logically apply to more than one catagory
          below, each program appears only once
     -----------------------------------------------------------------------

  1. Functional Listing
       1. Data Compression
       2. Editing, Sorting and Manipulation
       3. Filtering, Transforms and Attributes
       4. Gain, NMO, Stack and Standard Processes
       5. Graphics
       6. Import/Export
       7. Migration and Dip Moveout
       8. Simulation and Model Building
       9. Utilities

    * Alphabetical name list            * 258 items
...
\end{verbatim}}\noindent
To see the full listing see Appendix~{\ref{app:B} or point your
web browser to the http address above.

\section{Sukeyword}
Many of the SU programs that draw on header field information
have the parameter ``key='' listed in their selfdocs, with
the reference to ``keywords.''

The SU keywords are based on the SEGY trace header fields.
To find out what these header fields are, and what they stand
for in the SU data type, type:

{ \small\begin{verbatim} 
% sukeyword -o
\end{verbatim}}


{ \small \begin{verbatim}
typedef struct {	/* segy - trace identification header */

	int tracl;	/* trace sequence number within line */

	int tracr;	/* trace sequence number within reel */

	int fldr;	/* field record number */

	int tracf;	/* trace number within field record */

	int ep;	/* energy source point number */
....
\end{verbatim}}\noindent
Please type ``sukeyword -o '' or see Appendix~{\ref{app:B} for the full text.

To find an individual  keyword (for example ``cdp'') type:
{ \small\begin{verbatim} 
% sukeyword  cdp
\end{verbatim}}\noindent

{\small\begin{verbatim}
        int ep; /* energy source point number */

        int cdp;        /* CDP ensemble number */

        int cdpt;       /* trace number within CDP ensemble */

        short trid;     /* trace identification code:
                        1 = seismic data
                        2 = dead
                        3 = dummy
                        4 = time break
                        5 = uphole
                        6 = sweep
                        7 = timing
                        8 = water break
                        9---, N = optional use (N = 32,767)

                        Following are CWP id flags:

                         9 = autocorrelation

                        10 = Fourier transformed - no packing
                             xr[0],xi[0], ..., xr[N-1],xi[N-1]
\end{verbatim}}

\chapter{Using Seismic Unix}

There are common questions that arise with the use of 
any package.
As with any data processing package, there must be
ways of
\begin{itemize}
\item reading tapes,
\item data format conversion,
\item trace header setting and editing, 
\item manipulating the data,
\item processing the data,
\item making synthetic data,
\item viewing the output,
\item outputting the data to common media.
\end{itemize}

It is the intent of this chapter to deal with these issues.

\section{Reading Data from Tapes}

``Reading tapes is more of an art than a science.'' This is
true in general, and is especially true with SU.
The variability of hardware formats, as well as the variability
of data format types, makes the creation of a ``general tape
reading utility'' a challenging, if not impossible, task.

The following programs are useful for the specialized data input and output
tasks related to geophysical applications, as well as to the internal
SU data format
\begin{itemize}
\item BHEDTOPAR - convert a Binary tape HEaDer file to PAR file format
\item DT1TOSU - Convert ground-penetrating radar data in the 
Sensors \& Software X.dt1 GPR format to SU format
\item SEGDREAD - read an SEG-D tape 
\item SEGYCLEAN - zero out unassigned portion of header
\item SEGYREAD - read an SEG-Y tape 
\item SEGYHDRS - make SEG-Y ascii and binary headers for segywrite
\item SEGYWRITE - write an SEG-Y tape 
\item SETBHED - SET the fields in a SEGY Binary tape HEaDer file
\item SUADDHEAD - put headers on bare traces and set the tracl and ns fields 
\item SUPASTE - paste existing SEGY headers on existing data
\item SUSTRIP - remove the SEGY headers from the traces 
\end{itemize}

The following programs are useful for general data input, output,
and data type conversion, which may also find use in tape reading,

\begin{itemize}
\item A2B - convert ascii floats to binary
\item B2A - convert binary floats to ascii
\item FTNSTRIP - convert Fortran floats to C-style floats 
\item H2B - convert 8 bit hexidecimal floats to binary
\item RECAST - RECAST data type (convert from one data type to another)
\item TRANSP - TRANSPose an n1 by n2 element matrix 
\end{itemize}

\subsection{The SEGY format}
The data format that is expected by all programs in the CWP/SU package
whose names begin with the letters `su'  (with the exception of the
program `subset'), consists of ``SEGY traces written in the native
binary format of the machine you are running the programs on.''
To understand what this phrase means, we must understand what the
SEGY standard is.

In the early 1980's, the most common data storage format was SEG-Y.
This is the Society of Exploration Geophysicists Y format which is
described in the SEG's publication {\em Digital Tape Standards\/}.
The format is still widely used, today, though there is no
guarantee that the format is used ``by the book.''

The SEGY data format consists of 3 parts. The first part is a
3200 byte EBCDIC card image header which contains 40 cards
(i.e. 40 lines of text with 80 characters per line) worth of
text data describing the tape.
The second part is a 400 byte binary header containing information
about the contents of the tape reel. The third portion of 
the SEG-Y format consists of the actual seismic traces. Each trace
has a 240 byte {\em trace header\/}. The data follow, written in
one of 4 possible 32 formats in IBM floating point notation
as defined in IBM Form GA 22-6821. (Note, this ``IBM format'' is
not the common IEEE format found on modern IBM PC's.)

The SU data format is based on the trace portion of the SEGY format.
The primary difference between the SEGY traces and SU traces is that
the data portion of the SU format are floats, written in the native
binary float format of the machine you are running SU on.

\subsubsection{Getting SEG-Y data into SU}
The program ``segyread'' is used to convert data from the SEGY format
to the SU format.
If you type:
{\small\begin{verbatim}
% segyread
\end{verbatim} }  \noindent
You will see the selfdoc for this program.

When reading a SEGY tape, or datafile, you will need to be aware of
the byte-order (endian) of the machine you are running on.
The so-called ``big-endian'' or high-byte IEEE format is found on SGI,
SUN, IBM RS6000, and all Motorola chip-based systems. 
The ``little-endian'' or low-byte systems are systems that are based
on Intel and Dec chips.

You will also need to know what device your tape drive is.

A typical execution of segyread on a big-endian machine, looks like this:
{\small\begin{verbatim}
% segyread tape=/dev/rmt0 verbose=1 endian=1 > data.su
\end{verbatim}}\noindent

More often you will have to use the following
{\small\begin{verbatim}
% segyread tape=/dev/rmt0 verbose=1 endian=1 | segyclean > data.su
\end{verbatim}}\noindent

There are optional header fields (bytes 181-240) in the SEGY trace
headers. There is no standard for what may go in these fields, so
many people have items that they place in these fields. SU is
no exception. There are several parameters which are used by SU graphics
programs that may stored in these fields.
Segyclean zeros out the values of the optional header fields so that
SU graphics programs don't become confused by this information.

There are additional issues, such as whether or not your device
is buffered or unbufferd (i.e. 9 track 1/2 reel tape, or 8mm Exabyte)
tape which may have to be experimented with when you actually
try to read a tape.
Also, if you are trying to read a tape on a different system than
the one it was made on, you may simply not be able to read the tape.

The most common problem with reading tapes is matching the density
that the tape was written in, with the tapedrive that the tape is
being read on.
Some systems, for example Silicon Graphics (SGI) systems, have
many tape devices, which support different hardware configurations
and tape densities.
Other systems, most
notably recent versions of Linux have an improved version of the Unix
command ``mt'' which has a ``setdensities'' option.
In either case, it is common for tapes to be made using the default
settings of a tape drive, or its default densities. 

As a last resort in all tape reading situations, it is often possible to
use the Unix device-to-device copying program ``dd'' to make an image
of the entire tape on disk

{\small \begin{verbatim}
% dd if=/dev/rmtx of=filename bs=32767 conv=noerror
\end{verbatim}}\noindent
where ``/dev/rmtx'' is replaced with your tapedrive device and ``filename''
is some file name you choose.
If this works, then the next step is to try using segyread as above,
with ``tape=filename.''

If dd fails, then it is likely that the hardware format of your tapedrive
is not compatible with your tape.

\subsubsection{SEG-Y abuses}
Unfortunately, there are formats which are called ``SEGY'' but which
are not true to the SEG's standards for SEGY.
One common variation is to honor most of the SEGY convention, but
have the traces be in an IEEE format.
Such data would be read via:

{\small\begin{verbatim}
% segyread tape=/dev/rmt0 verbose=1 endian=1 conv=0 | segyclean > data.su
\end{verbatim}}\noindent
where the ``conv=0'' tells the program not to attempt the IBM to float
conversion.

There is also a ``DOS SEGY'' format which is similar to the previous
format, with the exception that the traces and headers are all written
in a little-endian format. On a big-endian machine,
the command to read such a dataset would be
{\small\begin{verbatim}
% segyread tape=/dev/rmt0 verbose=1 endian=0 conv=0 | segyclean > data.su
\end{verbatim}}\noindent
will read the data. Note, that endian=0 is set to swap the bytes.
(All of the bytes, header and data are in the swapped format.)
On a little-endian machine, the procedure is 
{\small\begin{verbatim}
% segyread tape=/dev/rmt0 verbose=1 endian=1 conv=0 | segyclean > data.su
\end{verbatim}}\noindent
with endian=1, in this case preventing byteswapping.

In each case, if we had a diskfile with some `filename', we would
use ``tape=filename.''
\subsection{Other SEG formats}
There are other SEG formats (SEG-A, SEG-B, SEG-X, SEG-C, SEG-D, SEG-1,
and SEG-2).
Of these, SEG-D, SEG-B, and SEG-2 are the types that you will most
commonly encounter. 
There is a ``segdread'' program which supports only 1 of the vast
number of variations on SEG-D.

In the directory \$CWPROOT/Third\_Party/   is a seg2segy conversion
program which may be used to convert SEG-2 to SEG-Y.

\subsection{Non-SEG tape formats}

Currently, there is only one non-SEG tape format that is completely supported
in the SU package, and two others which are supported through third-party
codes which have not been integrated into the package.
This is the Sensors \& Software DT1 format,
via ``dt1tosu,'' which is a GPR (ground penetrating radar) format.
In the \$CWPROOT/src/Third\_Party directory are two additional
non-SEG conversion programs, these are ``segytoseres'' and ``bison2su.''

\section{Data Format conversion}

Often, it is necessary to transfer data from other systems, or
to input data which may be in a variety of formats. A number
of tools and tricks are available in SU for dealing with these
issues.
The following programs may be useful for such conversion problems

\begin{itemize}
\item A2B - convert ascii floats to binary
\item B2A - convert binary floats to ascii
\item FTNSTRIP - convert Fortran floats to C-style floats 
\item H2B - convert 8 bit hexidecimal floats to binary
\item RECAST - RECAST data type (convert from one data type to another)
\item TRANSP - TRANSPose an n1 by n2 element matrix 
\item SUADDHEAD - put headers on bare traces and set the tracl and ns fields 
\item SUSTRIP - remove the SEGY headers from the traces 
\item SUPASTE - paste existing SEGY headers on existing data
\item SWAPBYTES - SWAP the BYTES of various  data types
\item SUSWAPBYTES - SWAP the BYTES in SU data to convert data from big endian
to little endian byte order, and vice versa
\end{itemize}

The purpose of this section is to discuss situations where these
programs may be used.
\subsection{A2B and B2A}

Of all of the formats of data, the most transportable (and most 
space consuming) is ASCII. No matter what system you are working
on, it is possible to transport ASCII data to and from that system.
Also, because text editors support ASCII, it is usually possible to
data entry and data editing in the simplest of text editors.

Such data probably come in a multicolumn format, separated either
by spaces or tabs. To convert such a, say 5 column, dataset into
binary floats, type:
{\small\begin{verbatim}
% a2b < data.ascii n1=5 > data.binary 
\end{verbatim}\noindent}
The reverse operation is
{\small\begin{verbatim}
% b2a < data.binary n1=5 > data.ascii 
\end{verbatim}}\noindent

\subsection{FTNSTRIP}
Often, because Fortran is a popular language in seismic data processing,
data will be obtained that was either created or processed in some
way with Fortran.
Binary data in Fortran are separated by beginning-of-record
and end-of-record delimeters. Binary data created by C programs do not have
any such delimiters.
To use Fortran data in a C program requires that the Fortran labels
be stripped off, via
{\small\begin{verbatim}
ftnstrip < fortdata > cdata
\end{verbatim}}\noindent

\subsubsection{Going from C to Fortran}

It may not be so easy to go the other way. On
the SGI Power Challenge, it is possible to read a file of C-floats
called ``infile'' via, open and read statments that look like:

{\small\begin{verbatim}
OPEN(99,file='infile',form='system') 

DO i=1,number
READ(99) tempnumber
Array(i)=tempnumber
END DO

CLOSE(99)
\end{verbatim}}\noindent

The statement "form='system'" does not work on all machines, however,
as it is likely that this is not standard Fortran.  The general format
command to read in binary is "form='unformatted'".  This may not work
on other systems, (for example, SUN). Indeed, it may not be generally
guaranteed that you can
read binary files in Fortran that have been created with a C-programs
(as in SU).

If you have problems with binary and the input files are not too
big you could convert to ASCII (using 'b2a') and use formatted I/0

{\small\begin{verbatim}
OPEN(99,file='infile')

DO i=1,number
READ(99,*) tempnumber
Array(i)=tempnumber
END DO

CLOSE(99)
\end{verbatim}}\noindent

\subsection{H2B}

The issue of converting 8 bit hexidecimal may seem to be one
that would not come up very often.
However 8 bit hex is a common format for bitmapped images
(grayscale PostScript) and if you wish to take a scanned image
and turn it into floats for further processing, then it will
come up.

If you have a scanned image, written as a 256 level grayscale
bitmapped PostScript image, then the bitmap portion is in  8 bit hex.
By removing all of the PostScript commands, and leaving only the
bitmap then the command
{\small\begin{verbatim}
h2b < hexdata > floatdata
\end{verbatim}}\noindent
will convert the bitmap into a form that can be viewed and processed
by programs in the CWP/SU package.

\subsection{RECAST}

Of course, C supports a variety of types, and instead of having
a bunch of program to convert each type into every other type,
there is a progam called ``recast'' that will do the job for a
large collection of these types. 

Types supported by recast for input and output:
\begin{itemize}
\item float - floating point
\item double - double precision
\item int - (signed) integer
\item char - character
\item uchar - unsigned char
\item short - short integer
\item long - long integer
\item ulong - unsigned long integer
\end{itemize}
For example, to convert integers to floats
{\small\begin{verbatim}
% recast < data.ints  in=int out=float > data.floats
\end{verbatim}}\noindent
The name of this program derives from the fact that an explicit
type conversion in the C-language is called a ``cast.''

\subsection{TRANSP}

In the course of any of the operations, it is often necessary
to transpose datasets. In particular, data which are represented
in a multi-column ASCII format, will likely need to be transposed
after being converted from ASCII to binary with ``a2b''.
The reason for this is that it is convenient to have the fast
dimension of the data be the time (or depth) dimension.

Our example above was a 5 column dataset, which you might think
of as  5 seismic traces, with the same number of samples in each
trace, side by side in the file. The processing
sequence
{\small\begin{verbatim}
% a2b < data.ascii n1=5 > data.binary 
\end{verbatim}\noindent}
will result in a file with the fast dimension being in the 
trace number direction, rather than the number of samples direction
The additional processing sequence
{\small\begin{verbatim}
% transp < data.binary n1=5 > data.transp
\end{verbatim}\noindent}
will put the data in the desirable form of the fast dimension being
in the number of samples direction, so that each trace is accessed
successively.

\subsection{SUADDHEAD}
Once data have been put in the correct form, that is to say, 
an array of C-style floats organized with the fast dimension in
the direction of increasing sample number per trace, then
it is necessary to add headers so that these data may be accessible
to other SU programs.

If the data are SEGY, SEGD, DT1, or Bison/Geometrics data, 
read from a tape or diskfile, then the programs segyread, segdread,
dt1tosu, or bison2su will have set the trace header values
so that the output will be ``SU data.''

For data read from some other means, such as an ASCII dataset, 
headers have to be added, this is done by using ``suaddhead.''
If our dataset consists of a file of binary C-style floats with,
say 1024 samples per trace, then the command sequence

{\small\begin{verbatim}
% suaddhead < data.bin ns=1024 > data.su
\end{verbatim}\noindent}
will yield the SU datafile ``data.su.''


\chapter{Coding in SU}
\section {A template SU program\label{SU:sec:template}}
Although variations are usually needed, a template for a typical {\small\sf SU} program
looks like the program listing below (we excerpted lines from the program {\tt sumute} to build this template).  The numbers in square brackets at the end of the lines in the listing are not part of the listing---we added them to facilitate discussion of the template.  The secret to efficient {\small\sf SU} coding is finding an existing program similar to the one you want to write.  If you have trouble locating the right code or codes to ``clone,'' ask us---this can be the toughest part of the job!
 
{\small\begin{verbatim}
/* SUMUTE: $Revision: 1.16 $ ; $Date: 1998/01/15 17:46:03 $      */  [1]

#include "su.h"                                                     [2]
#include "segy.h"

/*********************** self documentation **********************/ [3]
char *sdoc[] = {
"                                                                ",
" SUMUTE - ......                                                ",
"                                                                ",
" sumute <stdin >stdout                                          ",
"                                                                ",
" Required parameters:                                           ",
"         none                                                   ",
"                                                                ",
" Optional parameters:                                           ",
"        ...                                                     ",
"                                                                ",
" Trace header fields accessed: ns                               ",
" Trace header fields modified: none                             ",
"                                                                ",
NULL};
/**************** end self doc ***********************************/

/* Credits:
 *
 *        CWP: Jack Cohen, John Stockwell
 */


segy tr;                                                             [4]

main(int argc, char **argv)
{
        int ns;                /* number of samples          */      [5]
        ...


        /* Initialize */                 
        initargs(argc, argv);                                        [6]
        requestdoc(1);                                               [7]

        /* Get parameters */
        if (!getparint("ntaper", &ntaper))        ntaper = 0;        [8]

                                                
        /* Get info from first trace */
        if (!gettr(&tr)) err("can't read first trace");              [9]
        if (!tr.dt) err("dt header field must be set");              [10]

        /* Loop over traces */
        do {                                                         [11]
                int nt     = (int) tr.ns;                            [12]

                if (below == 0) {                                    [13]
                        nmute = NINT((t - tmin)/dt);
                        memset((void *) tr.data, (int) '\0', nmute*FSIZE);
                        for (i = 0; i < ntaper; ++i)
                                tr.data[i+nmute] *= taper[i];
                } else {
                        nmute = NINT((nt*dt - t)/dt);
                        memset((void *) (tr.data+nt-nmute),
                                        (int) '\0', nmute*FSIZE);
                        for (i = 0; i < ntaper; ++i)
                                tr.data[nt-nmute-1-i] *= taper[i];
                }
                puttr(&tr);                                           [14]
        } while (gettr(&tr));                                         [15]
        
        return EXIT_SUCCESS;                                          [16]
}
\end{verbatim}}\noindent
{\bf Discussion of numbered lines:}

\begin{enumerate}
\item We maintain the internal versions of the codes with the UNIX utility {\sf RCS}.  This item shows the string template for {\sf RCS}.
\item The file {\tt su.h} includes (directly or indirectly) all our locally defined macros and prototypes.  The file {\tt segy.h} has the definitions for the trace header fields.
\item The starred lines delimit the ``self-doc'' information---include them exactly as you find them in the codes since they are used by the automatic documentation shells.  The style of the self-doc shown is typical except that often additional usage information is shown at the bottom and, of course, often there are more options.  Look at some existing codes for ideas.
\item This is an external declaration of an {\small\sf SU} ({\sf SEG-Y}) trace buffer.  It is external to avoid wasting stack space.
\item We usually describe the global variables at the time of declaration.
Examine codes related to yours to increase consistency of nomenclature
(there is no official {\small\sf SU} naming standard).
\item The {\tt initargs} subroutine sets {\small\sf SU}'s command line passing facility (see page~\pageref{SU:page:getpar}).
\item The {\tt requestdoc} subroutine call specifies the circumstances under which self-doc will be echoed to the user.  The argument `1' applies to the  typical program that uses only standard input (i.e. \verb+<+) to read an {\small\sf SU} trace file.  Use `0' for codes that create synthetic data (like {\tt suplane}) and `2' for codes that require two input files (we could say ``et cetera,'' but there are no existing {\small\sf SU} mains that require {\em three} or more input files).
\item This is typical code for reading `parameters from the command line.  Interpret it like this: ``If the user did not specify a value, then use the default value.''  The subroutine must be type-specific, here we are getting an {\em integer} parameter.
\item Read the first trace, exit if empty.  The subroutine {\tt fgettr} ``knows about'' the {\small\sf SU} trace format.  Usually the trace file is read from standard input and then we use {\tt gettr} which is a macro based on {\tt fgettr} defined in {\tt su.h}.  Note that this code implies that the first trace is read into the trace buffer (here called {\tt tr}), therefore we will have to process this trace before the next call to {\tt fgettr}.
\item We've read that first trace because, we need to get some trace parameters from the first trace header.  Usually these are items like the number of samples ({\tt tr.ns}) and/or the sampling interval ({\tt tr.dt}) that, by the {\sf SEGY-Y} standard, are the same for all traces.
\item Since the first trace has been (typically) read before the main processing loop starts, we use a ``do-while'' that reads a new trace at the {\em bottom} of the loop.
\item We favor using {\em local} variables where permitted.
\item This is the seismic algorithm--here incomplete.  We've left in some of the actual {\tt sumute} code because it happens to contains lines that will be useful in the new code, we'll be writing below.  You may want to call a subroutine here to do the real work.
\item {\tt fputtr} and {\tt puttr} are the output analogs of {\tt fgettr} and {\tt gettr}.
\item The loop end.  {\tt gettr} returns a 0 when the trace file is exhausted and the processing then stops.
\item This is an {\sf ANSI-C} macro conventionally used to indicate successful program termination.
\end{enumerate}

\section{Writing a new program: {\tt suvlength}}

A user asked about {\small\sf SU} processing for variable length traces.  At his
institute, data are collected from time of excitation to a variable
termination time.  The difficulty is that {\small\sf SU} processing is based on
the {\sf SEG-Y} standard which mandates that all traces in the data set
be of the same length.  Rather than contemplating changing all of {\small\sf SU},
it seems to us that the solution is to provide a program that converts
the variable length data to fixed length data by padding with zeroes
where necessary at the end of the traces---let's name this new program
{\tt suvlength}.  We can make the length of the output traces a user
parameter.  If there is a reasonable choice, it makes sense to provide
a default value for parameters.  Here, using the length of the first
trace seems the best choice since that value can be ascertained before
the main processing loop starts.

So far, so good.  But now our plan runs into a serious snag: the
fundamental trace getting facility, {\tt gettr}, itself assumes fixed
length traces (or perhaps we should say that {\tt gettr} deliberately
enforces the fixed length trace standard).  But, if you think about
it, you'll realize that {\tt gettr} itself has to take special measures
with the {\em first} trace to figure out its length.  All we have to do
is make a new trace getting routine that employs that first trace
logic for {\em every} trace.  Here, we'll suppress the details of
writing the ``fvgettr'' subroutine and turn to converting the
template above into the new {\tt suvlength} code:

{\small\begin{verbatim}
/* SUVLENGTH: $Revision: 1.16 $ ; $Date: 1998/01/15 17:46:03 $   */

#include "su.h"
#include "segy.h"

/*********************** self documentation **********************/
char *sdoc[] = {
"                                                                ",
" SUVLENGTH - Adjust variable length traces to common length     ",
"                                                                ",
" suvlength <variable_length_traces >fixed_length_traces         ",
"                                                                ",
" Required parameters:                                           ",
"         none                                                   ",
"                                                                ",
" Optional parameters:                                           ",
"        ns      output number of samples (default: 1st trace ns)",
NULL};
/**************** end self doc ***********************************/

/* Credits:
 *        CWP: Jack Cohen, John Stockwell
 */

/* prototype */
int fvgettr(FILE *fp, segy *tp);

segy tr;

main(int argc, char **argv)
{
        int ns;        /* number of samples on output traces  */


        /* Initialize */                 
        initargs(argc, argv);
        requestdoc(1);
 
        /* Get parameters */
        ...
        
        /* Get info from first trace */
        ...

        ...

        return EXIT_SUCCESS;                                          [16]
}

/* fvgettr code goes here */
        ...

\end{verbatim}}\noindent
Now we run into a small difficulty.  Our only parameter has a default
value that is obtained only after we read in the first trace.  The
obvious solution is to reverse the parameter getting and the trace
getting in the template.  Thus we resume:
{\small\begin{verbatim}
        /* Get info from first trace and set ns */ 
        if (!fvgettr(stdin, &tr))  err("can't get first trace"); 
        if (!getparint("ns", &ns))    ns = tr.ns;

        /* Loop over the traces */
        do {
                int nt = tr.ns;
\end{verbatim}}\noindent
Now comes the actual seismic algorithm---which is rather trivial in
the present case:  add zeroes to the end of the input trace if the
output length is specified greater than the input length.  We could
write a simple loop to do the job, but the task is done most
succinctly by using the {\sf ANSI-C} routine {\tt memset}.  However, we
confess that unless we've used it recently, we usually forget how to
use this routine.  One solution is to {\tt cd} to the {\tt su/main}
directory and use {\tt grep} to find other uses of {\tt memset}.  When
we did this, we found that {\tt sumute} had usage closest to what we
needed and that is why we started from a copy of that code.  Here is
the complete main for {\tt suvlength}:
{\small\begin{verbatim}
/* SUVLENGTH: $Revision: 1.16 $ ; $Date: 1998/01/15 17:46:03 $        */

#include "su.h"
#include "segy.h"

/*********************** self documentation **********************/
char *sdoc[] = {
"                                                                 ",
" SUVLENGTH - Adjust variable length traces to common length      ",
"                                                                 ",
" suvlength <vdata >stdout                                        ",
"                                                                 ",
" Required parameters:                                            ",
"         none                                                    ",
"                                                                 ",
" Optional parameters:                                            ",
"          ns     output number of samples (default: 1st trace ns)",
NULL};
/**************** end self doc ***********************************/

/* Credits:
 *        CWP: Jack Cohen, John Stockwell
 *
 * Trace header fields accessed:  ns
 * Trace header fields modified:  ns
 */

/* prototype */
int fvgettr(FILE *fp, segy *tp);

segy tr;

main(int argc, char **argv)
{
        int ns;                /* samples on output traces        */


        /* Initialize */
        initargs(argc, argv);
        requestdoc(1);


        /* Get info from first trace */ 
        if (!fvgettr(stdin, &tr))  err("can't get first trace"); 
        if (!getparint("ns", &ns))    ns = tr.ns;


        /* Loop over the traces */
        do {
                int nt = tr.ns;
                                
                if (nt < ns) /* pad with zeros */
                        memset((void *)(tr.data + nt), '\0', (ns-nt)*FSIZE);
                tr.ns = ns;
                puttr(&tr);
        } while (fvgettr(stdin, &tr));
        
        return EXIT_SUCCESS;
}


#include "header.h"

/* fvgettr - get a segy trace from a file by file pointer (nt can vary)
 *
 * Returns:
 *        int: number of bytes read on current trace (0 after last trace)
 *
 * Synopsis:
 *        int fvgettr(FILE *fp, segy *tp)
 *
 * Credits:
 *        Cloned from .../su/lib/fgettr.c
 */

int fvgettr(FILE *fp, segy *tp)
   ...
\end{verbatim}}\noindent
{\bf Remark}: In the actual {\small\sf SU}, the subroutine {\tt fvgettr} has been
extracted as a library function and we also made a convenience macro
{\tt vgettr} for the case of standard input.  But these are secondary
considerations that don't arise for most applications.

For any new {\small\sf SU} code, one should provide an example shell program to show how
the new code is to be used.  Here is such a program for X Windows graphics:
{\small\begin{verbatim}
#! /bin/sh
# Trivial test of suvlength with X Windows graphics

WIDTH=700
HEIGHT=900
WIDTHOFF=50
HEIGHTOFF=20

>tempdata
>vdata
suplane >tempdata  # default is 32 traces with 64 samples per trace
suplane nt=72 >>tempdata
suvlength <tempdata ns=84 |
sushw key=tracl a=1 b=1 >vdata

# Plot the data 
suxwigb <vdata \
        perc=99 title="suvlength test"\
        label1="Time (sec)" label2="Traces" \
        wbox=$WIDTH hbox=$HEIGHT xbox=$WIDTHOFF ybox=$HEIGHTOFF &

# Remove #comment sign on next line to test the header
#sugethw <vdata tracl ns | more
\end{verbatim}}\noindent

\section{SU and UNIX}
You need not learn a special seismic language to use
{\small\sf SU}.  If you know how
to use UNIX shell-redirecting and pipes, you are ready to start
using {\small\sf SU}---the seismic commands and options can be used just as you
would use the built-in UNIX commands.  In particular, you
can write ordinary UNIX shell scripts to combine frequent
command combinations into meta-commands (i.e., processing flows).
These scripts can be thought of as ``job files.''

\begin{table}[htbp]
\label{SU:tab:unix}
\caption{UNIX Symbols}
\begin{tabular}{||l||l||}  \hline\hline
process1 $<$ file1 & process1 takes input from file1 \\
process2 $>$ file2 & process2 writes on (new) file2 \\
process3 $>>$ file3 & process3 appends to file3  \\
process4 $|$ process5 & output of process4 is input to process5  \\
process6 $<<$ text & take input from following lines  \\ \hline \hline
\end{tabular}
\end{table}

So let's begin with a capsule review of the basic UNIX operators
as summarized in Table~\ref{SU:tab:unix}.
The symbols $<$, $>$, and $>>$ are known as ``redirection operators,''
since they redirect input and output into or out of the command
(i.e., process).
The symbol $|$ is called a ``pipe,'' since we can picture
data flowing from one process to another through the ``pipe.''
Here is a simple {\small\sf SU} ``pipeline'' with input ``indata'' and
output ``outdata'':

{\small\begin{verbatim}
sufilter f=4,8,42,54 <indata |
sugain tpow=2.0 >outdata
\end{verbatim}}\noindent
This example shows a band-limiting operation being ``piped'' into
a gaining operation.  The input data set \verb:indata: is directed into
the program {\bf sufilter} with the \verb:<: operator, and similarly, the output data set \verb:outdata: receives the data because of the \verb:>: operator.
The output of {\bf sufilter} is connected to the input of {\bf sugain} by use of the \verb:|: operator.

\label{SU:page:getpar}The strings with the \verb:=: signs illustrate
how parameters are passed to {\small\sf SU} programs.  The program {\bf sugain}
receives the assigned value 2.0 to its parameter \verb:tpow:, while
the program {\bf sufilter} receives the assigned four component {\em vector}
to its parameter \verb:f:.  To find out what the valid parameters are
for a given program, we use the self-doc facility.

By the way, space around the UNIX
redirection and pipe symbols is optional---the example shows
one popular style.  On the other hand, spaces around the \verb:=:
operator are {\em not} permitted.

The first four symbols in
Table~\ref{SU:tab:unix} are the basic grammar of UNIX;
the final $<<$ entry
is the symbol for the less commonly used ``here document'' redirection.
Despite its rarity in interactive use,
{\small\sf SU} shell programs are significantly enhanced by
appropriate use of the $<<$ operator---we will illustrate this below.

Many built-in UNIX commands do not have a self-documentation
facility like {\small\sf SU}'s---instead, most do have ``man'' pages.
For example,

{\small\begin{verbatim}
% man cat

CAT(1)              UNIX Programmer's Manual               CAT(1)



NAME
     cat - catenate and print

SYNOPSIS
     cat [ -u ] [ -n ] [ -s ] [ -v ] file ...

DESCRIPTION
     Cat reads each file in sequence and displays it on the stan-
     dard output.  Thus

                    cat file

     displays the file on the standard output, and

                    cat file1 file2 >file3
--More--
\end{verbatim}}\noindent
You need to know a bit more UNIX lore
to use {\small\sf SU} efficiently---we'll introduce these tricks of the trade in
the context of the examples discussed later in this chapter.


\section{Exploring SU}
This section is a simulated example of an interactive session with {\small\sf SU}.
\subsection{Looking for DMO programs}
Later we will discuss the construction of {\small\sf SU} processing streams, our
present purpose is just to illustrate how to look around
for the raw materials for such streams.
Let's by using {\bf sufind} to see if there are any {\sf DMO} programs:
{\small\begin{verbatim}
% sufind dmo

 SUDMOFK - DMO via F-K domain (log-stretch) method for common-offset gathers

 sudmofk <stdin >stdout cdpmin= cdpmax= dxcdp= noffmix= [...]           


 SUDMOTX - DMO via T-X domain (Kirchhoff) method for common-offset gathers

 sudmotx <stdin >stdout cdpmin= cdpmax= dxcdp= noffmix= [optional parms]


 SUFDMOD2 - Finite-Difference MODeling (2nd order) for acoustic wave equation

 sufdmod2 <vfile >wfile nx= nz= tmax= xs= zs= [optional parameters]     


 SUSTOLT - Stolt migration for stacked data or common-offset gathers    

 sustolt <stdin >stdout cdpmin= cdpmax= dxcdp= noffmix= [...]           
\end{verbatim}}\noindent
The last two ``hits'' are spurious,
but we see that two {\sf DMO} programs have been found.
\subsection{Getting information about SU programs}
Use the self-doc facility to get more information about {\bf sudmofk}:
{\small\begin{verbatim}
% sudmofk
                                                                        
 SUDMOFK - DMO via F-K domain (log-stretch) method for common-offset gathers
                                                                        
 sudmofk <stdin >stdout cdpmin= cdpmax= dxcdp= noffmix= [...]           
                                                                        
 Required Parameters:                                                   
 cdpmin                  minimum cdp (integer number) for which to apply DMO
 cdpmax                  maximum cdp (integer number) for which to apply DMO
 dxcdp                   distance between adjacent cdp bins (m) 
 noffmix                 number of offsets to mix (see notes)           
                                                                        
 Optional Parameters:                                                   
 tdmo=0.0                times corresponding to rms velocities in vdmo (s)
 vdmo=1500.0             rms velocities corresponding to times in tdmo (m/s)
 sdmo=1.0                DMO stretch factor; try 0.6 for typical v(z)   
 fmax=0.5/dt             maximum frequency in input traces (Hz) 
 verbose=0               =1 for diagnostic print                        
                                                                        
 Notes:                                                         
 Input traces should be sorted into common-offset gathers.  One common- 
 offset gather ends and another begins when the offset field of the trace
 headers changes.                                                       
                                                                        
 The cdp field of the input trace headers must be the cdp bin NUMBER, NOT
 the cdp location expressed in units of meters or feet.         
                                                                        
 The number of offsets to mix (noffmix) should typically equal the ratio of
 the shotpoint spacing to the cdp spacing.  This choice ensures that every
 cdp will be represented in each offset mix.  Traces in each mix will   
 contribute through DMO to other traces in adjacent cdps within that mix.
                                                                        
 The tdmo and vdmo arrays specify a velocity function of time that is   
 used to implement a first-order correction for depth-variable velocity.
 The times in tdmo must be monotonically increasing.                    
                                                                        
 For each offset, the minimum time at which a non-zero sample exists is 
 used to determine a mute time.  Output samples for times earlier than this
 mute time will be zeroed.  Computation time may be significantly reduced
 if the input traces are zeroed (muted) for early times at large offsets.
                                                                        
 Trace header fields accessed:  ns, dt, delrt, offset, cdp.             
\end{verbatim}}\noindent

\subsection{Viewing header field definitions}
Note that the end of the last program description referred to
``header fields'';
these names are {\em not} standard and, as mentioned earlier, you can
get definitions by using {\bf sukeyword}.  For example,
{\small\begin{verbatim}
% sukeyword delrt

...skipping
                           may be positive or negative */

        short delrt;    /* delay recording time, time in ms between
                           initiation time of energy source and time
                           when recording of data samples begins
                           (for deep water work if recording does not
                           start at zero time) */

        short muts;     /* mute time--start */

        short mute;     /* mute time--end */

        unsigned short ns;      /* number of samples in this trace */

        unsigned short dt;      /* sample interval; in micro-seconds */

        short gain;     /* gain type of field instruments code:
                                1 = fixed
                                2 = binary
                                3 = floating point
                                4 ---- N = optional use */

--More--(53%)
\end{verbatim}}\noindent

\subsection{Viewing program names}
{\small\sf SU} program names are often obscure (we aren't proud of this).
Here's how to get help with remembering the exact name of a program
when you recall a fragment of the name:

{\small\begin{verbatim}
% sufind -n head

 SUADDHEAD - put headers on bare traces and set the tracl and ns fields
 UPDATEHEAD - update ../doc/Headers/Headers.all

For more information type: "program_name <CR>"
\end{verbatim}}\noindent
Recall also that {\bf suhelp} and {\bf suname} give comprehensive listings
of the {\small\sf SU} programs.

Note that we used the {\tt -n} option of the  {\bf sufind} command.  Using the self-doc facility, we can get the full story:
{\small\begin{verbatim}
% sufind

sufind - get info from self-docs about SU programs
Usage: sufind [-v -n] string
"sufind string" gives brief synopses
"sufind -v string" verbose hunt for relevant items
"sufind -n name_fragment" searches for command name
\end{verbatim}}\noindent

\section{Understanding and using SU shell programs}
The essence of good {\small\sf SU} usage is constructing (or cloning!)
UNIX shell programs to create and record processing flows.
In this section, we give some
annotated examples to get you started. 
\subsection{A simple SU processing flow example\label{SU:sec:Plotshell}}
Most {\small\sf SU} programs read from standard input and write to standard output.
Therefore, one can build complex processing flows by simply
connecting {\small\sf SU} programs with UNIX pipes.
Most flows will end with one of the {\small\sf SU} plotting programs.
Because typical processing flows are lengthy and involve many
parameter settings, it is convenient to put the {\small\sf SU} commands in a
shell file.

{\bf Remark}: All the UNIX shells, Bourne (sh), Cshell (csh),
Korn (ksh), \ldots, include a programming language.  In this document,
we exclusively use the Bourne shell programming language.

Our first example is a simple shell program called {\bf Plot}.
The numbers in square brackets at the
end of the lines in the following listing are not part of the
shell program---we added them as keys to the discussion
that follows the listing.

{\small\begin{verbatim}
#! /bin/sh                                              [1]
# Plot:   Plot a range of cmp gathers
# Author: Jane Doe
# Usage:  Plot cdpmin cdpmax

data=$HOME/data/cmgs                                    [2]

# Plot the cmp gather.
suwind <$data key=cdp min=$1 max=$2 |                   [3]
sugain tpow=2 gpow=.5 |
suximage f2=0 d2=1 \                                    [4]
        label1="Time (sec)" label2="Trace number" \
        title="CMP Gathers $1 to $2" \
        perc=99 grid1=solid &                           [5]
\end{verbatim}}\noindent
{\bf Discussion of numbered lines:}

\begin{enumerate}
\item The symbol \verb:#: is the comment symbol---anything on the remainder
of the line is not executed by the UNIX shell.  The combination
\verb:#!: is an exception to this rule: the shell uses the
file name following
this symbol as a path to the program that is to execute the remainder
of the shell program.

\item The author apparently intends that the shell be edited
if it is necessary to change the data set---she made this easier to
do by introducing the shell variable \verb:data: and assigning
to it the full pathname of the data file.  The assigned value
of this parameter is accessed as \verb:$data: within the shell program.
The parameter \verb:$HOME: appearing as the first component of the file
path name is a UNIX maintained environment variable
containing the path of the user's home directory.  In general,
there is no need for the data to be located in the user's home
directory, but the user would need ``read permission'' on the
data file for the shell program to succeed.

{\bf WARNING!}  Spaces are significant to the UNIX shell---it  uses
them to parse command lines.  So despite all we've learned about
making code easy to read, do {\em not} put spaces next to the \verb:=: symbol.
(Somewhere around 1977, one author's (Jack) first attempt to learn UNIX was
derailed for several weeks by making this mistake.)

\item The main pipeline of this shell code selects a certain set of cmp gathers with {\bf suwind}, gains this subset with {\bf sugain} and pipes the result into
the plotting program {\bf suximage}.  As indicated in the Usage comment,
the cmp range is specified by command line arguments.
Within the shell program, these arguments are
referenced as \verb:$1:, \verb:$2: (i.e., first argument, second argument).

\item The lines within the {\bf suximage} command are continued by the
backslash escape character.

\noindent{\bf WARNING!}  The line continuation backslash must be the {\em final}
character on the line---an invisible space or tab following the
backslash is one of the most common and frustrating bugs in UNIX
shell programming.

\item The final \verb:&: in the shell program
puts the plot window into ``background'' so we can continue
working in our main window.  This is the X-Windows
usage---the \verb:&: should {\em not} be used with the analogous PostScript
plotting programs (e.g., supsimage).  For example, with {\bf supsimage} in
place of {\bf suximage}, the \verb:&: might be replaced by \verb:| lpr:.

The {\small\sf SU} plotting programs are special---their self-doc doesn't
show all the parameters accepted.  For example, most of the parameters
accepted by {\bf suximage}
are actually specified in the self-documentation for the
generic {\small\sf CWP} plotting program {\bf ximage}.  This apparent flaw
in the self-documentation is actually a side
effect of a key {\small\sf SU} design decision.  The {\small\sf SU} graphics
programs call on the generic plotting programs to do the actual plotting.
The alternative design was to have tuned graphics programs
for various seismic applications.
Our design choice keeps things simple,
but it implies a basic limitation in {\small\sf SU}'s graphical capabilities.

The plotting programs are the vehicle for presenting your results.
Therefore you should take the time to carefully look
through the self-documentation for {\em both} the ``{\small\sf SU} jacket'' programs
({\bf suximage}, {\bf suxwigb}, \ldots) and the generic plotting
programs ({\bf ximage}, {\bf xwigb}, \ldots).

\end{enumerate}

\subsection{Executing shell programs}
The simplest way to execute a UNIX shell program is to give
it ``execute permission.''  For example, to make our above {\bf Plot} shell
program executable:
{\small\begin{verbatim}
chmod +x Plot
\end{verbatim}}\noindent
Then to execute the shell program:
{\small\begin{verbatim}
Plot 601 610
\end{verbatim}}\noindent
Here we assume that the parameters \verb:cdpmin=601:, \verb:cdpmax=610: are
appropriate values for the \verb:cmgs: data set.
Figure~\ref{fig:Plot} shows an output generated by the \verb:Plot: shell
program.
\begin{figure}[htbp]
\epsfysize 280pt
\centerline{\epsffile{Plot.eps}}
\caption{Output of the \protect\verb:Plot: shell program.}
\label{fig:Plot}
\end{figure}


\subsection{A typical SU processing flow\label{SU:sec:Dmoshell}}
Suppose you want to use {\bf sudmofk}.  You've read the self-doc, but
a detailed example is always welcome isn't it?  The place to look is
the directory {\bf su/examples}.  In this case, we are lucky and find
the shell program, {\bf Dmo}.  Again, the numbers in square brackets at the
end of the lines shown below are {\em not} part of the listing.
{\small\begin{verbatim}
#! /bin/sh
# dmo
set -x                                                            [1]

# set parameters
input=cdp201to800                                                 [2]
temp=dmocogs
output=dmocmgs
smute=1.7
vnmo=1500,1550,1700,2000,2300,2600,3000                           [3]
tnmo=0.00,0.40,1.00,2.00,3.00,4.00,6.00


# sort to common-offset, nmo, dmo, inverse-nmo, sort back to cmp
susort <$input offset cdp |                                       [4]
sunmo smute=$smute vnmo=$vnmo tnmo=$tnmo |                        [5]
sudmofk cdpmin=201 cdpmax=800 dxcdp=13.335 noffmix=4 verbose=1 |  [6]
sunmo invert=1 smute=$smute vnmo=$vnmo tnmo=$tnmo >$temp          [7]
susort <$temp cdp offset >$output                                 [8]
\end{verbatim}}\noindent
{\bf Discussion of numbered lines:}

The core of the shell program (lines 5-7) is recognized as the typical
dmo process: crude nmo, dmo, and then ``inverse'' nmo.
The dmo processing is surrounded by sorting operations
(lines 4 and 8).  Here is a detailed discussion of the shell program
keyed to the numbers appended to the listing (see also the discussion
above for the \verb:Plot: shell):

\begin{enumerate}
\item Set a debugging mode that asks UNIX
to echo the lines that are executed.  You can comment
this line off when its output is no longer of interest.  An
alternate debugging flag is \verb:set -v: which echos
lines as they are read by the shell interpreter.  You
can use both modes at once if you like.

\item This line and the next two lines set filenames that,
in this case, are in the same directory as the shell program itself.
Again, the reason for using parameters here is to make it easy
to ``clone'' the shell for use with other data sets.
Those of us who work with only a few data sets at any given time,
find it convenient to devote a directory to a given data set and
keep the shells used to process the data in that directory as
documentation of the processing parameters used.  ({\small\sf SU} does not have
a built-in ``history'' mechanism.)

\item The dmo process requires a set of velocity-time picks for
the subsidiary nmo processes.  Because these picks must be consistent
between the nmo and the inverse nmo, it is a good idea to make them
parameters to avoid editing mistakes.  Again, note the format
of {\small\sf SU} parameter vectors: comma-separated strings with no spaces.
The nmo program ({\bf sunmo}) will give an error message and abort
if the \verb:vnmo: and \verb:tnmo: vectors have different lengths.

\item Note that {\bf susort} allows the use of {\em secondary}
sort keys.  Do not assume that a secondary field that is
initially in the ``right'' order will remain in that order
after the sort---if you care about the order of some secondary
field, specify it (as this shell program does). In this line,
we sort the data according to increasing offsets and then, within
each offset, we sort according to increasing cdp number.

\item The forward nmo step.

\item The dmo step.

\item The inverse nmo step.

\item Sort back to cdp and have increasing offset within each cdp.
\end{enumerate}

If you want to thoroughly understand this shell program, your next
step is to study the self-docs of the programs involved:

{\small\begin{verbatim}
% sunmo

SUNMO - NMO for an arbitrary velocity function of time and CDP

sunmo <stdin >stdout [optional parameters]

Optional Parameters:
vnmo=2000         NMO velocities corresponding to times in tnmo
tnmo=0            NMO times corresponding to velocities in vnmo

...
\end{verbatim}}\noindent
Related shell programs are {\bf su/examples/Nmostack} and
{\bf su/examples/Mig}.

\section{Extending SU by shell programming}
Shell programming can be used to
greatly extend the reach of {\small\sf SU} without writing C code.
See, for example, {\bf CvStack}, {\bf FilterTest}, {\bf FirstBreak}, and
{\bf Velan} in {\bf su/examples}.

It is a sad fact that the UNIX shell is not
a high level programming language---consequently, effective shell
coding often involves arcane tricks.  In this section, we'll
provide some useful templates for some of the
common UNIX shell programming idioms.

We use {\bf CvStack} as an
illustration.  The core of this shell is a
double loop over velocities and cdps that produces
{\em velocity panels}---a concept
not contained in any single {\small\sf SU} program.

{\bf Remark}:  For most of us,
writing a shell like {\bf CvStack} from scratch is a time-consuming affair.
To cut down the development time,
your authors excerpt from existing shells to make new ones
even when we don't quite remember what every detail means.
We suggest that you do the same!

We won't comment on the lines already explained in our previous
two shell code examples
(see Sections~\ref{SU:sec:Plotshell} and~\ref{SU:sec:Dmoshell}),
but instead focus on the new features used in {\bf CvStack}.

{\small\begin{verbatim}
#! /bin/sh
# Constant-velocity stack of a range of cmp gathers
# Authors: Jack, Ken
# NOTE: Comment lines preceding user input start with  #!#
set -x

#!# Set input/output file names and data parameters
input=cdp601to610
stackdata=cvstack
cdpmin=601 cdpmax=610
fold=30
space=1         # 1 null trace between panels

#!# Determine velocity sampling.
vmin=1500   vmax=3000   dv=150

### Determine ns and dt from data (for sunull)
nt=`sugethw ns <$input | sed 1q | sed 's/.*ns=//'`                [1]
dt=`sugethw dt <$input | sed 1q | sed 's/.*dt=//'`

### Convert dt to seconds from header value in microseconds
dt=`bc -l <<END                                                   [2]
        scale=4
        $dt / 1000000
END`


### Do the velocity analyses.
>$stackdata  # zero output file                                   [3]
v=$vmin
while [ $v -le $vmax ]                                            [4]
do
        cdp=$cdpmin
        while [ $cdp -le $cdpmax ]                                [5]
        do
                suwind <$input \                                  [6]
                        key=cdp min=$cdp max=$cdp count=$fold |
                sunmo cdp=$cdp vnmo=$v tnmo=0.0 |
                sustack >>$stackdata
                cdp=`bc -l <<END                                  [7]                               
                        $cdp + 1
END`
        done
        sunull ntr=$space nt=$nt dt=$dt >>$stackdata              [8]
        v=`bc -l <<END
                $v + $dv
END`
done


### Plot the common velocity stacked data
ncdp=`bc -l <<END
        $cdpmax-$cdpmin+1
END`
f2=$vmin
d2=`bc -l <<END
        $dv/($ncdp + $space)                                      [9]
END`

sugain <$stackdata tpow=2.0 |

suximage perc=99 f2=$f2 d2=$d2 \
        title="File: $input  Constant-Velocity Stack " \
        label1="Time (s)"  label2="Velocity (m/s)" & 

exit                                                              [10]
\end{verbatim}}\noindent
{\bf Discussion of numbered lines:}

\begin{enumerate}
\item This elaborate construction gets some information
from the first trace header of the data set.  The program {\bf sugethw}
lists the values of the specified keys in the successive traces.  For
example,
{\small\begin{verbatim}
% suplane | sugethw tracl ns
 tracl=1            ns=64       

 tracl=2            ns=64       

 tracl=3            ns=64       

 tracl=4            ns=64       

 tracl=5            ns=64       

 tracl=6            ns=64    
   
 ...
\end{verbatim}}\noindent
Although {\bf sugethw} is eager to give the values for every trace in the
data set, we only need it once.  The solution is to use the UNIX stream
editor ({\bf sed}).  In fact, we use it twice.  By default, {\bf sed} passes
along its input to its output.  Our first use is merely to tell {\bf sed}
to quit after it puts the first line in the pipe.  The second pass through
{\bf sed} strips off the unwanted material before the integer.
In detail, the second {\bf sed} command reads: replace (or substitute)
everything up to the characters \verb:ns=: with nothing, i.e., delete
those characters.


\item We are proud of this trick.
The Bourne shell does not provide floating point
arithmetic.  Where this is needed, we use the UNIX built-in
{\bf bc} calculator program with the ``here document'' facility.
Here, we make the commonly needed conversion of sampling interval which
is given in micro-seconds in the {\sf SEG-Y} header,
but as seconds in {\small\sf SU} codes.  Note carefully the {\em back}quotes
around the entire calculation---we assign the result of this
calculation to the shell variable on the left of the equal sign,
here \verb:dt:.  The calculation may take several lines.
We first set the number of decimal places with \verb:scale=4:
and then do the conversion to seconds.  The characters \verb:END:
that follow the here document redirection symbol \verb:<<: are arbitrary,
the shell takes its input from the text in the shell file
until it comes to a line that contains the same
characters again.  For more information about {\bf bc}:
{\small\begin{verbatim}
% man bc
\end{verbatim}}\noindent

\item As the comment indicates, this is a special use of the output
redirection symbol that has the effect of destroying any pre-existing
file of the same name or opening a new file with that name.  In fact,
this is what \verb:>: always does as its first action---it's a dangerous
operator!  If you intend to {\em append}, then, as mentioned earlier, use
\verb:>>:.

\item This is the outer loop over velocities.
Another warning about spaces---the spaces around the bracket
symbols are essential.

{\bf Caveat}: The bracket notation is a nice
alternative to the older clunky \verb:test: notation:
{\small\begin{verbatim}
while test $v -le $vmax
\end{verbatim}}\noindent
Because the bracket notation is not documented on the typical {\bf sh} manual
page, we have some qualms about using it.  But, as far as we know,
all modern {\bf sh} commands support it---please let us know
if you find one that doesn't.

{\bf WARNING!}  OK, now you know that there is a UNIX command
called \verb:test:.  So don't use the name ``test'' for one of your
shell (or C) programs---depending on your \verb:$PATH: setting, you could
be faced with seemingly inexplicable output.

\item This is the inner loop over cdps.

\item Reminder: No spaces or tabs after the line continuation
symbol!

\item Notice that we broke the nice indentation structure by
putting the final \verb:END: against the left margin.  That's because
the {\bf sh} manual page says that the termination should contain
only the \verb:END: (or whatever you use).  In fact, most versions
support indentation.  We didn't think the added beautification was
worth the risk in a shell meant for export.  Also note that we used
{\bf bc} for an integer arithmetic calculation even though
integer arithmetic is built into the Bourne shell---why learn
two arcane rituals, when one will do?  See \verb:man expr:, if
you are curious.
\begin{figure}[htbp]
\epsfysize 300pt
\centerline{\epsffile{CvStack.eps}}
\caption{Output of the \protect\verb:CvStack: shell program.}
\label{fig:cvstack}
\end{figure}

\item {\bf sunull} is a program I (Jack) wrote to create all-zero traces
to enhance displays of the sort produced by \verb:CvStack:.
Actually, I had written this program many times, but this was the first
time I did it on purpose.  (Yes, that was an attempt at humor.)

\item An arcane calculation to get velocity labeling
on the trace axis.  Very impressive!  I wonder what it means?
(See last item.)

\item The \verb:exit: statement is useful because you might want
to save some ``spare parts'' for future use.  If so, just put them
after the \verb:exit: statement and they won't be executed.
\end{enumerate}

\noindent Figure~\ref{fig:cvstack} shows an output generated by \verb:CvStack:.

\section{Some core programs}

Reading the self-documentation and trying out the following {\small\sf SU} programs
will give you a good start in learning {\small\sf SU}. 

\subsection{Examining the trace headers}
\begin{description}
\item{\bf surange} --- print minimum and maximum values of trace header fields
\item{\bf sugethw} --- print values of selected header fields
\item{\bf suascii} --- print header and data values
\item{\bf suxedit} --- interactively examine headers and traces
\end{description}

\subsection{Some common processing programs}
\begin{description}
\item{\bf suacor} --- compute autocorrelations
\item{\bf sufilter} --- multipurpose zero phase filter (includes bandpass)
\item{\bf sugain} --- gain (with lots of options)
\item{\bf sumute} --- zero samples before a time that depends on offset
\item{\bf sunmo} --- normal-moveout correction
\item{\bf supef} --- prediction error filtering
\item{\bf susort} --- sort traces by values of trace header fields
\item{\bf sustack} --- stack (sum) traces
\item{\bf suvelan} --- velocity analysis
\item{\bf suwind} --- window (i.e., get a subset of) traces
\end{description}

\subsection{Some common plotting programs}
\begin{description}
\item{\bf suximage} --- gray scale X Windows plotting
\item{\bf suxwigb} --- bit mapped wiggle trace X Windows plotting
\item{\bf supsimage} --- gray scale PostScript plotting
\item{\bf supswigb} --- bit mapped wiggle trace PostScript plotting
\end{description}

\section{A brief tour of the source directories}
The {\small\sf SU} software is a layered product.  The layers correspond to the
following directories:
\begin{description}
\item{\bf cwp} Library of scientific routines (e.g. fft routines)
written in ``vanilla'' C. Utility mains and shells.
\item{\bf par} Library supporting the {\small\sf CWP} programming
style (i.e., self-doc, error reporting, parameter passing).
Mains that use (only) these facilities.  Shells for maintaining the
online documentation database.
\item{\bf su} Seismic processing codes that use the
{\sf SEG-Y} trace structure.    Subroutines that manage this
structure. Codes that buffer the generic graphics
routines listed below.  Shells that provide backward compatibility with
earlier releases.
\item{graphics libraries}
        \begin{enumerate}
        \item {\bf psplot}---PostScript graphics:
                \begin{enumerate}
                \item pscontour: contour plots
                \item pscube: 3D data cube
                \item psgraph: curve plotting
                \item psimage: raster plotting
                \item psmovie: supports frames
                \item pswigb: bit mapped wiggle traces (fast)
                \item pswigp: polygon wiggle traces (slow)
                \item PostScript support programs
                \end{enumerate}
        \item {\bf xplot}---xlib based X Windows graphics
                \begin{enumerate}
                \item ximage: raster plotting
                \item xwigb: bit mapped wiggle traces
                \item X Windows support programs
                \end{enumerate}
        \item \bf{Xtcwp}---toolkit based X Windows graphics
                \begin{enumerate}
                \item xgraph: curve plotting
                \item xmovie: supports frames
                \item X Windows resource files
                \end{enumerate}
        \end{enumerate}
\end{description}
These are only the highlights.  If you intend to add your
own C mains to the package, it is worthwhile
spending a few hours browsing through the source code.

\chapter{Frequently Asked Questions}

This chapter addresses questions often asked by new {\small\sf SU} users.
Some answers refer to the directory {\tt CWPROOT}.  We use this
symbolic name for the directory that contains the {\sf CWP/SU} source code, include files, libraries, and executables.  You are asked to specify
this directory name during the {\small\sf SU} installation procedure.

\section{Installation questions}
Complete information about the installation
process is found in the {\sf README}
files supplied with the distribution.
Here we discuss only some commonly found installation problems.

\begin{question}
I get error messages about missing {\tt fgetpos} and {\tt fsetpos} routines, 
even though I am using the {\sf GCC} compiler.
How do I get around this problem?
\end{question}

\begin{rmans}
We've seen this problem most often with older 
{\sf SUN OS} 4.xx (pre-{\sf SOLARIS}). 
These {\sf SUN} systems may not have the {\tt fgetpos} and {\tt fsetpos} subroutines defined.
Because these two routines are not currently used in the {\small\sf SU} package,
we have modified the installation process to permit the
user to define a compile-time flag to circumvent this problem.
Please uncomment the {\sf OPTC} line in the paragraph in Makefile.config
that looks like this:
\begin{verbatim}
# For SUN installing with GCC compiler but without GLIBC libraries
#OPTC = -O -DSUN_A -DSUN
\end{verbatim}
and do a "make remake".
\end{rmans}

\begin{question}
I get error messages regarding missing
{\tt strtoul}, and/or {\tt strerror} routines, even though I am using
the {\sf GCC} compiler. How do I get around this problem?
\end{question}

\begin{rmans}
Again, this is  most often seen with the older {\sf SUN OS}. 
The fix is the same as for the previous question.
\end{rmans}

\begin{question}
\label{SU:q:gcc}
Why do I get missing subroutine messages about {\sf ANSI C} routines?
Isn't the {\sf GCC} compiler supposed to be an {\sf ANSI} compiler?
\end{question}

\begin{rmans}
The {\sf GCC} compiler is just that, a compiler. It
draws on the libraries that are present on the machine.
If the {\sf GNU} libraries (this is the "glibc" package)
have not been installed, then the
{\sf GCC} compiler will use the libraries that are native to the machine
you are running on. Because the four routines listed above are
not available in the {\sf SUN 4. OS}, {\sf GCC} does not recognize them.
However, installing the {\sf GNU} libraries will make the {\sf GCC} compiler
behave as a full {\sf ANSI C} compiler.
\end{rmans}

\begin{question}
\label{SU:q:bugs}
Why do I get missing subroutine messages about {\sf ANSI C} routines?
I can't get the code to compile because my compiler can't find
"bzero" or "bcopy", how can  I fix this?
\end{question}

\begin{rmans}
You really shouldn't be having this problem, because
we try to keep to the ANSI standard, but sometimes old
style function calls creep in. The problem of rooting these things out
is exacerbated because many systems still support the old style calls.

If you have trouble installing
because your compiler can't find "bcopy" or "bzero"
make the following replacements.

Replace all statements of the form

{\small \begin{verbatim}
bzero( a, b);
\end{verbatim}} \noindent

with statements of the form:

{\small \begin{verbatim}
memset( (void *) a , (int) '\0', b );
\end{verbatim}} \noindent

Please replace all instances
of statements of the form of:
{\small \begin{verbatim}
bcopy ( a , b, c);
\end{verbatim}} \noindent
with a statements of the form:
{\small \begin{verbatim}
memcpy( (void *) b, (const void *) a, c );
\end{verbatim}} \noindent
\end{rmans}

\section{Data format questions}

In this section, we address questions about converting data
that are in various formats into {\small\sf SU} format.

\begin{question}
What is the data format that {\small\sf SU} programs expect?
\end{question}

\begin{rmans}
The {\small\sf SU} data format is based on the {\sf SEG-Y} format. The {\small\sf SU} format
consists of data traces each of which has a header.
The {\small\sf SU} trace header is identical to {\sf SEG-Y} trace header.
Both the header and the trace data are written in the
native binary format of your machine.

\noindent{\bf Caution}: The optional fields
in the {\sf SEG-Y} trace header are used for different purposes
at different sites.  {\small\sf SU} itself makes use of certain of these fields.
Thus, you may need to use {\tt segyclean}---see the answer to
Question~\ref{SU:q:segyclean}.
{\small\sf SU} format does not have the binary and ebcdic tape headers that
are part of the {\sf SEG-Y} format.

After installing the package, you can get more information on the
{\sf SEG-Y}/{\small\sf SU} header by typing: 
{\small \begin{verbatim}
% sukeyword -o
\end{verbatim}}\noindent
This lists the include file {\tt segy.h} that defines the {\small\sf SU} trace header.
\end{rmans}

\begin{question}
Is there any easy way of adding necessary 
{\sf SEG-Y} information to our own modeled data to prepare
our data for processing using the {\small\sf SU} package?
\end{question}

\begin{rmans}
It depends on the details of how your data was written to the file:
\begin{enumerate}
\item If you have a `datafile'
that is in the form of binary floating point numbers of the type
that would be created by a C program, then use {\tt suaddhead} to
put {\small\sf SU} ({\sf SEG-Y}) trace headers on the data. Example:
{\small \begin{verbatim}
% suaddhead < datafile  ns=N_SAMP > data.su
\end{verbatim}}\noindent
Here, \verb:N_SAMP: is the (integer) number of samples per
trace in the data.

\item If your data are Fortran-style floats, then you would use:
{\small \begin{verbatim}
% suaddhead < datafile ftn=1 ns=NS > data.su
\end{verbatim}}\noindent
See also, Question~\ref{SU:q:fortran}.

\item If your data are {\sf ASCII}, then use:
{\small \begin{verbatim}
% a2b n1=N1 < data.ascii | suaddhead ns=NS > data.su
\end{verbatim}}\noindent
Here \verb:N1: is the number of floats per line in the file
{\tt data.ascii}.

\item If you have some other data type, then you may use:
{\small \begin{verbatim}
% recast < data.other in=IN out=float | suaddhead ns=NS > data.su
\end{verbatim}}\noindent
where \verb:IN: is the type (int, double, char, etc...) 
\end{enumerate}

\noindent
For further information, consult the self-docs of the programs
{\tt suaddhead}, {\tt a2b}, and~{\tt recast}.
\end{rmans}

\begin{question}
\label{SU:q:segyclean}
I used {\tt segyread} to read a {\sf SEG-Y} tape.
Everything seems to work fine,
but when I plot my data with suximage, the window is black.
What did I do wrong?
\end{question}

\begin{rmans}
When you read an {\sf SEG-Y} tape, you need to pipe the data through
{\tt segyclean} to zero the optional {\sf SEG-Y} trace header field.
If the {\small\sf SU} programs see nonzero values in certain parts
of the optional field, they try
to display the data as ``nonseismic data,'' using those values
to set the plot parameters.

Another possibility is that there are a few data values that are so
large that they are overwhelming the 256 gray scale levels in the
graphics.
The way to get around this problem is to set {\bf perc=99} in the
graphics program. For example:
{\small \begin{verbatim}
% suximage < sudata  perc=99 &
\end{verbatim}} \noindent This will clip data values with size in
the top 1 percentile of the total data.
\end{rmans}

\begin{question}
I am trying to plot data with the {\tt pswigb}
(or {\tt pswigp}, or {\tt xwigb}, or  \ldots)
program.  I know that I have data with
\verb:n1=NSAMP: and \verb:n2=NTRACES:,
but when I plot, I find that I have to set \verb:n1=NSAMP+60: for the plot
to look even remotely correct. Why is this?
\end{question}

\begin{rmans}
It is likely that you are trying to plot with the wrong tool.
The input data format of the programs,
{\tt pswigb}, {\tt pswigp}, {\tt pscontour}, {\tt pscube}, {\tt psmovie},
{\tt xwigb}, {\tt xgraph}, and~{\tt xmovie},
expect data to consist of simple floating point numbers.
If your data are {\small\sf SU} data ({\sf SEG-Y}) traces,
then there is an additional
header at the beginning of each trace,
which, on most computer architectures,
is the same number (240) of bytes
as the storage for 60 floats.
 
\sloppypar{To plot these data, use respectively:
{\tt supswigb}, {\tt supswigp}, {\tt supscontour}, {\tt supscube},
{\tt supsmovie}, {\tt suxwigb}, {\tt suxgraph}, or~{\tt suxmovie}.}

Also, it is not necessary to specify the dimensions of the data for these
latter programs.  The {\tt su}-versions of the codes determine
the necessary information from the appropriate header values.
(In fact, that is {\em all} they do---the actual graphics is
handled by the version without the {\tt su} prefix.)
\end{rmans}

\begin{question}
I want to check the size of a file to see if it has the right number
of values, but I am not sure how to take the header into account.
How is this done?
\end{question}

\begin{rmans}
If the file consists of simple floating point numbers, then the
size in bytes equals the size of a float times the number of
samples (\verb:SIZE = 4 * N_SAMP:).
The {\small\sf SU} data ({\sf SEG-Y} traces)
also have a header (240 bytes per trace)
giving the total number of bytes as:\\
\verb:(240 + 4 N_SAMP ) N_TRACES:.\\
\noindent
The byte count computed in this way
is the number that the UNIX command {\tt ls -l} shows.

{\bf Caveats}: The above calculations assume that you have
the conventional architecture and that the header definition
in {\tt segy.h} has not been altered.  Watch out as machines
with 64 bit word size become common!
\end{rmans}

\begin{question}
\label{SU:q:fortran}
I have some data in Fortran form and tried to convert it to {\small\sf SU} data
via the following:
{\small \begin{verbatim}
% suaddhead < data.fortran ns=N_SAMP ftn=1 > data.su
\end{verbatim}}\noindent
but this did not work properly. I am sure that my fortran data
are in unformatted binary floats. What should I do?
\end{question}

\begin{rmans}
There are different ways of interpreting the term ``unformatted''
with regard to fortran data.  Try:
{\small \begin{verbatim}
% ftnstrip < data.fortran | suaddhead ns=N_SAMP > data.su
\end{verbatim}}\noindent

The program {\tt ftnstrip} can often succeed in converting
your fortran data into C-like binary data, even when the
\verb:ftn=1: option in {\tt suaddhead} fails.
\end{rmans} 

\begin{question}
I just successfully installed the {\sf CWP/SU} package, but when I
try to run the demo scripts, I get many error messages describing
programs that the shell script cannot find. How do I fix this?
\end{question}


\begin{rmans}
You need to put {\tt CWPROOT/bin} (where {\tt CWPROOT}
is {\tt /your/root/path} that
contains the {\sf CWP/SU} source code, include files,
libraries, and executables)
in your shell {\tt PATH}. This is done in your {\tt .cshrc} file
if you run under
{\tt csh} or {\tt tcsh}.
In Bourne shell ({\tt sh}), Born Again shell ({\tt bash}), or Korn shell
({\tt ksh}) the {\tt PATH} variable is in your {\tt .profile} file.
You also need
to type
{\small\begin{verbatim}
% rehash
\end{verbatim}}\noindent
if you are running C-shell {\tt /bin/csh} or  TC-shell {\tt /bin/tcsh}
as your working shell environment, if you have not relogged since 
you compiled the codes. 
\end{rmans} 

\begin{question}
How do I transfer data between {\small\sf SU} and a commercial package, such
as Promax.
\end{question}

\begin{rmans}
The short answer is that you make a SEGY tape on disk file.
To do convert a file called, say, "data.su" to a segy file
do the following:

{\small \begin{verbatim}
% segyhdrs < data.su
% segywrite tape=data.segy < data.su
\end{verbatim}} \noindent

Now use Promax to read data.segy. This file is a
"Promax tape-on-disk file in IBM Real format."
Choose Promax menus accordingly.

\noindent For other commercial packages, use the appropriate
commands to read a SEGY tape on disk file.

\noindent To go from the commercial package to {\small\sf SU} follow
the reverse steps. Create a file that is a SEGY tape image
and then use 
{\small \begin{verbatim}
% segyread tape=data.segy | segyclean > data.su
\end{verbatim}} \noindent
\end{rmans}
 
\begin{question}
I would like to strip the trace headers off of some SU data, perform
an operation of some type on the bare traces and put the headers
back on without losing any of the header information. How do I do this?
\end{question}

\begin{rmans}
Do the following:

{\small \begin{verbatim}
% sustrip < data.su head=headers > data.binary
\end{verbatim}} \noindent

(Do whatever was desired to data.binary to make data1.binary)

{\small \begin{verbatim}
% supaste < data1.binary head=headers > data1.su
\end{verbatim}} \noindent
\end{rmans}

\begin{question}
I have made some data on an IBM RS6000 and have transferred it to
my Linux-based PC system. The data looks ok on the RS6000,
but when I try to work with it on the PC, none of the SU programs seem
to work. What is wrong?
\end{question}

\begin{rmans}
The problem you have encountered is that there are two IEEE binary
formats called respectively `big endian` and `little endian` or,
alternately `high byte` and `low byte`. These terms refer to the
order of the bytes that represent the data. IBM RS6000, Silicon
Graphics, NeXT (black hardware), SUN, HP, PowerPC, any Motorola
chip-based platforms are `big endian` machines, whereas, Intel-based
PCs and Dec and Dec Alpha products are `little endian` platforms.

Two programs are supplied in the CWP/SU package for swapping
the bytes for data transfer. These are  {\bf swapbytes} and {\bf suswapbytes}.

The program {\bf swapbytes} is designed to permit the user to swap
the bytes on binary data that are all one type of data (floats, doubles,
shorts, unsigned shorts, longs, unsigned longs, and ints).

For data that are in the {\small\sf SU} format, the program {\bf suswapbytes} is
provided.

Furthermore, within the programs {\bf segyread} and {\bf segywrite}
there are ``swap='' flags that permit the user to specify whether
the platform they are working on are ``big endian'' or ``little endian''
platforms.

In older releases of {\small\sf SU} there were problems with the bitwise operations
that would be encountered in the wiggle-trace drawing routines. However,
these problems have been fixed via the ENDIANFLAG that appears in
Makefile.config.
\end{rmans}

\begin{question}
How do I convert data that are in the SEG-2 format to SEGY?
\end{question}

\begin{rmans}
In \$CWPROOT/src/Third\_Party/seg2segy   there are two programs
that have been made available to us by the University of Pau
in France, for this purpose. These should be easy to install
on any system where {\small\sf SU} has been installed.

Once you have converted   data.seg2  to  data.segy, you may
read it into the {\small\sf SU} format via:

{\small \begin{verbatim}
% segyread tape=data.segy > data.su
\end{verbatim}} \noindent
\end{rmans}

\section{Tape reading and writing}
This section contains frequently asked questions about reading
and writing {\sf SEG-Y} tapes with {\small\sf SU}.

\noindent Tape reading/writing is more of an art than a science.
Here are a few tips. 
\begin{enumerate}
\item Make sure your tape drive is set to be variable block
    length. If you are on an {\sf IBM RS6000}, this means you
    will need to use {\tt smit} to set {\tt blocksize=0} on your tape
    device. Having the tape drive set to some default
    constant blocksize (say blocksize=1024 or 512)
    will foil all attempts to read an {\sf SEG-Y} tape.
\item To read multiple tape files on a tape, use the non
     rewinding device. On an {\tt RS6000} this would be
      something like {\tt /dev/rmtx.1}, see {\tt man mt} for details.
\item If this still doesn't work, then try:
{\small \begin{verbatim}
% dd if=/dev/rmtx of=temps bs=32767 conv=noerror
\end{verbatim}}\noindent
Here, {\tt /dev/rmtx} (not the real name of the device,
it varies from system
to system) is your regular (rewinding) tape device.
In the option, {\tt bs=32767}, we gave the right blocksize ($2^{16}+1$)
for an {\tt IBM/RS6000}.  Try
\verb:bs=32765:  ($2^{16}-1$) on a {\sf SUN}. 
This will dump the entire contents of the tape onto
a single file.
\end{enumerate}


\begin{question}
How do I write multiple SEG-Y files onto a tape?
\end{question}


\begin{rmans}
Here is a shell script for writing multiple files on a tape:
{\small \begin{verbatim}
#! /bin/sh

DEV=/dev/nrxt0  # non rewinding tape device

mt -f $DEV rewind

j=0
jmax=40

while test "$j" -ne "$jmax"
do
        j=`expr $j + 1`
        echo "writing tape file  $j"
        segywrite tape=$DEV bfile=b.$j hfile=h.$j verbose=1 buff=0 < ozdata.$j
done

exit 0
\end{verbatim}}\noindent
\end{rmans}

\section{Geometry Setting}
\begin{question}
How do I do ``geometry setting'' in SU?
\end{question}
\begin{rmans}

There is a common seismic data manipulation task that often is 
called "geometry setting" in commercial packages in which the
user converts information in the survey observers' logs
into values in the trace headers.

\noindent The CWP/SU package does indeed, have provisions for getting and
setting header fields, as well as computing a third header field
from one or two other header fields. The programs that you need
to use for this are:


\vspace{1ex}
\indent sugethw    ("SU get header word") \\
\indent sushw      ("SU set header word") \\
\indent suchw      ("SU change or compute header word")
\vspace{1ex}

\noindent Type the name of each program to see the self
documentation of that code.

\vspace{1ex}
\noindent In addition, to find out what the header field "keywords"
mentioned in these programs are:  type:    sukeyword -o

\vspace{1ex}
\noindent You may have the information in a variety of forms.
The most common and least complicated assumptions of that form 
will be made here.

\vspace{1ex}
\noindent The task requires the following basic steps.

\vspace{1ex}
\begin{enumerate}
\item Get your data into SU format. The SU format is not exactly SEGY,
   but it does preserve the SEGY header information. If you are
   starting with SEGY data (either on tape, or on in the form of
   a diskfile) then you use "segyread" to read the data into an
   su file format.

   For tape:

{\small \begin{verbatim}
      % segyread tape=/dev/rmt0 bfile=data.1 header=h.1 | segyclean > data.su
\end{verbatim}}\noindent

   For diskfile

{\small \begin{verbatim}
      %  segyread tape=data.segy bfile=data.1 header=h.1 | segyclean > data.su
\end{verbatim}} \noindent
   The file   data.segy is assumed here to be a "tape image" of segy data.
   You have to be careful because some commercial software will write
   SEGY-like data, by mimicking the layout of the SEGY format, but 
   this format will not be in the true IBM tape format that SEGY is defined
   to be.  In Promax, if you write a SEGY file in IBM Real format, then this
   will be true SEGY tape image.
   working on.

\item If you have your data in the SU format, then you may view the
   ranges of the SEGY headers (headers that are not set will not
   be shown) via:
{\small \begin{verbatim}
   % surange < data.su
\end{verbatim}}

\item Data often comes with some fields already set. To dump these
   fields in a format that is convenient for geometry setting,
   you would use    sugethw  in the following way:

{\small \begin{verbatim}
   % sugethw < data.su  output=geom  key=key1,key2,... > hfile.ascii
\end{verbatim}}

   The strings "key1,key2,..." are the keywords representing the desired
   SEGY trace header fields. These keywords may be listed via:

{\small \begin{verbatim}
   % sukeyword -o
\end{verbatim}}

\item Once you have dumped the desired header fields  into  hfile.ascii
   then you may edit them with the editor of your choice. The point
   is that you may create a multi-column ascii file that lists the
   values of specific header fields (trace by trace, as they appear
   in data.su) by *any* method you wish. Each column will contain
   the value of a specific header field to be set.

\item Now that you have created the ascii file containing your header values,
   you may load these values into data.su via:

{\small \begin{verbatim}
   % a2b < hfile.ascii n1=N_columns > hfile.bin
\end{verbatim}} \noindent
   Here,  N\_columns is the number of columns in   hfile.ascii.
   This is to convert hfile.ascii to a binary file.

   Now use:
{\small \begin{verbatim}
   % sushw < data.su key=key1,key2,...  infile=hfile.bin > data1.su
\end{verbatim}} \noindent
   Here   key1,key2,... are the appropriate keywords representing
   the fields being set, listed in the exact order the values appear,
   column by column in hfile.ascii.

\item If you want to compute a third header field from two given header
   field values, then you may use: {\bf suchw} for this.
   Also, if the header fields that you want to set are
   systematic in some way (are constant for each trace or vary
   linearly across a gather), then you don't have to use the
   "infile=" option. You may simply give the
   necessary values to   sushw.   See the selfdocs for   sushw and
   suchw  for examples of these.
\end{enumerate}
\end{rmans}

\section{General}
This section addresses general questions about the {\small\sf SU} package.

\begin{question}
What are these funny words gelev, selev, fldr, etc. that I see
in various places?
\end{question}

\begin{rmans}
These are the "keywords" that are required for many of the codes.
They refer to SU (Segy) header fields.

\begin{verbatim}
   Type:   sukeyword -o                to see the whole list 
   Type:   sukeyword keyword           to see the listing for an individual
                                       keyword
\end{verbatim}
\end{rmans}

\begin{question}
What do the terms ``little endian'' and ``big endian'' and  mean?
\end{question}

\begin{rmans}
There are two IEEE binary formats, called respectively
'little endian' and 'big endian'. These are also called
'high byte' and 'low byte', respectively.
These refer to the byte order in the bitwise representation of
binary data. The following platforms are 'little endian': DEC and
Intel-based PC's. The other common platforms are "big endian":
IBM RS6000, Silicon Graphics, NeXT (black hardware), SUN,
HP, PowerPC, any Motorola chip-based platform.
\end{rmans}

\begin{question}
Why are {\sf CWP/SU} releases given by integers (22, 23, 24, etc...)
instead of the more familiar decimal release numbers (1.1, 1.3, etc...)?
\end{question}

\begin{rmans}
The {\sf CWP/SU} release numbers are chosen to correspond
to the {\tt SU NEWS} email messages.
The individual codes in the package have traditional decimal
release numbers (assigned by {\sf RCS}), but these are all different.
The package changes in incremental, but non-uniform ways, so the standard
notation seems inappropriate. However, the user may view 24 to be
2.4. We may adopt this convention in the future.

{\bf Remark}:  In the early days, we {\em did} use {\sf RCS} to
simultaneously update all the codes to 2.1, 3.1, \ldots .  This
practice died a natural death somewhere along the way.
\end{rmans}

\begin{question}
How often are the codes updated?
\end{question}

\begin{rmans}
The {\sf CWP/SU} package is updated at roughly 3-6 month intervals.
We mail announcements of these releases to all known users.  Since
we do not provide support for outdated versions,
we urge you to remain current.
\end{rmans}

\begin{question}
I have a complicated collection of input parameters for a {\sf CWP/SU}
program.  I want to run the command from the command line of a terminal
window, but I don't want to retype the entire string of input parameters.
What do I do?
\end{question}

\begin{rmans}
{\sf CWP/SU} programs that take their input parameters from the command
line also have the feature of being able to read from a
``parameter file.''   This is invoked by setting
the parameter \verb:par=parfile:, where {\tt parfile} is a file containing
the desired commandline string.

For example:
{\small \begin{verbatim}
suplane ntr=20 nt=40 dt=.001 | ...
\end{verbatim}}\noindent
is completely equivalent to the command:
{\small \begin{verbatim}
suplane par=parfile | ...
\end{verbatim}}\noindent
if the string
{\small \begin{verbatim}
ntr=20 nt=40 dt=.001
\end{verbatim}}\noindent
is contained  in `parfile.'
\end{rmans}

\begin{question}
I can't find an sudoc entry for the function "ints8r," yet the
SU manual says that all library functions have online documentation?
What am I doing wrong?
\end{question}

\begin{rmans}
The proper search procedure for a library function (such as ints8r) is:
{\small \begin{verbatim}
% sufind ints8r
\end{verbatim}} \noindent
Which yields:

\begin{verbatim}
INTSINC8 - Functions to interpolate uniformly-sampled data via 8-coeff. sinc
                approximations:

ints8c  interpolation of a uniformly-sampled complex function y(x) via an


For more information type: "sudoc program_name <CR>"
\end{verbatim}

The name INTSINC8 is the name of the file that contains the
library function ins8c. You may now use "sudoc" to find out more
information via:

{\small \begin{verbatim}
% sudoc intsinc8
\end{verbatim}} \noindent

Which yields:

\begin{verbatim}
In /usr/local/cwp/src/cwp/lib: 
INTSINC8 - Functions to interpolate uniformly-sampled data via 8-coeff. sinc
                approximations:

ints8c  interpolation of a uniformly-sampled complex function y(x) via an
         8-coefficient sinc approximation.
ints8r  Interpolation of a uniformly-sampled real function y(x) via a
                table of 8-coefficient sinc approximations

Function Prototypes:
void ints8c (int nxin, float dxin, float fxin, complex yin[], 
        complex yinl, complex yinr, int nxout, float xout[], complex yout[]);
void ints8r (int nxin, float dxin, float fxin, float yin[], 
        float yinl, float yinr, int nxout, float xout[], float yout[]);

Input:
nxin            number of x values at which y(x) is input
dxin            x sampling interval for input y(x)
fxin            x value of first sample input
yin             array[nxin] of input y(x) values:  yin[0] = y(fxin), etc.
yinl            value used to extrapolate yin values to left of yin[0]
yinr            value used to extrapolate yin values to right of yin[nxin-1]
nxout           number of x values a which y(x) is output
xout            array[nxout] of x values at which y(x) is output

Output:
yout            array[nxout] of output y(x):  yout[0] = y(xout[0]), etc.

Notes:
Because extrapolation of the input function y(x) is defined by the
left and right values yinl and yinr, the xout values are not restricted
to lie within the range of sample locations defined by nxin, dxin, and
fxin.

The maximum error for frequiencies less than 0.6 nyquist is less than
one percent.

Author:  Dave Hale, Colorado School of Mines, 06/02/89

\end{verbatim}
\end{rmans}

\begin{question}
I have written my own SU programs and would like them to appear 
in the "suname" and "sudoc" listings. How do I do this?
\end{question}

\begin{rmans}
Run {\bf updatedocall} (source code located in CWPROOT/par/shell).
If you have put this code under a new path, then you must add
this path to the list of paths in the updatedoc script.
For the selfdoc information to be captured by the updatedoc script,
you will need to have the following marker lines at the beginning
and end of the selfdoc and additional information portion of the 
source code of your program.
\begin{verbatim}
/*********************** self documentation **********************/
/**************** end self doc ********************************/
\end{verbatim}
Be sure to clone these directly out of an existing SU program, rather
than typing them yourself, so that the pattern is the exact one
expected by the updatedoc script.
\end{rmans}

\begin{question}
I have a gray scale (not color) PostScript file made with psimage
and would like to convert it to a color PostScript format, but do
not have the original binary data that I made the file from. How
do I do this?
\end{question}

\begin{rmans}
You have to restore the binary file to make the new color PostScript
file.  Here is how you do it. (Here, we are assuming a bit-mapped
graphic as would be produced by psimage or supsimage).
\begin{enumerate}
\item Make a backup of your PostScript file.
\item edit the PostScript file removing everything but the
    hexidecimal binary image that makes up the majority of
    the file.
\item use    h2b   to convert the hexidecimal file to binary
\item You will find that the file is flipped from the original
    input file.  Use   transp   to flip the data. Note that the
    n1 and n2 values that are used by transp are the dimensions
    of the input data, which are the reverse of the output data.
\item You now have a 0-255 representation of your binary data
    which you should be able to plot again any way you desire.
\end{enumerate}

This method may be used to convert scanned images to {\small\sf SU} format,
as well, with the next step in the procedure to be putting {\small\sf SU}
headers on the data with  {\bf suaddhead}.
\end{rmans}


\chapter{How to Write an SU Program}
\section {A template SU program\label{SU:sec:template}}
Although variations are usually needed, a template for a typical {\small\sf SU} program
looks like the program listing below (we excerpted lines from the program {\tt sumute} to build this template).  The numbers in square brackets at the end of the lines in the listing are not part of the listing---we added them to facilitate discussion of the template.  The secret to efficient {\small\sf SU} coding is finding an existing program similar to the one you want to write.  If you have trouble locating the right code or codes to ``clone,'' ask us---this can be the toughest part of the job!
 
{\small\begin{verbatim}
/* SUMUTE: $Revision: 1.16 $ ; $Date: 1998/01/15 17:46:03 $      */  [1]

#include "su.h"                                                     [2]
#include "segy.h"

/*********************** self documentation **********************/ [3]
char *sdoc[] = {
"                                                                ",
" SUMUTE - ......                                                ",
"                                                                ",
" sumute <stdin >stdout                                          ",
"                                                                ",
" Required parameters:                                           ",
"         none                                                   ",
"                                                                ",
" Optional parameters:                                           ",
"        ...                                                     ",
"                                                                ",
" Trace header fields accessed: ns                               ",
" Trace header fields modified: none                             ",
"                                                                ",
NULL};
/**************** end self doc ***********************************/

/* Credits:
 *
 *        CWP: Jack Cohen, John Stockwell
 */


segy tr;                                                             [4]

main(int argc, char **argv)
{
        int ns;                /* number of samples          */      [5]
        ...


        /* Initialize */                 
        initargs(argc, argv);                                        [6]
        requestdoc(1);                                               [7]

        /* Get parameters */
        if (!getparint("ntaper", &ntaper))        ntaper = 0;        [8]

                                                
        /* Get info from first trace */
        if (!gettr(&tr)) err("can't read first trace");              [9]
        if (!tr.dt) err("dt header field must be set");              [10]

        /* Loop over traces */
        do {                                                         [11]
                int nt     = (int) tr.ns;                            [12]

                if (below == 0) {                                    [13]
                        nmute = NINT((t - tmin)/dt);
                        memset((void *) tr.data, (int) '\0', nmute*FSIZE);
                        for (i = 0; i < ntaper; ++i)
                                tr.data[i+nmute] *= taper[i];
                } else {
                        nmute = NINT((nt*dt - t)/dt);
                        memset((void *) (tr.data+nt-nmute),
                                        (int) '\0', nmute*FSIZE);
                        for (i = 0; i < ntaper; ++i)
                                tr.data[nt-nmute-1-i] *= taper[i];
                }
                puttr(&tr);                                           [14]
        } while (gettr(&tr));                                         [15]
        
        return EXIT_SUCCESS;                                          [16]
}
\end{verbatim}}\noindent
{\bf Discussion of numbered lines:}

\begin{enumerate}
\item We maintain the internal versions of the codes with the UNIX utility {\sf RCS}.  This item shows the string template for {\sf RCS}.
\item The file {\tt su.h} includes (directly or indirectly) all our locally defined macros and prototypes.  The file {\tt segy.h} has the definitions for the trace header fields.
\item The starred lines delimit the ``self-doc'' information---include them exactly as you find them in the codes since they are used by the automatic documentation shells.  The style of the self-doc shown is typical except that often additional usage information is shown at the bottom and, of course, often there are more options.  Look at some existing codes for ideas.
\item This is an external declaration of an {\small\sf SU} ({\sf SEG-Y}) trace buffer.  It is external to avoid wasting stack space.
\item We usually describe the global variables at the time of declaration.
Examine codes related to yours to increase consistency of nomenclature
(there is no official {\small\sf SU} naming standard).
\item The {\tt initargs} subroutine sets {\small\sf SU}'s command line passing facility (see page~\pageref{SU:page:getpar}).
\item The {\tt requestdoc} subroutine call specifies the circumstances under which self-doc will be echoed to the user.  The argument `1' applies to the  typical program that uses only standard input (i.e. \verb+<+) to read an {\small\sf SU} trace file.  Use `0' for codes that create synthetic data (like {\tt suplane}) and `2' for codes that require two input files (we could say ``et cetera,'' but there are no existing {\small\sf SU} mains that require {\em three} or more input files).
\item This is typical code for reading `parameters from the command line.  Interpret it like this: ``If the user did not specify a value, then use the default value.''  The subroutine must be type-specific, here we are getting an {\em integer} parameter.
\item Read the first trace, exit if empty.  The subroutine {\tt fgettr} ``knows about'' the {\small\sf SU} trace format.  Usually the trace file is read from standard input and then we use {\tt gettr} which is a macro based on {\tt fgettr} defined in {\tt su.h}.  Note that this code implies that the first trace is read into the trace buffer (here called {\tt tr}), therefore we will have to process this trace before the next call to {\tt fgettr}.
\item We've read that first trace because, we need to get some trace parameters from the first trace header.  Usually these are items like the number of samples ({\tt tr.ns}) and/or the sampling interval ({\tt tr.dt}) that, by the {\sf SEGY-Y} standard, are the same for all traces.
\item Since the first trace has been (typically) read before the main processing loop starts, we use a ``do-while'' that reads a new trace at the {\em bottom} of the loop.
\item We favor using {\em local} variables where permitted.
\item This is the seismic algorithm--here incomplete.  We've left in some of the actual {\tt sumute} code because it happens to contains lines that will be useful in the new code, we'll be writing below.  You may want to call a subroutine here to do the real work.
\item {\tt fputtr} and {\tt puttr} are the output analogs of {\tt fgettr} and {\tt gettr}.
\item The loop end.  {\tt gettr} returns a 0 when the trace file is exhausted and the processing then stops.
\item This is an {\sf ANSI-C} macro conventionally used to indicate successful program termination.
\end{enumerate}

\section{Writing a new program: {\tt suvlength}}

A user asked about {\small\sf SU} processing for variable length traces.  At his
institute, data are collected from time of excitation to a variable
termination time.  The difficulty is that {\small\sf SU} processing is based on
the {\sf SEG-Y} standard which mandates that all traces in the data set
be of the same length.  Rather than contemplating changing all of {\small\sf SU},
it seems to us that the solution is to provide a program that converts
the variable length data to fixed length data by padding with zeroes
where necessary at the end of the traces---let's name this new program
{\tt suvlength}.  We can make the length of the output traces a user
parameter.  If there is a reasonable choice, it makes sense to provide
a default value for parameters.  Here, using the length of the first
trace seems the best choice since that value can be ascertained before
the main processing loop starts.

So far, so good.  But now our plan runs into a serious snag: the
fundamental trace getting facility, {\tt gettr}, itself assumes fixed
length traces (or perhaps we should say that {\tt gettr} deliberately
enforces the fixed length trace standard).  But, if you think about
it, you'll realize that {\tt gettr} itself has to take special measures
with the {\em first} trace to figure out its length.  All we have to do
is make a new trace getting routine that employs that first trace
logic for {\em every} trace.  Here, we'll suppress the details of
writing the ``fvgettr'' subroutine and turn to converting the
template above into the new {\tt suvlength} code:

{\small\begin{verbatim}
/* SUVLENGTH: $Revision: 1.16 $ ; $Date: 1998/01/15 17:46:03 $   */

#include "su.h"
#include "segy.h"

/*********************** self documentation **********************/
char *sdoc[] = {
"                                                                ",
" SUVLENGTH - Adjust variable length traces to common length     ",
"                                                                ",
" suvlength <variable_length_traces >fixed_length_traces         ",
"                                                                ",
" Required parameters:                                           ",
"         none                                                   ",
"                                                                ",
" Optional parameters:                                           ",
"        ns      output number of samples (default: 1st trace ns)",
NULL};
/**************** end self doc ***********************************/

/* Credits:
 *        CWP: Jack Cohen, John Stockwell
 */

/* prototype */
int fvgettr(FILE *fp, segy *tp);

segy tr;

main(int argc, char **argv)
{
        int ns;        /* number of samples on output traces  */


        /* Initialize */                 
        initargs(argc, argv);
        requestdoc(1);
 
        /* Get parameters */
        ...
        
        /* Get info from first trace */
        ...

        ...

        return EXIT_SUCCESS;                                          [16]
}

/* fvgettr code goes here */
        ...

\end{verbatim}}\noindent
Now we run into a small difficulty.  Our only parameter has a default
value that is obtained only after we read in the first trace.  The
obvious solution is to reverse the parameter getting and the trace
getting in the template.  Thus we resume:
{\small\begin{verbatim}
        /* Get info from first trace and set ns */ 
        if (!fvgettr(stdin, &tr))  err("can't get first trace"); 
        if (!getparint("ns", &ns))    ns = tr.ns;

        /* Loop over the traces */
        do {
                int nt = tr.ns;
\end{verbatim}}\noindent
Now comes the actual seismic algorithm---which is rather trivial in
the present case:  add zeroes to the end of the input trace if the
output length is specified greater than the input length.  We could
write a simple loop to do the job, but the task is done most
succinctly by using the {\sf ANSI-C} routine {\tt memset}.  However, we
confess that unless we've used it recently, we usually forget how to
use this routine.  One solution is to {\tt cd} to the {\tt su/main}
directory and use {\tt grep} to find other uses of {\tt memset}.  When
we did this, we found that {\tt sumute} had usage closest to what we
needed and that is why we started from a copy of that code.  Here is
the complete main for {\tt suvlength}:
{\small\begin{verbatim}
/* SUVLENGTH: $Revision: 1.16 $ ; $Date: 1998/01/15 17:46:03 $        */

#include "su.h"
#include "segy.h"

/*********************** self documentation **********************/
char *sdoc[] = {
"                                                                 ",
" SUVLENGTH - Adjust variable length traces to common length      ",
"                                                                 ",
" suvlength <vdata >stdout                                        ",
"                                                                 ",
" Required parameters:                                            ",
"         none                                                    ",
"                                                                 ",
" Optional parameters:                                            ",
"          ns     output number of samples (default: 1st trace ns)",
NULL};
/**************** end self doc ***********************************/

/* Credits:
 *        CWP: Jack Cohen, John Stockwell
 *
 * Trace header fields accessed:  ns
 * Trace header fields modified:  ns
 */

/* prototype */
int fvgettr(FILE *fp, segy *tp);

segy tr;

main(int argc, char **argv)
{
        int ns;                /* samples on output traces        */


        /* Initialize */
        initargs(argc, argv);
        requestdoc(1);


        /* Get info from first trace */ 
        if (!fvgettr(stdin, &tr))  err("can't get first trace"); 
        if (!getparint("ns", &ns))    ns = tr.ns;


        /* Loop over the traces */
        do {
                int nt = tr.ns;
                                
                if (nt < ns) /* pad with zeros */
                        memset((void *)(tr.data + nt), '\0', (ns-nt)*FSIZE);
                tr.ns = ns;
                puttr(&tr);
        } while (fvgettr(stdin, &tr));
        
        return EXIT_SUCCESS;
}


#include "header.h"

/* fvgettr - get a segy trace from a file by file pointer (nt can vary)
 *
 * Returns:
 *        int: number of bytes read on current trace (0 after last trace)
 *
 * Synopsis:
 *        int fvgettr(FILE *fp, segy *tp)
 *
 * Credits:
 *        Cloned from .../su/lib/fgettr.c
 */

int fvgettr(FILE *fp, segy *tp)
   ...
\end{verbatim}}\noindent
{\bf Remark}: In the actual {\small\sf SU}, the subroutine {\tt fvgettr} has been
extracted as a library function and we also made a convenience macro
{\tt vgettr} for the case of standard input.  But these are secondary
considerations that don't arise for most applications.

For any new {\small\sf SU} code, one should provide an example shell program to show how
the new code is to be used.  Here is such a program for X Windows graphics:
{\small\begin{verbatim}
#! /bin/sh
# Trivial test of suvlength with X Windows graphics

WIDTH=700
HEIGHT=900
WIDTHOFF=50
HEIGHTOFF=20

>tempdata
>vdata
suplane >tempdata  # default is 32 traces with 64 samples per trace
suplane nt=72 >>tempdata
suvlength <tempdata ns=84 |
sushw key=tracl a=1 b=1 >vdata

# Plot the data 
suxwigb <vdata \
        perc=99 title="suvlength test"\
        label1="Time (sec)" label2="Traces" \
        wbox=$WIDTH hbox=$HEIGHT xbox=$WIDTHOFF ybox=$HEIGHTOFF &

# Remove #comment sign on next line to test the header
#sugethw <vdata tracl ns | more
\end{verbatim}}\noindent

\section{sufind}
{\bf sufind} is a program that searches the self-documentations
for a given string.  For example,
{\small\begin{verbatim}
% sufind fft

 FFTLAB - Motif-X based graphical 1D Fourier Transform

 Usage:  fftlab


HANKEL - Functions to compute discrete Hankel transforms

hankelalloc     allocate and return a pointer to a Hankel transformer
hankelfree      free a Hankel transformer

PFAFFT - Functions to perform Prime Factor (PFA) FFT's, in place

npfa            return valid n for complex-to-complex PFA
npfar           return valid n for real-to-complex/complex-to-real PFA

 SUAMP - output amp, phase, real or imag trace from             
        (frequency, x) domain data                              

 suamp <stdin >stdout mode=amp                                  

 SUFFT - fft real time traces to complex frequency traces       

 suftt <stdin >sdout sign=1                                     


 SUFRAC -- take general (fractional) time derivative or integral of     
            data, plus a phase shift.  Input is TIME DOMAIN data.       

 sufrac power= [optional parameters] <indata >outdata                   

 SUIFFT - fft complex frequency traces to real time traces      

 suiftt <stdin >sdout sign=-1                                   


 SUMIGPS - MIGration by Phase Shift with turning rays                   

 sumigps <stdin >stdout [optional parms]                                


 SUMIGTK - MIGration via T-K domain method for common-midpoint stacked data

 sumigtk <stdin >stdout dxcdp= [optional parms]                 


 SURADON - forward generalized Radon transform from (x,t) -> (p,tau) space.

 suradon <stdin >stdout [Optional Parameters]                           



For more information type: "program_name <CR>"
\end{verbatim}}\noindent
The final line of this output ends with a symbol meant to indicate that the user is to type a carriage return.\footnote{The phrase ``carriage return'' refers to an older technology, the typewriter.  Ask your parents for further details.}

\section{sukeyword}
{\small\sf SU} programs that manipulate the trace headers
use specific names called ``keywords'' to identify the 
header fields.  The {\bf sukeyword} program enables the user to list
the definition file for the keywords.  For example,
{\small\begin{verbatim}
% sukeyword fldr

...skipping
        int tracr;     /* trace sequence number within reel */

        int fldr;      /* field record number */

        int tracf;     /* trace number within field record */

        int ep;        /* energy source point number */

        int cdp;       /* CDP ensemble number */

        int cdpt;      /* trace number within CDP ensemble */

        short trid;     /* trace identification code:
                        1 = seismic data
                        2 = dead
                        3 = dummy
                        4 = time break
                        5 = uphole
                        6 = sweep
                        7 = timing
                        8 = water break
                        9---, N = optional use (N = 32,767)
--More--(13%)
\end{verbatim}}\noindent

\section{Other help mechanisms}
\begin{itemize}
\item {\bf gendocs} is a program that creates the LaTeX document,
  {\bf selfdocs.tex}, that contains a complete set of all the
  self-documentations in the distribution (over 300 pages!).  A
  PostScript version of this document is available in our anonymous
  ftp site\\ (\verb:pub/cwpcodes/documentation.xx.tar.Z:).

\noindent
Here, {\tt xx} denotes the number of the current release.

\item The top level {\bf demos} directory contains a number of tutorial
  shell scripts.  Its subdirectories contain \verb:README: files that
  give detailed information.  Assuming that you start in the
  {\bf demos} directory, here is a roadmap to get you started:

  The Making\_Data demos shows the basics of making synthetic data
  shot gathers and common offset sections using susynlv.  Particular
  attention is paid to illustrating good display labeling.

  The Filtering/Sufilter demo illustrates some real data processing to
  eliminate ground roll and first arrivals.  The demos in the
  Filtering subdirectories give instructions for accessing the data
  from the CWP ftp site.

  The Deconvolution demo uses simple synthetic spike traces to
  illustrate both dereverberation and spiking decon using supef and
  other tools.  The demos include commands for systematically
  examining the effect of the filter parameters using loops.

  The Sorting\_Traces Tutorial is an interactive script that
  reinforces some of the basic UNIX and {\small\sf SU} lore discussed in this
  document.  The interactivity is limited to allowing you to set the
  pace.  Such tutorials quickly get annoying, but we felt that one
  such was needed to cover some issues that didn't fit into our
  standard demo format.  There is also a standard, but less complete,
  demo on this topic.

  The next step is to activate the Selecting\_Traces Demo.  Then
  proceed to the NMO Demo.  Beyond that, visit the Demo directories
  that interest you.  The {\bf demos} directory tree is still under
  active development---please let us know if the demos are helpful and
  how they can be improved.

\item The essence of {\small\sf SU} usage is the construction of shell programs
  to carry out coordinated data processing.  The {\bf su/examples}
  directory contains a number of such programs.  By the way, the terms
  ``shell scripts,'' ``shell programs,'' ``shell files,'' and
  ``shells,'' are used interchangeably in the UNIX literature.

\item The {\bf faq} directory contains a growing collection of answers
  to frequenty asked questions about {\small\sf SU}, including detailed information
  about tape reading, data format questions, and seismic processing tips.

\item The text book, {\em Theory of Seismic Imaging}, by John A.
  Scales, Samizdat Press, 1994, is available in our anonymous ftp site
  in both 300 and 400 dots per inch PostScript format:
  \verb:pub/samizdat/texts/imaging/imaging_300dpi.ps.Z: or
  \verb:imaging_400dpi.ps.Z:.  The exercises in this text make
  extensive use of {\small\sf SU}.
%and 400 dots per inch PostScript format:\\
%\verb:pub/samizdat/texts/imaging/imaging_300dpi.ps.Z: or\\
%\verb:pub/samizdat/texts/imaging/imaging_400dpi.ps.Z:.
%\ \ The exercises in this\\
%text make extensive use of {\small\sf SU}.

\item You should not hesitate to look at the source code itself.
  Section~\ref{SU:sec:template} explains the key {\small\sf SU} coding idioms.
  Please let us know if you discover any inconsistencies between the
  source and our documentation of it.  We also welcome suggestions for
  improving the comments and style of our codes.

\item Direct email to:  john@dix.mines.edu 
if you have comments, questions, or suggestions regarding {\small\sf SU}.

\end{itemize}

                           initiation time of energy source and time
                           when recording of data samples begins
                           (for deep water work if recording does not
                           start at zero time) */

        short muts;     /* mute time--start */

        short mute;     /* mute time--end */

        unsigned short ns;      /* number of samples in this trace */

        unsigned short dt;      /* sample interval; in micro-seconds */

        short gain;     /* gain type of field instruments code:
                                1 = fixed
                                2 = binary
                                3 = floating point
                                4 ---- N = optional use */

--More--(53%)
\end{verbatim}}\noindent

\subsection{Viewing program names}
{\small\sf SU} program names are often obscure (we aren't proud of this).
Here's how to get help with remembering the exact name of a program
when you recall a fragment of the name:

{\small\begin{verbatim}
% sufind -n head

 SUADDHEAD - put headers on bare traces and set the tracl and ns fields
 UPDATEHEAD - update ../doc/Headers/Headers.all

For more information type: "program_name <CR>"
\end{verbatim}}\noindent
Recall also that {\bf suhelp} and {\bf suname} give comprehensive listings
of the {\small\sf SU} programs.

Note that we used the {\tt -n} option of the  {\bf sufind} command.  Using the self-doc facility, we can get the full story:
{\small\begin{verbatim}
% sufind

sufind - get info from self-docs about SU programs
Usage: sufind [-v -n] string
"sufind string" gives brief synopses
"sufind -v string" verbose hunt for relevant items
"sufind -n name_fragment" searches for command name
\end{verbatim}}\noindent

\section{Understanding and using SU shell programs}
The essence of good {\small\sf SU} usage is constructing (or cloning!)
UNIX shell programs to create and record processing flows.
In this section, we give some
annotated examples to get you started. 
\subsection{A simple SU processing flow example\label{SU:sec:Plotshell}}
Most {\small\sf SU} programs read from standard input and write to standard output.
Therefore, one can build complex processing flows by simply
connecting {\small\sf SU} programs with UNIX pipes.
Most flows will end with one of the {\small\sf SU} plotting programs.
Because typical processing flows are lengthy and involve many
parameter settings, it is convenient to put the {\small\sf SU} commands in a
shell file.

{\bf Remark}: All the UNIX shells, Bourne (sh), Cshell (csh),
Korn (ksh), \ldots, include a programming language.  In this document,
we exclusively use the Bourne shell programming language.

Our first example is a simple shell program called {\bf Plot}.
The numbers in square brackets at the
end of the lines in the following listing are not part of the
shell program---we added them as keys to the discussion
that follows the listing.

{\small\begin{verbatim}
#! /bin/sh                                              [1]
# Plot:   Plot a range of cmp gathers
# Author: Jane Doe
# Usage:  Plot cdpmin cdpmax

data=$HOME/data/cmgs                                    [2]

# Plot the cmp gather.
suwind <$data key=cdp min=$1 max=$2 |                   [3]
sugain tpow=2 gpow=.5 |
suximage f2=0 d2=1 \                                    [4]
        label1="Time (sec)" label2="Trace number" \
        title="CMP Gathers $1 to $2" \
        perc=99 grid1=solid &                           [5]
\end{verbatim}}\noindent
{\bf Discussion of numbered lines:}

\begin{enumerate}
\item The symbol \verb:#: is the comment symbol---anything on the remainder
of the line is not executed by the UNIX shell.  The combination
\verb:#!: is an exception to this rule: the shell uses the
file name following
this symbol as a path to the program that is to execute the remainder
of the shell program.

\item The author apparently intends that the shell be edited
if it is necessary to change the data set---she made this easier to
do by introducing the shell variable \verb:data: and assigning
to it the full pathname of the data file.  The assigned value
of this parameter is accessed as \verb:$data: within the shell program.
The parameter \verb:$HOME: appearing as the first component of the file
path name is a UNIX maintained environment variable
containing the path of the user's home directory.  In general,
there is no need for the data to be located in the user's home
directory, but the user would need ``read permission'' on the
data file for the shell program to succeed.

{\bf WARNING!}  Spaces are significant to the UNIX shell---it  uses
them to parse command lines.  So despite all we've learned about
making code easy to read, do {\em not} put spaces next to the \verb:=: symbol.
(Somewhere around 1977, one author's (Jack) first attempt to learn UNIX was
derailed for several weeks by making this mistake.)

\item The main pipeline of this shell code selects a certain set of cmp gathers with {\bf suwind}, gains this subset with {\bf sugain} and pipes the result into
the plotting program {\bf suximage}.  As indicated in the Usage comment,
the cmp range is specified by command line arguments.
Within the shell program, these arguments are
referenced as \verb:$1:, \verb:$2: (i.e., first argument, second argument).

\item The lines within the {\bf suximage} command are continued by the
backslash escape character.

\noindent{\bf WARNING!}  The line continuation backslash must be the {\em final}
character on the line---an invisible space or tab following the
backslash is one of the most common and frustrating bugs in UNIX
shell programming.

\item The final \verb:&: in the shell program
puts the plot window into ``background'' so we can continue
working in our main window.  This is the X-Windows
usage---the \verb:&: should {\em not} be used with the analogous PostScript
plotting programs (e.g., supsimage).  For example, with {\bf supsimage} in
place of {\bf suximage}, the \verb:&: might be replaced by \verb:| lpr:.

The {\small\sf SU} plotting programs are special---their self-doc doesn't
show all the parameters accepted.  For example, most of the parameters
accepted by {\bf suximage}
are actually specified in the self-documentation for the
generic {\small\sf CWP} plotting program {\bf ximage}.  This apparent flaw
in the self-documentation is actually a side
effect of a key {\small\sf SU} design decision.  The {\small\sf SU} graphics
programs call on the generic plotting programs to do the actual plotting.
The alternative design was to have tuned graphics programs
for various seismic applications.
Our design choice keeps things simple,
but it implies a basic limitation in {\small\sf SU}'s graphical capabilities.

The plotting programs are the vehicle for presenting your results.
Therefore you should take the time to carefully look
through the self-documentation for {\em both} the ``{\small\sf SU} jacket'' programs
({\bf suximage}, {\bf suxwigb}, \ldots) and the generic plotting
programs ({\bf ximage}, {\bf xwigb}, \ldots).

\end{enumerate}

\subsection{Executing shell programs}
The simplest way to execute a UNIX shell program is to give
it ``execute permission.''  For example, to make our above {\bf Plot} shell
program executable:
{\small\begin{verbatim}
chmod +x Plot
\end{verbatim}}\noindent
Then to execute the shell program:
{\small\begin{verbatim}
Plot 601 610
\end{verbatim}}\noindent
Here we assume that the parameters \verb:cdpmin=601:, \verb:cdpmax=610: are
appropriate values for the \verb:cmgs: data set.
Figure~\ref{fig:Plot} shows an output generated by the \verb:Plot: shell
program.
\begin{figure}[htbp]
\epsfysize 280pt
\centerline{\epsffile{Plot.eps}}
\caption{Output of the \protect\verb:Plot: shell program.}
\label{fig:Plot}
\end{figure}


\subsection{A typical SU processing flow\label{SU:sec:Dmoshell}}
Suppose you want to use {\bf sudmofk}.  You've read the self-doc, but
a detailed example is always welcome isn't it?  The place to look is
the directory {\bf su/examples}.  In this case, we are lucky and find
the shell program, {\bf Dmo}.  Again, the numbers in square brackets at the
end of the lines shown below are {\em not} part of the listing.
{\small\begin{verbatim}
#! /bin/sh
# dmo
set -x                                                            [1]

# set parameters
input=cdp201to800                                                 [2]
temp=dmocogs
output=dmocmgs
smute=1.7
vnmo=1500,1550,1700,2000,2300,2600,3000                           [3]
tnmo=0.00,0.40,1.00,2.00,3.00,4.00,6.00


# sort to common-offset, nmo, dmo, inverse-nmo, sort back to cmp
susort <$input offset cdp |                                       [4]
sunmo smute=$smute vnmo=$vnmo tnmo=$tnmo |                        [5]
sudmofk cdpmin=201 cdpmax=800 dxcdp=13.335 noffmix=4 verbose=1 |  [6]
sunmo invert=1 smute=$smute vnmo=$vnmo tnmo=$tnmo >$temp          [7]
susort <$temp cdp offset >$output                                 [8]
\end{verbatim}}\noindent
{\bf Discussion of numbered lines:}

The core of the shell program (lines 5-7) is recognized as the typical
dmo process: crude nmo, dmo, and then ``inverse'' nmo.
The dmo processing is surrounded by sorting operations
(lines 4 and 8).  Here is a detailed discussion of the shell program
keyed to the numbers appended to the listing (see also the discussion
above for the \verb:Plot: shell):

\begin{enumerate}
\item Set a debugging mode that asks UNIX
to echo the lines that are executed.  You can comment
this line off when its output is no longer of interest.  An
alternate debugging flag is \verb:set -v: which echos
lines as they are read by the shell interpreter.  You
can use both modes at once if you like.

\item This line and the next two lines set filenames that,
in this case, are in the same directory as the shell program itself.
Again, the reason for using parameters here is to make it easy
to ``clone'' the shell for use with other data sets.
Those of us who work with only a few data sets at any given time,
find it convenient to devote a directory to a given data set and
keep the shells used to process the data in that directory as
documentation of the processing parameters used.  ({\small\sf SU} does not have
a built-in ``history'' mechanism.)

\item The dmo process requires a set of velocity-time picks for
the subsidiary nmo processes.  Because these picks must be consistent
between the nmo and the inverse nmo, it is a good idea to make them
parameters to avoid editing mistakes.  Again, note the format
of {\small\sf SU} parameter vectors: comma-separated strings with no spaces.
The nmo program ({\bf sunmo}) will give an error message and abort
if the \verb:vnmo: and \verb:tnmo: vectors have different lengths.

\item Note that {\bf susort} allows the use of {\em secondary}
sort keys.  Do not assume that a secondary field that is
initially in the ``right'' order will remain in that order
after the sort---if you care about the order of some secondary
field, specify it (as this shell program does). In this line,
we sort the data according to increasing offsets and then, within
each offset, we sort according to increasing cdp number.

\item The forward nmo step.

\item The dmo step.

\item The inverse nmo step.

\item Sort back to cdp and have increasing offset within each cdp.
\end{enumerate}

If you want to thoroughly understand this shell program, your next
step is to study the self-docs of the programs involved:

{\small\begin{verbatim}
% sunmo

SUNMO - NMO for an arbitrary velocity function of time and CDP

sunmo <stdin >stdout [optional parameters]

Optional Parameters:
vnmo=2000         NMO velocities corresponding to times in tnmo
tnmo=0            NMO times corresponding to velocities in vnmo

...
\end{verbatim}}\noindent
Related shell programs are {\bf su/examples/Nmostack} and
{\bf su/examples/Mig}.

\section{Extending SU by shell programming}
Shell programming can be used to
greatly extend the reach of {\small\sf SU} without writing C code.
See, for example, {\bf CvStack}, {\bf FilterTest}, {\bf FirstBreak}, and
{\bf Velan} in {\bf su/examples}.

It is a sad fact that the UNIX shell is not
a high level programming language---consequently, effective shell
coding often involves arcane tricks.  In this section, we'll
provide some useful templates for some of the
common UNIX shell programming idioms.

We use {\bf CvStack} as an
illustration.  The core of this shell is a
double loop over velocities and cdps that produces
{\em velocity panels}---a concept
not contained in any single {\small\sf SU} program.

{\bf Remark}:  For most of us,
writing a shell like {\bf CvStack} from scratch is a time-consuming affair.
To cut down the development time,
your authors excerpt from existing shells to make new ones
even when we don't quite remember what every detail means.
We suggest that you do the same!

We won't comment on the lines already explained in our previous
two shell code examples
(see Sections~\ref{SU:sec:Plotshell} and~\ref{SU:sec:Dmoshell}),
but instead focus on the new features used in {\bf CvStack}.

{\small\begin{verbatim}
#! /bin/sh
# Constant-velocity stack of a range of cmp gathers
# Authors: Jack, Ken
# NOTE: Comment lines preceding user input start with  #!#
set -x

#!# Set input/output file names and data parameters
input=cdp601to610
stackdata=cvstack
cdpmin=601 cdpmax=610
fold=30
space=1         # 1 null trace between panels

#!# Determine velocity sampling.
vmin=1500   vmax=3000   dv=150

### Determine ns and dt from data (for sunull)
nt=`sugethw ns <$input | sed 1q | sed 's/.*ns=//'`                [1]
dt=`sugethw dt <$input | sed 1q | sed 's/.*dt=//'`

### Convert dt to seconds from header value in microseconds
dt=`bc -l <<END                                                   [2]
        scale=4
        $dt / 1000000
END`


### Do the velocity analyses.
>$stackdata  # zero output file                                   [3]
v=$vmin
while [ $v -le $vmax ]                                            [4]
do
        cdp=$cdpmin
        while [ $cdp -le $cdpmax ]                                [5]
        do
                suwind <$input \                                  [6]
                        key=cdp min=$cdp max=$cdp count=$fold |
                sunmo cdp=$cdp vnmo=$v tnmo=0.0 |
                sustack >>$stackdata
                cdp=`bc -l <<END                                  [7]                               
                        $cdp + 1
END`
        done
        sunull ntr=$space nt=$nt dt=$dt >>$stackdata              [8]
        v=`bc -l <<END
                $v + $dv
END`
done


### Plot the common velocity stacked data
ncdp=`bc -l <<END
        $cdpmax-$cdpmin+1
END`
f2=$vmin
d2=`bc -l <<END
        $dv/($ncdp + $space)                                      [9]
END`

sugain <$stackdata tpow=2.0 |

suximage perc=99 f2=$f2 d2=$d2 \
        title="File: $input  Constant-Velocity Stack " \
        label1="Time (s)"  label2="Velocity (m/s)" & 

exit                                                              [10]
\end{verbatim}}\noindent
{\bf Discussion of numbered lines:}

\begin{enumerate}
\item This elaborate construction gets some information
from the first trace header of the data set.  The program {\bf sugethw}
lists the values of the specified keys in the successive traces.  For
example,
{\small\begin{verbatim}
% suplane | sugethw tracl ns
 tracl=1            ns=64       

 tracl=2            ns=64       

 tracl=3            ns=64       

 tracl=4            ns=64       

 tracl=5            ns=64       

 tracl=6            ns=64    
   
 ...
\end{verbatim}}\noindent
Although {\bf sugethw} is eager to give the values for every trace in the
data set, we only need it once.  The solution is to use the UNIX stream
editor ({\bf sed}).  In fact, we use it twice.  By default, {\bf sed} passes
along its input to its output.  Our first use is merely to tell {\bf sed}
to quit after it puts the first line in the pipe.  The second pass through
{\bf sed} strips off the unwanted material before the integer.
In detail, the second {\bf sed} command reads: replace (or substitute)
everything up to the characters \verb:ns=: with nothing, i.e., delete
those characters.


\item We are proud of this trick.
The Bourne shell does not provide floating point
arithmetic.  Where this is needed, we use the UNIX built-in
{\bf bc} calculator program with the ``here document'' facility.
Here, we make the commonly needed conversion of sampling interval which
is given in micro-seconds in the {\sf SEG-Y} header,
but as seconds in {\small\sf SU} codes.  Note carefully the {\em back}quotes
around the entire calculation---we assign the result of this
calculation to the shell variable on the left of the equal sign,
here \verb:dt:.  The calculation may take several lines.
We first set the number of decimal places with \verb:scale=4:
and then do the conversion to seconds.  The characters \verb:END:
that follow the here document redirection symbol \verb:<<: are arbitrary,
the shell takes its input from the text in the shell file
until it comes to a line that contains the same
characters again.  For more information about {\bf bc}:
{\small\begin{verbatim}
% man bc
\end{verbatim}}\noindent

\item As the comment indicates, this is a special use of the output
redirection symbol that has the effect of destroying any pre-existing
file of the same name or opening a new file with that name.  In fact,
this is what \verb:>: always does as its first action---it's a dangerous
operator!  If you intend to {\em append}, then, as mentioned earlier, use
\verb:>>:.

\item This is the outer loop over velocities.
Another warning about spaces---the spaces around the bracket
symbols are essential.

{\bf Caveat}: The bracket notation is a nice
alternative to the older clunky \verb:test: notation:
{\small\begin{verbatim}
while test $v -le $vmax
\end{verbatim}}\noindent
Because the bracket notation is not documented on the typical {\bf sh} manual
page, we have some qualms about using it.  But, as far as we know,
all modern {\bf sh} commands support it---please let us know
if you find one that doesn't.

{\bf WARNING!}  OK, now you know that there is a UNIX command
called \verb:test:.  So don't use the name ``test'' for one of your
shell (or C) programs---depending on your \verb:$PATH: setting, you could
be faced with seemingly inexplicable output.

\item This is the inner loop over cdps.

\item Reminder: No spaces or tabs after the line continuation
symbol!

\item Notice that we broke the nice indentation structure by
putting the final \verb:END: against the left margin.  That's because
the {\bf sh} manual page says that the termination should contain
only the \verb:END: (or whatever you use).  In fact, most versions
support indentation.  We didn't think the added beautification was
worth the risk in a shell meant for export.  Also note that we used
{\bf bc} for an integer arithmetic calculation even though
integer arithmetic is built into the Bourne shell---why learn
two arcane rituals, when one will do?  See \verb:man expr:, if
you are curious.
\begin{figure}[htbp]
\epsfysize 300pt
\centerline{\epsffile{CvStack.eps}}
\caption{Output of the \protect\verb:CvStack: shell program.}
\label{fig:cvstack}
\end{figure}

\item {\bf sunull} is a program I (Jack) wrote to create all-zero traces
to enhance displays of the sort produced by \verb:CvStack:.
Actually, I had written this program many times, but this was the first
time I did it on purpose.  (Yes, that was an attempt at humor.)

\item An arcane calculation to get velocity labeling
on the trace axis.  Very impressive!  I wonder what it means?
(See last item.)

\item The \verb:exit: statement is useful because you might want
to save some ``spare parts'' for future use.  If so, just put them
after the \verb:exit: statement and they won't be executed.
\end{enumerate}

\noindent Figure~\ref{fig:cvstack} shows an output generated by \verb:CvStack:.

\section{Some core programs}

Reading the self-documentation and trying out the following {\small\sf SU} programs
will give you a good start in learning {\small\sf SU}. 

\subsection{Examining the trace headers}
\begin{description}
\item{\bf surange} --- print minimum and maximum values of trace header fields
\item{\bf sugethw} --- print values of selected header fields
\item{\bf suascii} --- print header and data values
\item{\bf suxedit} --- interactively examine headers and traces
\end{description}

\subsection{Some common processing programs}
\begin{description}
\item{\bf suacor} --- compute autocorrelations
\item{\bf sufilter} --- multipurpose zero phase filter (includes bandpass)
\item{\bf sugain} --- gain (with lots of options)
\item{\bf sumute} --- zero samples before a time that depends on offset
\item{\bf sunmo} --- normal-moveout correction
\item{\bf supef} --- prediction error filtering
\item{\bf susort} --- sort traces by values of trace header fields
\item{\bf sustack} --- stack (sum) traces
\item{\bf suvelan} --- velocity analysis
\item{\bf suwind} --- window (i.e., get a subset of) traces
\end{description}

\subsection{Some common plotting programs}
\begin{description}
\item{\bf suximage} --- gray scale X Windows plotting
\item{\bf suxwigb} --- bit mapped wiggle trace X Windows plotting
\item{\bf supsimage} --- gray scale PostScript plotting
\item{\bf supswigb} --- bit mapped wiggle trace PostScript plotting
\end{description}

\section{A brief tour of the source directories}
The {\small\sf SU} software is a layered product.  The layers correspond to the
following directories:
\begin{description}
\item{\bf cwp} Library of scientific routines (e.g. fft routines)
written in ``vanilla'' C. Utility mains and shells.
\item{\bf par} Library supporting the {\small\sf CWP} programming
style (i.e., self-doc, error reporting, parameter passing).
Mains that use (only) these facilities.  Shells for maintaining the
online documentation database.
\item{\bf su} Seismic processing codes that use the
{\sf SEG-Y} trace structure.    Subroutines that manage this
structure. Codes that buffer the generic graphics
routines listed below.  Shells that provide backward compatibility with
earlier releases.
\item{graphics libraries}
        \begin{enumerate}
        \item {\bf psplot}---PostScript graphics:
                \begin{enumerate}
                \item pscontour: contour plots
                \item pscube: 3D data cube
                \item psgraph: curve plotting
                \item psimage: raster plotting
                \item psmovie: supports frames
                \item pswigb: bit mapped wiggle traces (fast)
                \item pswigp: polygon wiggle traces (slow)
                \item PostScript support programs
                \end{enumerate}
        \item {\bf xplot}---xlib based X Windows graphics
                \begin{enumerate}
                \item ximage: raster plotting
                \item xwigb: bit mapped wiggle traces
                \item X Windows support programs
                \end{enumerate}
        \item \bf{Xtcwp}---toolkit based X Windows graphics
                \begin{enumerate}
                \item xgraph: curve plotting
                \item xmovie: supports frames
                \item X Windows resource files
                \end{enumerate}
        \end{enumerate}
\end{description}
These are only the highlights.  If you intend to add your
own C mains to the package, it is worthwhile
spending a few hours browsing through the source code.

\appendix
\chapter{Obtaining and Installing SU  \label{app:A}}
The {\small\sf SU} package contains seismic processing programs along with
libraries of scientific routines, graphics routines and
routines supporting the {\small\sf SU} coding conventions.
The package is available by anonymous ftp at the site
ftp.cwp.mines.edu (138.67.12.4). The directory path is pub/cwpcodes.
The package may also be obtained on the World Wide Web at
http://www.cwp.mines.edu/cwpcodes.
Take the files:
\begin{enumerate}
\item README\_BEFORE\_UNTARRING
\item untar\_me\_first.xx.tar.Z
\item cwp.su.all.xx.tar.Z
\end{enumerate}
Here the {\tt xx} denotes the number of the current release.
An incremental update is also available for updating the
previous release {\tt yy} to the current release {\tt xx}.  Take the files:

\begin{enumerate}
\item README\_BEFORE\_UNTARRING
\item README\_UPDATE
\item untar\_me\_first.xx.tar.Z
\item update.yy.to.xx.tar.Z
\item update.list
\end{enumerate}

\noindent If you find that {\tt ftp} times out during the transmission of the
files, the package is available in smaller pieces in the subdirectory
{\tt outside\_usa}.

\noindent For readers who are not familiar with anonymous ftp,
an annotated transaction listing follows in section~\ref{SU:sec:anonftp}.

\section{Obtaining files by anonymous ftp\label{SU:sec:anonftp}}
\begin{tabular}{lll}
Type: & &  \\
\% ftp 138.67.12.4        & --- &  138.67.12.4 is our ftp site  \\
username: anonymous       & --- &   your username is ``anonymous''     \\
password: yourname@your.machine.name   & --- &  type anything here  \\
& &  \\
ftp$>$     & --- & this is the prompt you see \\
& & when you are in ftp 
\end{tabular}

\indent You are now logged in via ftp to the CWP anonymous ftp site.
You may type:

\begin{tabular}{lll}
ftp$>$ ls             & --- & to see the contents of the directories \\
ftp$>$ cd dirname     & --- & to change directories to ``dirname'' \\
ftp$>$ binary         & --- & to set ``binary mode'' for transferring files \\
        & &            You must do this before you try to transfer any \\
        & &            binary file. This includes all files with the form \\
        & &            some\_name.tar.Z extension. \\
ftp$>$ get filename   & --- & to transfer  ``filename'' from our site to your machine \\
ftp$>$ mget pattern*  & --- & to transfer all files with names of the ``pattern*'' \\
For example: & & \\
&& \\
ftp$>$ mget *.tar.Z   & --- & will transfer all files with the form of name.tar.Z \\
& &              to your machine. You will be asked whether you  \\
& &              really want each file of this name pattern transferred, \\
& &               before ftp actually does it.  \\
ftp$>$ bye            & --- & to exit from ftp 
\end{tabular}

\section{Requirements for installing the package}
The only requirements for installing the package are:
\begin{enumerate}
\item A machine running the UNIX operating system.
\item An {\sf ANSI C} compiler.
\item A version of make which supports include files
\item 16-60 megabytes (depending on system) of disk space for
the source and compiled binary. If space is an issue, then the
compiled binaries may be ``stripped'' by cd'ing to \$CWPROOT/bin
and typing "strip *".
\end{enumerate}

\noindent
The package has been successfully installed on:
\begin{itemize}
%update
\item IBM RS6000
\item SUN SPARC STATIONS
\item HP 9000 series machines
\item HP Apollo
\item NeXT
\item Convex
\item DEC
\item Silicon Graphics
\item PC's running LINUX, PRIME TIME, SCO, FREE BSD, ESIX, and NeXTSTEP 486
\end{itemize}

There are README files in the distribution with special notes about some
of these platforms.  We depend on the {\small\sf SU} user community to alert us to 
installation problems, so if you run into difficulties, please let us know.

The distribution contains a series of files that detail the installation 
process.  Read them in the following order:

{\small\begin{verbatim}
LEGAL_STATEMENT --- license,  legal statement
README_BEFORE_UNTARRING --- initial information
README_FIRST --- general information
README_TO_INSTALL --- installation instructions
Portability/README_*    --- portability information for various platforms
README_GETTING_STARTED --- how to begin using the codes
\end{verbatim}}\noindent
Many of these files are contained within untar\_me\_first.xx.tar.Z.

\section{A quick test}
\begin{figure}
\epsfxsize 250pt
\centerline{\epsffile{suplane.eps}}
\caption{Output of the \protect\verb:suplane: pipeline.}
\label{fig:suplane}
\end{figure}

Once you have completed the installation, here is a quick test you can make
to see if you have a functioning seismic system.
For an X-windows machine, the ``pipeline''
\begin{verbatim}
suplane | suximage &
\end{verbatim}
should produce the graphic shown in Figure~\ref{fig:suplane}. 
If you have a PostScript printer, then you should get a hard copy version
with the pipeline
\begin{verbatim}
suplane | supswigb | lpr
\end{verbatim}
If you have Display PostScript, or a PostScript previewer, then to get
a screen display, replace the \verb:lpr: command in the pipeline by
the command that opens a PostScript file, for example:
\begin{verbatim}
suplane | supswigb | ghostview -
\end{verbatim}

Another set of test pipelines are
\begin{verbatim}
susynlv | supsimage | lpr
\end{verbatim}
\begin{verbatim}
susynlv | supswigb | ghostview -
\end{verbatim}

\begin{figure}
\epsfxsize 300pt
\centerline{\epsffile{susynlv.eps}}
\caption{Output of the \protect\verb:susynlv: pipeline.}
\label{fig:susynlv}
\end{figure}

\chapter{Help Facililties \label{app:B}}
\section{Suhelp}
The full text of the output from:
\begin{verbatim}
% suhelp
\end{verbatim} \noindent

\input suhelp.tex

\section{Suname}
The full text of the output from:
\begin{verbatim}
% suname
\end{verbatim}\noindent
\input suname.tex

\section{Suhelp.html}
This is the text listing of Chris Liner's SU Help page.

\input suhelphtml.tex

\end{document}

  The Making\_Data demos shows the basics of making synthetic data
  shot gathers and common offset sections using susynlv.  Particular
  attention is paid to illustrating good display labeling.

  The Filtering/Sufilter demo illustrates some real data processing to
  eliminate ground roll and first arrivals.  The demos in the
  Filtering subdirectories give instructions for accessing the data
  from the CWP ftp site.

  The Deconvolution demo uses simple synthetic spike traces to
  illustrate both dereverberation and spiking decon using supef and
  other tools.  The demos include commands for systematically
  examining the effect of the filter parameters using loops.

  The Sorting\_Traces Tutorial is an interactive script that
  reinforces some of the basic UNIX and {\small\sf SU} lore discussed in this
  document.  The interactivity is limited to allowing you to set the
  pace.  Such tutorials quickly get annoying, but we felt that one
  such was needed to cover some issues that didn't fit into our
  standard demo format.  There is also a standard, but less complete,
  demo on this topic.

  The next step is to activate the Selecting\_Traces Demo.  Then
  proceed to the NMO Demo.  Beyond that, visit the Demo directories
  that interest you.  The {\bf demos} directory tree is still under
  active development---please let us know if the demos are helpful and
  how they can be improved.

\item The essence of {\small\sf SU} usage is the construction of shell programs
  to carry out coordinated data processing.  The {\bf su/examples}
  directory contains a number of such programs.  By the way, the terms
  ``shell scripts,'' ``shell programs,'' ``shell files,'' and
  ``shells,'' are used interchangeably in the UNIX literature.

\item The {\bf faq} directory contains a growing collection of answers
  to frequenty asked questions about {\small\sf SU}, including detailed information
  about tape reading, data format questions, and seismic processing tips.

\item The text book, {\em Theory of Seismic Imaging}, by John A.
  Scales, Samizdat Press, 1994, is available in our anonymous ftp site
  in both 300 and 400 dots per inch PostScript format:
  \verb:pub/samizdat/texts/imaging/imaging_300dpi.ps.Z: or
  \verb:imaging_400dpi.ps.Z:.  The exercises in this text make
  extensive use of {\small\sf SU}.
%and 400 dots per inch PostScript format:\\
%\verb:pub/samizdat/texts/imaging/imaging_300dpi.ps.Z: or\\
%\verb:pub/samizdat/texts/imaging/imaging_400dpi.ps.Z:.
%\ \ The exercises in this\\
%text make extensive use of {\small\sf SU}.

\item You should not hesitate to look at the source code itself.
  Section~\ref{SU:sec:template} explains the key {\small\sf SU} coding idioms.
  Please let us know if you discover any inconsistencies between the
  source and our documentation of it.  We also welcome suggestions for
  improving the comments and style of our codes.

\item Direct email to:  john@dix.mines.edu 
if you have comments, questions, or suggestions regarding {\small\sf SU}.

\end{itemize}
